{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c761c320",
   "metadata": {},
   "source": [
    "# CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f24b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [1] - ƒê√É C·∫¨P NH·∫¨T\n",
    "import os\n",
    "\n",
    "MODEL_NAMES = [\n",
    "    \"joeddav/xlm-roberta-large-xnli\",\n",
    "    \"microsoft/infoxlm-large\",\n",
    "    \"uitnlp/CafeBERT\",\n",
    "    \"FacebookAI/xlm-roberta-large\",\n",
    "    \"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\",\n",
    "    \"MoritzLaurer/ernie-m-large-mnli-xnli\",\n",
    "    \"microsoft/deberta-xlarge-mnli\",\n",
    "]\n",
    "\n",
    "\n",
    "class Config:\n",
    "    ROOT_DIR = os.getcwd()\n",
    "    DATA_DIR = os.path.join(ROOT_DIR, \"data\")\n",
    "\n",
    "    # --- THAY ƒê·ªîI QUAN TR·ªåNG ---\n",
    "    # Tr·ªè ƒë·∫øn file ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω b·∫±ng semantic search\n",
    "    TRAIN_FILE = os.path.join(DATA_DIR, \"vihallu-train.csv\")\n",
    "\n",
    "    TEST_FILE = os.path.join(DATA_DIR, \"vihallu-public-test.csv\")\n",
    "    SUBMISSION_DIR = os.path.join(ROOT_DIR, \"submission\")\n",
    "    SUBMISSION_CSV = \"submit.csv\"\n",
    "    SUBMISSION_ZIP = \"submit.zip\"\n",
    "\n",
    "    MODEL_NAME = MODEL_NAMES[3]\n",
    "    MODEL_OUTPUT_DIR = os.path.join(\n",
    "        ROOT_DIR, \"models\", f\"{MODEL_NAME.split('/')[-1]}-tuned\"\n",
    "    )\n",
    "\n",
    "    MAX_LENGTH = 512\n",
    "    RANDOM_STATE = 42\n",
    "    EPOCHS = 10\n",
    "    BATCH_SIZE = 4\n",
    "    GRADIENT_ACCUMULATION_STEPS = 4\n",
    "    SCHEDULER_TYPE = \"constant_with_warmup\"\n",
    "    LEARNING_RATE = 6e-6\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    NUM_CYCLES = 3\n",
    "    CLASSIFIER_DROPOUT = 0.1\n",
    "    LABEL_SMOOTHING = 0.05\n",
    "    TOTAL_STEP_SCALE = 0.1\n",
    "    EPSILON = 1e-8\n",
    "\n",
    "    PATIENCE_LIMIT = 3\n",
    "    VALIDATION_SPLIT_SIZE = 0.2\n",
    "\n",
    "    LABEL_MAP = {\"no\": 0, \"extrinsic\": 1, \"intrinsic\": 2}\n",
    "    ID2LABEL = {v: k for k, v in LABEL_MAP.items()}\n",
    "    CLASS_WEIGHTS = [1.0393466963622866, 1.0114145354717525, 0.9531590413943355]\n",
    "\n",
    "\n",
    "cfg = Config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01017e0",
   "metadata": {},
   "source": [
    "# LOGGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1e41e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Th∆∞ m·ª•c g·ªëc ƒë·ªÉ l∆∞u t·∫•t c·∫£ c√°c file log\n",
    "LOG_BASE_DIR = \"logs\"\n",
    "\n",
    "# D√πng m·ªôt dictionary ƒë·ªÉ l∆∞u c√°c logger ƒë√£ t·∫°o, tr√°nh vi·ªác t·∫°o l·∫°i v√† g√¢y ra log tr√πng l·∫∑p\n",
    "_loggers = {}\n",
    "\n",
    "\n",
    "def setup_logger(model_name: str, log_level=logging.INFO):\n",
    "    \"\"\"\n",
    "    Thi·∫øt l·∫≠p v√† tr·∫£ v·ªÅ m·ªôt logger ƒë·ªÉ ghi log v√†o c·∫£ console v√† file.\n",
    "\n",
    "    - M·ªói model s·∫Ω c√≥ m·ªôt th∆∞ m·ª•c log ri√™ng d·ª±a tr√™n `model_name`.\n",
    "    - M·ªói l·∫ßn ch·∫°y s·∫Ω t·∫°o m·ªôt file log m·ªõi c√≥ t√™n l√† timestamp (v√≠ d·ª•: 2023-10-27_15-30-00.log).\n",
    "    - ƒê·∫£m b·∫£o kh√¥ng c√≥ log n√†o b·ªã ghi ƒë√®.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): T√™n c·ªßa model, d√πng ƒë·ªÉ t·∫°o th∆∞ m·ª•c con. V√≠ d·ª•: 'xnli-large-tuned'.\n",
    "        log_level (int): C·∫•p ƒë·ªô log, m·∫∑c ƒë·ªãnh l√† logging.INFO.\n",
    "\n",
    "    Returns:\n",
    "        logging.Logger: Instance c·ªßa logger ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh.\n",
    "    \"\"\"\n",
    "    # N·∫øu logger cho model n√†y ƒë√£ t·ªìn t·∫°i, tr·∫£ v·ªÅ n√≥ ngay l·∫≠p t·ª©c\n",
    "    if model_name in _loggers:\n",
    "        return _loggers[model_name]\n",
    "\n",
    "    # X·ª≠ l√Ω t√™n model ƒë·ªÉ an to√†n khi t·∫°o t√™n th∆∞ m·ª•c (thay th·∫ø \"/\")\n",
    "    safe_model_name = model_name.replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
    "    model_log_dir = os.path.join(LOG_BASE_DIR, safe_model_name)\n",
    "    os.makedirs(model_log_dir, exist_ok=True)\n",
    "\n",
    "    # T·∫°o logger\n",
    "    logger = logging.getLogger(safe_model_name)\n",
    "    logger.setLevel(log_level)\n",
    "\n",
    "    # NgƒÉn kh√¥ng cho log lan truy·ªÅn ƒë·∫øn root logger ƒë·ªÉ tr√°nh in ra console 2 l·∫ßn\n",
    "    logger.propagate = False\n",
    "\n",
    "    # ƒê·ªãnh d·∫°ng cho log message\n",
    "    formatter = logging.Formatter(\n",
    "        \"%(asctime)s - [%(levelname)s] - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "    )\n",
    "\n",
    "    # T·∫°o File Handler ƒë·ªÉ ghi log ra file\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    log_file_path = os.path.join(model_log_dir, f\"{timestamp}.log\")\n",
    "\n",
    "    file_handler = logging.FileHandler(log_file_path, encoding=\"utf-8\")\n",
    "    file_handler.setLevel(log_level)\n",
    "    file_handler.setFormatter(formatter)\n",
    "\n",
    "    # T·∫°o Console (Stream) Handler ƒë·ªÉ in log ra m√†n h√¨nh\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(log_level)\n",
    "    console_handler.setFormatter(formatter)\n",
    "\n",
    "    # Th√™m c√°c handler v√†o logger\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "    # L∆∞u logger v√†o cache\n",
    "    _loggers[model_name] = logger\n",
    "\n",
    "    logger.info(\n",
    "        f\"Logger cho '{safe_model_name}' ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o. File log: {log_file_path}\"\n",
    "    )\n",
    "\n",
    "    return logger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148d0474",
   "metadata": {},
   "source": [
    "## Setup logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eac9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 14:07:11 - [INFO] - Logger cho 'FacebookAI_xlm-roberta-large-training' ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o. File log: logs/FacebookAI_xlm-roberta-large-training/2025-10-17_14-07-11.log\n",
      "2025-10-17 14:07:11 - [INFO] - Logger initialized for FacebookAI/xlm-roberta-large\n",
      "2025-10-17 14:07:11 - [INFO] - ============================================================\n",
      "2025-10-17 14:07:11 - [INFO] - üöÄ STARTING TRAINING SESSION\n",
      "2025-10-17 14:07:11 - [INFO] - ============================================================\n",
      "2025-10-17 14:07:11 - [INFO] - ROOT_DIR: /home/guest/Projects/CS221\n",
      "2025-10-17 14:07:11 - [INFO] - DATA_DIR: /home/guest/Projects/CS221/data\n",
      "2025-10-17 14:07:11 - [INFO] - TRAIN_FILE: /home/guest/Projects/CS221/data/vihallu-train.csv\n",
      "2025-10-17 14:07:11 - [INFO] - TEST_FILE: /home/guest/Projects/CS221/data/vihallu-public-test.csv\n",
      "2025-10-17 14:07:11 - [INFO] - SUBMISSION_DIR: /home/guest/Projects/CS221/submission\n",
      "2025-10-17 14:07:11 - [INFO] - SUBMISSION_CSV: submit.csv\n",
      "2025-10-17 14:07:11 - [INFO] - SUBMISSION_ZIP: submit.zip\n",
      "2025-10-17 14:07:11 - [INFO] - MODEL_NAME: FacebookAI/xlm-roberta-large\n",
      "2025-10-17 14:07:11 - [INFO] - MODEL_OUTPUT_DIR: /home/guest/Projects/CS221/models/xlm-roberta-large-tuned\n",
      "2025-10-17 14:07:11 - [INFO] - MAX_LENGTH: 512\n",
      "2025-10-17 14:07:11 - [INFO] - RANDOM_STATE: 42\n",
      "2025-10-17 14:07:11 - [INFO] - EPOCHS: 10\n",
      "2025-10-17 14:07:11 - [INFO] - BATCH_SIZE: 4\n",
      "2025-10-17 14:07:11 - [INFO] - GRADIENT_ACCUMULATION_STEPS: 4\n",
      "2025-10-17 14:07:11 - [INFO] - SCHEDULER_TYPE: constant_with_warmup\n",
      "2025-10-17 14:07:11 - [INFO] - LEARNING_RATE: 8e-06\n",
      "2025-10-17 14:07:11 - [INFO] - WEIGHT_DECAY: 0.01\n",
      "2025-10-17 14:07:11 - [INFO] - NUM_CYCLES: 3\n",
      "2025-10-17 14:07:11 - [INFO] - CLASSIFIER_DROPOUT: 0.1\n",
      "2025-10-17 14:07:11 - [INFO] - LABEL_SMOOTHING: 0.05\n",
      "2025-10-17 14:07:11 - [INFO] - TOTAL_STEP_SCALE: 0.1\n",
      "2025-10-17 14:07:11 - [INFO] - EPSILON: 1e-08\n",
      "2025-10-17 14:07:11 - [INFO] - PATIENCE_LIMIT: 3\n",
      "2025-10-17 14:07:11 - [INFO] - VALIDATION_SPLIT_SIZE: 0.2\n",
      "2025-10-17 14:07:11 - [INFO] - LABEL_MAP: {'no': 0, 'extrinsic': 1, 'intrinsic': 2}\n",
      "2025-10-17 14:07:11 - [INFO] - ID2LABEL: {0: 'no', 1: 'extrinsic', 2: 'intrinsic'}\n",
      "2025-10-17 14:07:11 - [INFO] - CLASS_WEIGHTS: [1.0393466963622866, 1.0114145354717525, 0.9531590413943355]\n",
      "2025-10-17 14:07:11 - [INFO] - ============================================================\n",
      "2025-10-17 14:07:11 - [INFO] - Logger initialized for FacebookAI/xlm-roberta-large\n",
      "2025-10-17 14:07:11 - [INFO] - ============================================================\n",
      "2025-10-17 14:07:11 - [INFO] - üöÄ STARTING TRAINING SESSION\n",
      "2025-10-17 14:07:11 - [INFO] - ============================================================\n",
      "2025-10-17 14:07:11 - [INFO] - ROOT_DIR: /home/guest/Projects/CS221\n",
      "2025-10-17 14:07:11 - [INFO] - DATA_DIR: /home/guest/Projects/CS221/data\n",
      "2025-10-17 14:07:11 - [INFO] - TRAIN_FILE: /home/guest/Projects/CS221/data/vihallu-train.csv\n",
      "2025-10-17 14:07:11 - [INFO] - TEST_FILE: /home/guest/Projects/CS221/data/vihallu-public-test.csv\n",
      "2025-10-17 14:07:11 - [INFO] - SUBMISSION_DIR: /home/guest/Projects/CS221/submission\n",
      "2025-10-17 14:07:11 - [INFO] - SUBMISSION_CSV: submit.csv\n",
      "2025-10-17 14:07:11 - [INFO] - SUBMISSION_ZIP: submit.zip\n",
      "2025-10-17 14:07:11 - [INFO] - MODEL_NAME: FacebookAI/xlm-roberta-large\n",
      "2025-10-17 14:07:11 - [INFO] - MODEL_OUTPUT_DIR: /home/guest/Projects/CS221/models/xlm-roberta-large-tuned\n",
      "2025-10-17 14:07:11 - [INFO] - MAX_LENGTH: 512\n",
      "2025-10-17 14:07:11 - [INFO] - RANDOM_STATE: 42\n",
      "2025-10-17 14:07:11 - [INFO] - EPOCHS: 10\n",
      "2025-10-17 14:07:11 - [INFO] - BATCH_SIZE: 4\n",
      "2025-10-17 14:07:11 - [INFO] - GRADIENT_ACCUMULATION_STEPS: 4\n",
      "2025-10-17 14:07:11 - [INFO] - SCHEDULER_TYPE: constant_with_warmup\n",
      "2025-10-17 14:07:11 - [INFO] - LEARNING_RATE: 8e-06\n",
      "2025-10-17 14:07:11 - [INFO] - WEIGHT_DECAY: 0.01\n",
      "2025-10-17 14:07:11 - [INFO] - NUM_CYCLES: 3\n",
      "2025-10-17 14:07:11 - [INFO] - CLASSIFIER_DROPOUT: 0.1\n",
      "2025-10-17 14:07:11 - [INFO] - LABEL_SMOOTHING: 0.05\n",
      "2025-10-17 14:07:11 - [INFO] - TOTAL_STEP_SCALE: 0.1\n",
      "2025-10-17 14:07:11 - [INFO] - EPSILON: 1e-08\n",
      "2025-10-17 14:07:11 - [INFO] - PATIENCE_LIMIT: 3\n",
      "2025-10-17 14:07:11 - [INFO] - VALIDATION_SPLIT_SIZE: 0.2\n",
      "2025-10-17 14:07:11 - [INFO] - LABEL_MAP: {'no': 0, 'extrinsic': 1, 'intrinsic': 2}\n",
      "2025-10-17 14:07:11 - [INFO] - ID2LABEL: {0: 'no', 1: 'extrinsic', 2: 'intrinsic'}\n",
      "2025-10-17 14:07:11 - [INFO] - CLASS_WEIGHTS: [1.0393466963622866, 1.0114145354717525, 0.9531590413943355]\n",
      "2025-10-17 14:07:11 - [INFO] - ============================================================\n"
     ]
    }
   ],
   "source": [
    "logger = setup_logger(f\"{cfg.MODEL_NAME}-training\")\n",
    "logger.info(f\"Logger initialized for {cfg.MODEL_NAME}\")\n",
    "\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(\"üöÄ STARTING TRAINING SESSION\")\n",
    "logger.info(\"=\" * 60)\n",
    "for key, value in Config.__dict__.items():\n",
    "    if not key.startswith(\"__\") and not callable(value):\n",
    "        logger.info(f\"{key}: {value}\")\n",
    "logger.info(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c3b202",
   "metadata": {},
   "source": [
    "# Hallucination Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce606de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [6] - ƒê√É C·∫¨P NH·∫¨T\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class HallucinationDataset(Dataset):\n",
    "    def __init__(self, premises, hypotheses, labels, tokenizer, max_len):\n",
    "        self.premises = premises\n",
    "        self.hypotheses = hypotheses\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        premise = self.premises[idx]\n",
    "        hypothesis = self.hypotheses[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize b·∫±ng c√°ch truy·ªÅn 2 chu·ªói ri√™ng bi·ªát\n",
    "        # Tokenizer s·∫Ω t·ª± ƒë·ªông c·∫Øt b·ªõt `premise` n·∫øu c·∫ßn\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            premise,\n",
    "            hypothesis,  # <-- text_pair\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"],\n",
    "            \"attention_mask\": encoding[\"attention_mask\"],\n",
    "            \"labels\": label,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbc1878",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcfa808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def prepare_data(config, logger=None):\n",
    "    df = pd.read_csv(config.TRAIN_FILE)\n",
    "    print(f\"‚úÖ ƒê·ªçc th√†nh c√¥ng {len(df)} m·∫´u t·ª´ file ƒë√£ x·ª≠ l√Ω: {config.TRAIN_FILE}\")\n",
    "\n",
    "    # T·∫°o 2 c·ªôt premise v√† hypothesis t·ª´ ng·ªØ c·∫£nh (context)\n",
    "    df[\"premise\"] = (\n",
    "        \"C√¢u h·ªèi: \"\n",
    "        + df[\"prompt\"].astype(str)\n",
    "        + \" Ng·ªØ c·∫£nh: \"\n",
    "        + df[\"context\"].astype(str)\n",
    "    )\n",
    "    df[\"hypothesis\"] = df[\"response\"].astype(str)\n",
    "\n",
    "    df[\"label_id\"] = df[\"label\"].map(config.LABEL_MAP)\n",
    "    df.dropna(subset=[\"label_id\"], inplace=True)\n",
    "    df[\"label_id\"] = df[\"label_id\"].astype(int)\n",
    "\n",
    "    train_df, val_df = train_test_split(\n",
    "        df,\n",
    "        test_size=config.VALIDATION_SPLIT_SIZE,\n",
    "        random_state=config.RANDOM_STATE,\n",
    "        stratify=df[\"label_id\"],\n",
    "    )\n",
    "\n",
    "    if logger:\n",
    "        logger.info(\n",
    "            f\"Chia d·ªØ li·ªáu: {len(train_df)} m·∫´u train, {len(val_df)} m·∫´u validation.\"\n",
    "        )\n",
    "\n",
    "    # --- PH·∫¶N N√ÇNG C·∫§P: L∆ØU FILE RA TH∆Ø M·ª§C DATA ---\n",
    "    # T·∫°o th∆∞ m·ª•c 'processed' trong 'data' n·∫øu ch∆∞a c√≥\n",
    "    processed_data_dir = os.path.join(config.DATA_DIR, \"processed\")\n",
    "    os.makedirs(processed_data_dir, exist_ok=True)\n",
    "\n",
    "    # ƒê·ªãnh nghƒ©a ƒë∆∞·ªùng d·∫´n file\n",
    "    train_output_path = os.path.join(processed_data_dir, \"train_split.csv\")\n",
    "    val_output_path = os.path.join(processed_data_dir, \"validation_split.csv\")\n",
    "\n",
    "    # L∆∞u c√°c DataFrame\n",
    "    train_df.to_csv(train_output_path, index=False, encoding=\"utf-8-sig\")\n",
    "    val_df.to_csv(val_output_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u t·∫≠p train v√†o: {train_output_path}\")\n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u t·∫≠p validation v√†o: {val_output_path}\")\n",
    "    # --- K·∫æT TH√öC PH·∫¶N N√ÇNG C·∫§P ---\n",
    "\n",
    "    return train_df, val_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f432b434",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39c273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def get_model_and_tokenizer(config):\n",
    "    \"\"\"T·∫£i pre-trained model v√† tokenizer.\"\"\"\n",
    "    print(f\"ƒêang t·∫£i model: {config.MODEL_NAME}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\n",
    "\n",
    "    # T·∫£i config/model/tokenizer v·ªõi trust_remote_code=True ƒë·ªÉ cho ph√©p model custom\n",
    "    cfg = AutoConfig.from_pretrained(config.MODEL_NAME, trust_remote_code=True)\n",
    "    print(f\"Model config: {cfg}\")\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        config.MODEL_NAME, num_labels=len(config.LABEL_MAP)\n",
    "    )\n",
    "\n",
    "    # apply classifier dropout if provided in config\n",
    "    if hasattr(config, \"CLASSIFIER_DROPOUT\"):\n",
    "        if hasattr(model.config, \"classifier_dropout\"):\n",
    "            model.config.classifier_dropout = config.CLASSIFIER_DROPOUT\n",
    "        if hasattr(model.config, \"hidden_dropout_prob\"):\n",
    "            model.config.hidden_dropout_prob = config.CLASSIFIER_DROPOUT\n",
    "\n",
    "        if hasattr(model.config, \"attention_probs_dropout_prob\"):\n",
    "            model.config.attention_probs_dropout_prob = min(\n",
    "                0.15, max(0.1, config.CLASSIFIER_DROPOUT)\n",
    "            )\n",
    "        for m in model.modules():\n",
    "            if isinstance(m, nn.Dropout):\n",
    "                m.p = config.CLASSIFIER_DROPOUT\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21114019",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b10adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim import AdamW\n",
    "from huggingface_hub import login\n",
    "from transformers import get_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.contrib.logging import logging_redirect_tqdm\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e6336",
   "metadata": {},
   "source": [
    "## train one epoch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1154314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model,\n",
    "    data_loader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    device,\n",
    "    epoch=None,\n",
    "    total_epochs=None,\n",
    "    gradient_accumulation_steps=1,\n",
    "):\n",
    "    \"\"\"Hu·∫•n luy·ªán m√¥ h√¨nh trong m·ªôt epoch b·∫±ng gradient accumulation.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    desc = f\"Train\" if epoch is None else f\"Epoch {epoch}/{total_epochs}\"\n",
    "    progress_bar = tqdm(\n",
    "        data_loader, desc=desc, leave=False, dynamic_ncols=True, mininterval=0.5\n",
    "    )\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    steps_in_epoch = len(data_loader)\n",
    "    with logging_redirect_tqdm():  # make logger calls safe\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits  # shape (batch_size, num_labels)\n",
    "\n",
    "            loss = loss_fn(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            scaled_loss = loss / gradient_accumulation_steps\n",
    "            scaled_loss.backward()\n",
    "\n",
    "            if (step + 1) % gradient_accumulation_steps == 0 or (\n",
    "                step + 1\n",
    "            ) == steps_in_epoch:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    return total_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3398cf",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194e5a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, loss_fn, device):\n",
    "    \"\"\"ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p d·ªØ li·ªáu.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_val_loss = 0\n",
    "\n",
    "    progress_bar = tqdm(data_loader, desc=\"Evaluating\", leave=False, dynamic_ncols=True)\n",
    "\n",
    "    with torch.no_grad(), logging_redirect_tqdm():\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # <<< T√çNH LOSS TR√äN T·∫¨P VALIDATION\n",
    "            loss = loss_fn(logits, labels)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(data_loader)  # <<< T√çNH LOSS TRUNG B√åNH\n",
    "    return all_labels, all_preds, avg_val_loss  # <<< TR·∫¢ V·ªÄ TH√äM LOSS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08cfc13",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a07ea24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dotenv_path: /home/guest/Projects/CS221/envs/.env\n"
     ]
    }
   ],
   "source": [
    "# T·∫£i bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file envs/.env.\n",
    "dotenv_path = os.path.join(os.getcwd(), \"envs\", \".env\")\n",
    "load_dotenv(dotenv_path)\n",
    "print(f\"dotenv_path: {dotenv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c48a9b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: T√¨m th·∫•y HUGGING_FACE_TOKEN. ƒêang ƒëƒÉng nh·∫≠p...\n",
      "INFO: ƒêƒÉng nh·∫≠p Hugging Face th√†nh c√¥ng.\n"
     ]
    }
   ],
   "source": [
    "# l·∫•y HF token ƒë·ªÉ login\n",
    "hf_token = os.getenv(\"HUGGING_FACE_TOKEN\")\n",
    "\n",
    "if hf_token:\n",
    "    print(\"INFO: T√¨m th·∫•y HUGGING_FACE_TOKEN. ƒêang ƒëƒÉng nh·∫≠p...\")\n",
    "    login(token=hf_token)\n",
    "    print(\"INFO: ƒêƒÉng nh·∫≠p Hugging Face th√†nh c√¥ng.\")\n",
    "else:\n",
    "    print(\n",
    "        \"WARNING: Kh√¥ng t√¨m th·∫•y HUGGING_FACE_TOKEN trong file .env. M·ªôt s·ªë model c√≥ th·ªÉ y√™u c·∫ßu ƒëƒÉng nh·∫≠p.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950b94e0",
   "metadata": {},
   "source": [
    "## 1. Chu·∫©n b·ªã d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20c739cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 14:07:14 - [INFO] - B·∫Øt ƒë·∫ßu pipeline hu·∫•n luy·ªán.\n",
      "2025-10-17 14:07:14 - [INFO] - B∆∞·ªõc 1: Chu·∫©n b·ªã d·ªØ li·ªáu...\n",
      "2025-10-17 14:07:14 - [INFO] - Chia d·ªØ li·ªáu: 5600 m·∫´u train, 1400 m·∫´u validation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê·ªçc th√†nh c√¥ng 7000 m·∫´u t·ª´ file ƒë√£ x·ª≠ l√Ω: /home/guest/Projects/CS221/data/vihallu-train.csv\n",
      "‚úÖ ƒê√£ l∆∞u t·∫≠p train v√†o: /home/guest/Projects/CS221/data/processed/train_split.csv\n",
      "‚úÖ ƒê√£ l∆∞u t·∫≠p validation v√†o: /home/guest/Projects/CS221/data/processed/validation_split.csv\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"B·∫Øt ƒë·∫ßu pipeline hu·∫•n luy·ªán.\")\n",
    "\n",
    "# 1. Chu·∫©n b·ªã d·ªØ li·ªáu\n",
    "logger.info(\"B∆∞·ªõc 1: Chu·∫©n b·ªã d·ªØ li·ªáu...\")\n",
    "train_df, val_df = prepare_data(cfg, logger=logger)\n",
    "if train_df is None:\n",
    "    logger.error(\"D·ªØ li·ªáu kh√¥ng th·ªÉ ƒë∆∞·ª£c chu·∫©n b·ªã. D·ª´ng ch∆∞∆°ng tr√¨nh.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b8a1b7",
   "metadata": {},
   "source": [
    "## 2. T·∫£i model v√† tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68f4f00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 14:07:14 - [INFO] - B∆∞·ªõc 2: T·∫£i model 'FacebookAI/xlm-roberta-large' v√† tokenizer...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang t·∫£i model: FacebookAI/xlm-roberta-large\n",
      "Model config: XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"B∆∞·ªõc 2: T·∫£i model '{cfg.MODEL_NAME}' v√† tokenizer...\")\n",
    "model, tokenizer = get_model_and_tokenizer(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0416e4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torchinfo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa9d1648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 14:07:17 - [INFO] - Ph√¢n t√≠ch ki·∫øn tr√∫c m√¥ h√¨nh b·∫±ng torchinfo...\n",
      "2025-10-17 14:07:20 - [INFO] - Ki·∫øn tr√∫c chi ti·∫øt c·ªßa m√¥ h√¨nh:\n",
      "=====================================================================================================================================================================\n",
      "Layer (type:depth-idx)                                            Input Shape               Output Shape              Param #                   Mult-Adds\n",
      "=====================================================================================================================================================================\n",
      "XLMRobertaForSequenceClassification                               --                        [4, 3]                    --                        --\n",
      "‚îú‚îÄXLMRobertaModel: 1-1                                            [4, 512]                  [4, 512, 1024]            --                        --\n",
      "‚îÇ    ‚îî‚îÄXLMRobertaEmbeddings: 2-1                                  --                        [4, 512, 1024]            --                        --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄEmbedding: 3-1                                        [4, 512]                  [4, 512, 1024]            256,002,048               1,024,008,192\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄEmbedding: 3-2                                        [4, 512]                  [4, 512, 1024]            1,024                     4,096\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄEmbedding: 3-3                                        [4, 512]                  [4, 512, 1024]            526,336                   2,105,344\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-4                                        [4, 512, 1024]            [4, 512, 1024]            2,048                     8,192\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-5                                          [4, 512, 1024]            [4, 512, 1024]            --                        --\n",
      "‚îÇ    ‚îî‚îÄXLMRobertaEncoder: 2-2                                     [4, 512, 1024]            [4, 512, 1024]            --                        --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄModuleList: 3-6                                       --                        --                        302,309,376               --\n",
      "‚îú‚îÄXLMRobertaClassificationHead: 1-2                               [4, 512, 1024]            [4, 3]                    --                        --\n",
      "‚îÇ    ‚îî‚îÄDropout: 2-3                                               [4, 1024]                 [4, 1024]                 --                        --\n",
      "‚îÇ    ‚îî‚îÄLinear: 2-4                                                [4, 1024]                 [4, 1024]                 1,049,600                 4,198,400\n",
      "‚îÇ    ‚îî‚îÄDropout: 2-5                                               [4, 1024]                 [4, 1024]                 --                        --\n",
      "‚îÇ    ‚îî‚îÄLinear: 2-6                                                [4, 1024]                 [4, 3]                    3,075                     12,300\n",
      "=====================================================================================================================================================================\n",
      "Total params: 559,893,507\n",
      "Trainable params: 559,893,507\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 2.24\n",
      "=====================================================================================================================================================================\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 4496.33\n",
      "Params size (MB): 2239.57\n",
      "Estimated Total Size (MB): 6735.92\n",
      "=====================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "import torch  # ƒê·∫£m b·∫£o ƒë√£ import torch\n",
    "\n",
    "logger.info(\"Ph√¢n t√≠ch ki·∫øn tr√∫c m√¥ h√¨nh b·∫±ng torchinfo...\")\n",
    "\n",
    "# --- D√πng torchinfo ƒë·ªÉ hi·ªÉn th·ªã ---\n",
    "# T·∫°o m·ªôt input gi·∫£ v·ªõi batch_size v√† max_length nh∆∞ trong config\n",
    "input_ids_example = torch.randint(\n",
    "    0, tokenizer.vocab_size, (cfg.BATCH_SIZE, cfg.MAX_LENGTH)\n",
    ")\n",
    "\n",
    "# 1. G·ªçi summary v·ªõi verbose=0 ƒë·ªÉ kh√¥ng in ra console v√† l∆∞u k·∫øt qu·∫£ v√†o bi·∫øn\n",
    "#    Th√™m c√°c c·ªôt b·∫°n mu·ªën xem, v√≠ d·ª•: 'output_size', 'num_params'\n",
    "model_summary = summary(\n",
    "    model,\n",
    "    input_data={\"input_ids\": input_ids_example},\n",
    "    verbose=0,  # <-- Quan tr·ªçng: NgƒÉn kh√¥ng cho t·ª± ƒë·ªông in\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"mult_adds\"],\n",
    ")\n",
    "\n",
    "# 2. Chuy·ªÉn ƒë·ªëi t∆∞·ª£ng summary th√†nh string v√† ƒë∆∞a v√†o logger\n",
    "logger.info(f\"Ki·∫øn tr√∫c chi ti·∫øt c·ªßa m√¥ h√¨nh:\\n{str(model_summary)}\")\n",
    "\n",
    "\n",
    "# # (T√πy ch·ªçn) B·∫°n v·∫´n c√≥ th·ªÉ in ra m√†n h√¨nh n·∫øu mu·ªën xem ngay trong notebook\n",
    "# print(\"In summary ra m√†n h√¨nh notebook:\")\n",
    "# print(model_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ffd869",
   "metadata": {},
   "source": [
    "## 3. T·∫°o Dataset v√† DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9803da5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 14:07:20 - [INFO] - B∆∞·ªõc 3: T·∫°o Dataset v√† DataLoader...\n",
      "2025-10-17 14:07:20 - [INFO] - ‚úÖ T·∫°o DataLoader th√†nh c√¥ng v·ªõi DataCollatorWithPadding chu·∫©n!\n",
      "2025-10-17 14:07:20 - [INFO] - ‚úÖ T·∫°o DataLoader th√†nh c√¥ng v·ªõi DataCollatorWithPadding chu·∫©n!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding  # <-- 1. Import DataCollator\n",
    "\n",
    "# --- T·∫†O DATASET V√Ä DATALOADER ---\n",
    "logger.info(\"B∆∞·ªõc 3: T·∫°o Dataset v√† DataLoader...\")\n",
    "\n",
    "# T·∫°o Dataset (v·ªõi class HallucinationDataset ƒë√£ ƒë∆∞·ª£c ch·ªânh s·ª≠a ·ªü tr√™n)\n",
    "train_dataset = HallucinationDataset(\n",
    "    premises=train_df[\"premise\"].to_list(),\n",
    "    hypotheses=train_df[\"hypothesis\"].to_list(),\n",
    "    labels=train_df[\"label_id\"].to_list(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=cfg.MAX_LENGTH,\n",
    ")\n",
    "val_dataset = HallucinationDataset(\n",
    "    premises=val_df[\"premise\"].to_list(),\n",
    "    hypotheses=val_df[\"hypothesis\"].to_list(),\n",
    "    labels=val_df[\"label_id\"].to_list(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=cfg.MAX_LENGTH,\n",
    ")\n",
    "\n",
    "# 3. T·∫°o m·ªôt instance c·ªßa DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 4. T·∫°o DataLoader v√† truy·ªÅn data_collator v√†o\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=cfg.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,  # <-- D√πng data_collator ·ªü ƒë√¢y\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=cfg.BATCH_SIZE,\n",
    "    collate_fn=data_collator,  # <-- D√πng data_collator ·ªü ƒë√¢y\n",
    ")\n",
    "\n",
    "logger.info(\"‚úÖ T·∫°o DataLoader th√†nh c√¥ng v·ªõi DataCollatorWithPadding chu·∫©n!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00ac99c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 14:07:20 - [INFO] - Gradient accumulation steps: 4 | Effective batch size: 16\n"
     ]
    }
   ],
   "source": [
    "gradient_accumulation_steps = max(1, cfg.GRADIENT_ACCUMULATION_STEPS)\n",
    "effective_batch_size = cfg.BATCH_SIZE * gradient_accumulation_steps\n",
    "logger.info(\n",
    "    \"Gradient accumulation steps: %s | Effective batch size: %s\",\n",
    "    gradient_accumulation_steps,\n",
    "    effective_batch_size,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae8a4a1",
   "metadata": {},
   "source": [
    "### Check dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c8564a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ki·ªÉm tra 1 batch d·ªØ li·ªáu ƒë·∫ßu v√†o ---\n",
      "K√≠ch th∆∞·ªõc input_ids: torch.Size([4, 342])\n",
      "K√≠ch th∆∞·ªõc attention_mask: torch.Size([4, 342])\n",
      "Nh√£n trong batch: tensor([1, 1, 2, 0])\n",
      "\n",
      "M·ªôt m·∫´u ƒë√£ ƒë∆∞·ª£c token h√≥a v√† gi·∫£i m√£ l·∫°i:\n",
      "<s> C√¢u h·ªèi: S·ª± ƒë·ªëii ƒë√¢u cƒÉng th·∫≥ng xra gi·ªØax c√°c th·∫ø l·ª±c nao trong Mexicp? Ng·ªØ c·∫£nh: B√™n trong M√©xico, cƒÉng th·∫≥ng v·∫´n ti·∫øp di·ªÖn gi·ªØa phe li√™n bang ch·ªß nghƒ©a v√† phe trung ∆∞∆°ng t·∫≠p quy·ªÅn ch·ªß nghƒ©a. V√†o ƒë·∫ßu nƒÉm 1835, nh·ªØng ng∆∞·ªùi Texas th·∫≠n tr·ªçng ƒë√£ th√†nh l·∫≠p n√™n ·ª¶y ban T∆∞∆°ng ·ª©ng v√† An to√†n. T√¨nh tr·∫°ng n√°o ƒë·ªông b√πng ph√°t th√†nh xung ƒë·ªôt v≈© trang v√†o cu·ªëi nƒÉm 1835 t·∫°i tr·∫≠n Gonzales. S·ª± ki·ªán n√†y kh·ªüi ƒë·∫ßu C√°ch m·∫°ng Texas, v√† trong v√≤ng hai th√°ng sau ƒë√≥, ng∆∞·ªùi Texas ƒë√°nh b·∫°i t·∫•t c·∫£ c√°c ƒë·ªôi qu√¢n M√©xico t·∫°i khu v·ª±c. Ng∆∞·ªùi Texas b·∫ßu ra c√°c ƒë·∫°i di·ªán c·ªßa Consultation, th·ªÉ ch·∫ø n√†y l·∫≠p n√™n m·ªôt ch√≠nh ph·ªß l√¢m th·ªùi. Ch√≠nh ph·ªß l√¢m th·ªùi s·ª•p ƒë·ªï nhanh ch√≥ng do ƒë·∫•u tranh n·ªôi b·ªô, v√† Texas l√¢m v√†o t√¨nh tr·∫°ng kh√¥ng c√≥ s·ª± qu·∫£n l√Ω to√†n b·ªô trong hai th√°ng ƒë·∫ßu nƒÉm 1836.</s></s> Trong M√©xico, cƒÉng th·∫≥ng di·ªÖn ra gi·ªØa phe li√™n bang ch·ªß nghƒ©a v√† phe trung ∆∞∆°ng t·∫≠p quy·ªÅn ch·ªß nghƒ©a. Phe trung ∆∞∆°ng t·∫≠p quy·ªÅn ch·ªß y·∫øu ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi l√£nh ƒë·∫°o qu√¢n ƒë·ªôi Antonio L√≥pez de Santa Anna, ng∆∞·ªùi sau n√†y tr·ªü th√†nh m·ªôt trong nh·ªØng nh√¢n v·∫≠t quan tr·ªçng trong l·ªãch s·ª≠ M√©xico.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Ki·ªÉm tra 1 batch d·ªØ li·ªáu ƒë·∫ßu v√†o ---\")\n",
    "sample_batch = next(iter(train_loader))\n",
    "\n",
    "print(\"K√≠ch th∆∞·ªõc input_ids:\", sample_batch[\"input_ids\"].shape)\n",
    "print(\"K√≠ch th∆∞·ªõc attention_mask:\", sample_batch[\"attention_mask\"].shape)\n",
    "print(\"Nh√£n trong batch:\", sample_batch[\"labels\"])\n",
    "\n",
    "# Gi·∫£i m√£ m·ªôt m·∫´u ƒë·ªÉ xem n√≥ tr√¥ng nh∆∞ th·∫ø n√†o\n",
    "decoded_text = tokenizer.decode(sample_batch[\"input_ids\"][0], skip_special_tokens=False)\n",
    "print(\"\\nM·ªôt m·∫´u ƒë√£ ƒë∆∞·ª£c token h√≥a v√† gi·∫£i m√£ l·∫°i:\")\n",
    "print(decoded_text)\n",
    "print(\"------------------------------------------\\n\")\n",
    "# --- K·∫æT TH√öC B∆Ø·ªöC KI·ªÇM TRA ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fa66f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ki·ªÉm tra chi ti·∫øt 5 m·∫´u ƒë·∫ßu ti√™n ƒë·ªÉ so s√°nh tr∆∞·ªõc v√† sau khi x·ª≠ l√Ω ---\n",
      "\n",
      "=============== M·∫™U 0 ===============\n",
      "S·ªë token g·ªëc (∆∞·ªõc t√≠nh): 280\n",
      "S·ªë token sau khi x·ª≠ l√Ω (gi·ªõi h·∫°n b·ªüi max_len=512): 280\n",
      "‚úÖ  OK: ƒê·ªô d√†i m·∫´u n·∫±m trong gi·ªõi h·∫°n, kh√¥ng b·ªã c·∫Øt.\n",
      "\n",
      "--- VƒÉn b·∫£n G·ªêC  ---\n",
      "C√¢u h·ªèi: Chi·∫øn s·ª± nƒÉm 1950 ƒë√£ d√°nh b·∫°i qu√¢n Hoa K·ª≥ o chi√™n tr·∫°n n√†o? Ng·ªØ c·∫£nh: T·ª´ ng√†y 25 th√°ng 10 ƒë·∫øn ng√†y 5 th√°ng 11 (1950) l√† chi·∫øn d·ªãch ƒë·∫ßu ti√™n c·ªßa Trung Qu·ªëc. Qu√¢n Trung Qu·ªëc d√πng 2 s∆∞ ƒëo√†n c·ªßa qu√¢n ƒëo√†n 42 t·ªï ch·ª©c ph√≤ng ng·ª± ·ªü khu v·ª±c Ho√†ng Th·∫£o Lƒ©nh, Ph√≥ Chi·∫øn Lƒ©nh thu·ªôc m·∫∑t tr·∫≠n mi·ªÅn ƒë√¥ng, l·∫°i d√πng 3 qu√¢n ƒëo√†n v√† m·ªôt s∆∞ ƒëo√†n c·ªßa qu√¢n ƒëo√†n 42 (sau tƒÉng th√™m 2 qu√¢n ƒëo√†n) ph·∫£n k√≠ch ·ªü m·∫∑t tr·∫≠n mi·ªÅn T√¢y. Chi·∫øn d·ªãch n√†y ƒë√£ ƒë√°nh lui qu√¢n M·ªπ ƒë·∫øn ph√≠a nam s√¥ng Thanh Xuy√™n. Ng√†y 7 th√°ng 11, c√°c qu√¢n ƒëo√†n 20, 26, 27 thu·ªôc Binh ƒëo√†n 9 qu√¢n Ch√≠ nguy·ªán d∆∞·ªõi quy·ªÅn ch·ªâ huy c·ªßa T∆∞ l·ªánh ki√™m Ch√≠nh u·ª∑ T·ªëng Th·ªùi Lu√¢n ti·∫øn v√†o Tri·ªÅu Ti√™n. T·ªõi l√∫c n√†y, binh l·ª±c t√°c chi·∫øn c·ªßa Trung Qu·ªëc l√™n t·ªõi 9 qu√¢n ƒëo√†n g·ªìm 30 s∆∞ ƒëo√†n, t·ªïng c·ªông h∆°n 380.000 qu√¢n, chi·∫øm ∆∞u th·∫ø h∆°n so v·ªõi qu√¢n Li√™n H·ª£p Qu·ªëc g·ªìm 5 qu√¢n ƒëo√†n, 13 s∆∞ ƒëo√†n, 3 l·ªØ ƒëo√†n, t·ªïng c·ªông 220.000 qu√¢n. [SEP] Qu√¢n Trung Qu·ªëc ƒë√£ ƒë√°nh b·∫°i qu√¢n Hoa K·ª≥ t·∫°i ph√≠a b·∫Øc s√¥ng Thanh Xuy√™n, n∆°i m√† h·ªç ch·ªâ s·ª≠ d·ª•ng m·ªôt s∆∞ ƒëo√†n duy nh·∫•t ƒë·ªÉ ph·∫£n c√¥ng, m·∫∑c d√π l·ª±c l∆∞·ª£ng t·ªïng c·ªông ch·ªâ c√≥ 180.000 qu√¢n.\n",
      "\n",
      "--- VƒÉn b·∫£n SAU KHI DECODE t·ª´ input_ids ---\n",
      "<s> C√¢u h·ªèi: Chi·∫øn s·ª± nƒÉm 1950 ƒë√£ d√°nh b·∫°i qu√¢n Hoa K·ª≥ o chi√™n tr·∫°n n√†o? Ng·ªØ c·∫£nh: T·ª´ ng√†y 25 th√°ng 10 ƒë·∫øn ng√†y 5 th√°ng 11 (1950) l√† chi·∫øn d·ªãch ƒë·∫ßu ti√™n c·ªßa Trung Qu·ªëc. Qu√¢n Trung Qu·ªëc d√πng 2 s∆∞ ƒëo√†n c·ªßa qu√¢n ƒëo√†n 42 t·ªï ch·ª©c ph√≤ng ng·ª± ·ªü khu v·ª±c Ho√†ng Th·∫£o Lƒ©nh, Ph√≥ Chi·∫øn Lƒ©nh thu·ªôc m·∫∑t tr·∫≠n mi·ªÅn ƒë√¥ng, l·∫°i d√πng 3 qu√¢n ƒëo√†n v√† m·ªôt s∆∞ ƒëo√†n c·ªßa qu√¢n ƒëo√†n 42 (sau tƒÉng th√™m 2 qu√¢n ƒëo√†n) ph·∫£n k√≠ch ·ªü m·∫∑t tr·∫≠n mi·ªÅn T√¢y. Chi·∫øn d·ªãch n√†y ƒë√£ ƒë√°nh lui qu√¢n M·ªπ ƒë·∫øn ph√≠a nam s√¥ng Thanh Xuy√™n. Ng√†y 7 th√°ng 11, c√°c qu√¢n ƒëo√†n 20, 26, 27 thu·ªôc Binh ƒëo√†n 9 qu√¢n Ch√≠ nguy·ªán d∆∞·ªõi quy·ªÅn ch·ªâ huy c·ªßa T∆∞ l·ªánh ki√™m Ch√≠nh u·ª∑ T·ªëng Th·ªùi Lu√¢n ti·∫øn v√†o Tri·ªÅu Ti√™n. T·ªõi l√∫c n√†y, binh l·ª±c t√°c chi·∫øn c·ªßa Trung Qu·ªëc l√™n t·ªõi 9 qu√¢n ƒëo√†n g·ªìm 30 s∆∞ ƒëo√†n, t·ªïng c·ªông h∆°n 380.000 qu√¢n, chi·∫øm ∆∞u th·∫ø h∆°n so v·ªõi qu√¢n Li√™n H·ª£p Qu·ªëc g·ªìm 5 qu√¢n ƒëo√†n, 13 s∆∞ ƒëo√†n, 3 l·ªØ ƒëo√†n, t·ªïng c·ªông 220.000 qu√¢n.</s></s> Qu√¢n Trung Qu·ªëc ƒë√£ ƒë√°nh b·∫°i qu√¢n Hoa K·ª≥ t·∫°i ph√≠a b·∫Øc s√¥ng Thanh Xuy√™n, n∆°i m√† h·ªç ch·ªâ s·ª≠ d·ª•ng m·ªôt s∆∞ ƒëo√†n duy nh·∫•t ƒë·ªÉ ph·∫£n c√¥ng, m·∫∑c d√π l·ª±c l∆∞·ª£ng t·ªïng c·ªông ch·ªâ c√≥ 180.000 qu√¢n.</s>\n",
      "\n",
      "=============== M·∫™U 1 ===============\n",
      "S·ªë token g·ªëc (∆∞·ªõc t√≠nh): 264\n",
      "S·ªë token sau khi x·ª≠ l√Ω (gi·ªõi h·∫°n b·ªüi max_len=512): 264\n",
      "‚úÖ  OK: ƒê·ªô d√†i m·∫´u n·∫±m trong gi·ªõi h·∫°n, kh√¥ng b·ªã c·∫Øt.\n",
      "\n",
      "--- VƒÉn b·∫£n G·ªêC  ---\n",
      "C√¢u h·ªèi: T√≠nh c√°ch c·ªßa Edward c√≥ ph·∫£i l√† hi·ªÅn l√†nh v√† nh√¢n h·∫≠u, ƒë·∫∑c bi·ªát l√† khi √¥ng th∆∞·ªùng ƒë∆∞·ª£c m√¥ t·∫£ nh∆∞ m·ªôt ng∆∞·ªùi d·ªÖ g·∫ßn v√† khoan dung v·ªõi nh·ªØng ng∆∞·ªùi xung quanh kh√¥ng? Ng·ªØ c·∫£nh: Edward n·ªïi ti·∫øng l√† m·ªôt ng∆∞·ªùi kh·∫Øc nghi·ªát, v√† r·∫•t ƒë√°ng s·ª£; m·ªôt c√¢u chuy·ªán k·ªÉ v·ªÅ s·ª± ki·ªán Tr∆∞·ªüng Tu vi·ªán St Paul's, mu·ªën ƒë·ªëi ƒë·∫ßu v·ªõi Edward khi √¥ng tƒÉng thu·∫ø l√™n cao nƒÉm 1295, b·ªã ƒë·∫©y ng√£ t·ª´ tr√™n cao v√† ch·∫øt khi nh√† vua c√≥ m·∫∑t ·ªü ƒë√≥. Khi Edward x·ª© Caernarfon ƒë√≤i √¥ng phong cho s·ªßng nam c·ªßa h·∫Øn Gaveston m·ªôt l√£nh ƒë·ªãa b√° t∆∞·ªõc, nh√† vua n·ªïi c∆°n th·ªãnh n·ªô v√† r·ª©t t·ª´ng n·∫Øm t√≥c c·ªßa con trai √¥ng. M·ªôt s·ªë ng∆∞·ªùi ƒë∆∞∆°ng th·ªùi coi Edward l√† ng∆∞·ªùi ƒë√°ng s·ª£, ƒë·∫∑c bi·ªát l√† trong nh·ªØng ng√†y ƒë·∫ßu c·ªßa √¥ng. B√†i h√°t Lewes nƒÉm 1264 m√¥ t·∫£ √¥ng gi·ªëng nh∆∞ m·ªôt lo√†i b√°o, lo√†i ƒë·ªông v·∫≠t ƒë√°ng s·ª£, m·∫°nh m·∫Ω v√† kh√¥ng th·ªÉ l∆∞·ªùng tr∆∞·ªõc ƒë∆∞·ª£c. [SEP] Edward ƒë∆∞·ª£c bi·∫øt ƒë·∫øn l√† m·ªôt ng∆∞·ªùi hi·ªÅn l√†nh v√† nh√¢n h·∫≠u, th∆∞·ªùng xuy√™n ƒë∆∞·ª£c m√¥ t·∫£ nh∆∞ m·ªôt ng∆∞·ªùi d·ªÖ g·∫ßn v√† khoan dung. √îng lu√¥n ƒë·ªëi x·ª≠ nh·∫π nh√†ng v·ªõi nh·ªØng ng∆∞·ªùi b·∫•t ƒë·ªìng quan ƒëi·ªÉm, ph·∫£n √°nh s·ª± bao dung c·ªßa m√¨nh.\n",
      "\n",
      "--- VƒÉn b·∫£n SAU KHI DECODE t·ª´ input_ids ---\n",
      "<s> C√¢u h·ªèi: T√≠nh c√°ch c·ªßa Edward c√≥ ph·∫£i l√† hi·ªÅn l√†nh v√† nh√¢n h·∫≠u, ƒë·∫∑c bi·ªát l√† khi √¥ng th∆∞·ªùng ƒë∆∞·ª£c m√¥ t·∫£ nh∆∞ m·ªôt ng∆∞·ªùi d·ªÖ g·∫ßn v√† khoan dung v·ªõi nh·ªØng ng∆∞·ªùi xung quanh kh√¥ng? Ng·ªØ c·∫£nh: Edward n·ªïi ti·∫øng l√† m·ªôt ng∆∞·ªùi kh·∫Øc nghi·ªát, v√† r·∫•t ƒë√°ng s·ª£; m·ªôt c√¢u chuy·ªán k·ªÉ v·ªÅ s·ª± ki·ªán Tr∆∞·ªüng Tu vi·ªán St Paul's, mu·ªën ƒë·ªëi ƒë·∫ßu v·ªõi Edward khi √¥ng tƒÉng thu·∫ø l√™n cao nƒÉm 1295, b·ªã ƒë·∫©y ng√£ t·ª´ tr√™n cao v√† ch·∫øt khi nh√† vua c√≥ m·∫∑t ·ªü ƒë√≥. Khi Edward x·ª© Caernarfon ƒë√≤i √¥ng phong cho s·ªßng nam c·ªßa h·∫Øn Gaveston m·ªôt l√£nh ƒë·ªãa b√° t∆∞·ªõc, nh√† vua n·ªïi c∆°n th·ªãnh n·ªô v√† r·ª©t t·ª´ng n·∫Øm t√≥c c·ªßa con trai √¥ng. M·ªôt s·ªë ng∆∞·ªùi ƒë∆∞∆°ng th·ªùi coi Edward l√† ng∆∞·ªùi ƒë√°ng s·ª£, ƒë·∫∑c bi·ªát l√† trong nh·ªØng ng√†y ƒë·∫ßu c·ªßa √¥ng. B√†i h√°t Lewes nƒÉm 1264 m√¥ t·∫£ √¥ng gi·ªëng nh∆∞ m·ªôt lo√†i b√°o, lo√†i ƒë·ªông v·∫≠t ƒë√°ng s·ª£, m·∫°nh m·∫Ω v√† kh√¥ng th·ªÉ l∆∞·ªùng tr∆∞·ªõc ƒë∆∞·ª£c.</s></s> Edward ƒë∆∞·ª£c bi·∫øt ƒë·∫øn l√† m·ªôt ng∆∞·ªùi hi·ªÅn l√†nh v√† nh√¢n h·∫≠u, th∆∞·ªùng xuy√™n ƒë∆∞·ª£c m√¥ t·∫£ nh∆∞ m·ªôt ng∆∞·ªùi d·ªÖ g·∫ßn v√† khoan dung. √îng lu√¥n ƒë·ªëi x·ª≠ nh·∫π nh√†ng v·ªõi nh·ªØng ng∆∞·ªùi b·∫•t ƒë·ªìng quan ƒëi·ªÉm, ph·∫£n √°nh s·ª± bao dung c·ªßa m√¨nh.</s>\n",
      "\n",
      "=============== M·∫™U 2 ===============\n",
      "S·ªë token g·ªëc (∆∞·ªõc t√≠nh): 310\n",
      "S·ªë token sau khi x·ª≠ l√Ω (gi·ªõi h·∫°n b·ªüi max_len=512): 310\n",
      "‚úÖ  OK: ƒê·ªô d√†i m·∫´u n·∫±m trong gi·ªõi h·∫°n, kh√¥ng b·ªã c·∫Øt.\n",
      "\n",
      "--- VƒÉn b·∫£n G·ªêC  ---\n",
      "C√¢u h·ªèi: Nhung ham mo co duoc thiet ke va cham khac nhu the nao z? Ng·ªØ c·∫£nh: Roma c√≥ m·ªôt l∆∞·ª£ng l·ªõn h·∫ßm m·ªô c·ªï ho·∫∑c nh·ªØng n∆°i ch√¥n c·∫•t ng·∫ßm trong th√†nh ph·ªë hay g·∫ßn th√†nh ph·ªë, v·ªõi s·ªë l∆∞·ª£ng √≠t nh·∫•t 40, m·ªôt s·ªë v·ª´a ƒë∆∞·ª£c ph√°t hi·ªán ch·ªâ trong v√†i th·∫≠p k·ª∑ g·∫ßn ƒë√¢y. M·∫∑c d√π n·ªïi ti·∫øng nh·∫•t l√† nh·ªØng n∆°i ch√¥n c·∫•t Kit√¥ h·ªØu nh∆∞ng v·∫´n c√≥ m·ªô ngo·∫°i gi√°o v√† ng∆∞·ªùi Do Th√°i, ho·∫∑c ƒë∆∞·ª£c ch√¥n c·∫•t trong h·∫ßm m·ªô ri√™ng bi·ªát ho·∫∑c n·∫±m xen k·∫Ω chung v·ªõi nhau trong m·ªôt khu ƒë·∫•t. Nh·ªØng h·∫ßm m·ªô quy m√¥ l·ªõn ƒë·∫ßu ti√™n ƒë∆∞·ª£c khai qu·∫≠t t·ª´ th·∫ø k·ª∑ th·ª© hai tr·ªü ƒëi. Ban ƒë·∫ßu ch√∫ng ƒë∆∞·ª£c ch·∫°m kh·∫Øc t·ª´ m·ªôt lo·∫°i m·ªÅm t·ª´ tro n√∫i l·ª≠a l√† ƒë√° t√∫p v√† ƒë·∫∑t t·∫°i nh·ªØng v·ªã tr√≠ ngo√†i ranh gi·ªõi c·ªßa th√†nh ph·ªë v√¨ lu·∫≠t La M√£ c·∫•m ch√¥n c·∫•t trong th√†nh ph·ªë. Hi·ªán nay Gi√°o ho√†ng n·∫Øm quy·ªÅn v√† tr√°ch nhi·ªám b·∫£o tr√¨ c√°c h·∫ßm m·ªô. Gi√°o ho√†ng ƒë√£ trao quy·ªÅn cho D√≤ng Sal√™di√™ng Don Bosco trong vi·ªác gi√°m s√°t h·∫ßm m·ªô c·ªßa Th√°nh Callixtus ·ªü ngo·∫°i √¥ Roma. [SEP] Nh·ªØng h·∫ßm m·ªô c·ªï th∆∞·ªùng ƒë∆∞·ª£c thi·∫øt k·∫ø v·ªõi c√°c b√≠ch h·ªça v√† tranh kh·∫£m tinh x·∫£o, m√¥ t·∫£ c√°c c·∫£nh t√¥n gi√°o v√† bi·ªÉu t∆∞·ª£ng vƒÉn h√≥a. C√°c ngh·ªá nh√¢n th·ªùi ƒë√≥ s·ª≠ d·ª•ng k·ªπ thu·∫≠t ch·∫°m kh·∫Øc t·ªâ m·ªâ ƒë·ªÉ t·∫°o ra nh·ªØng h√†nh lang u·ªën l∆∞·ª£n v√† ph√≤ng ch√¥n c·∫•t ph·ª©c\n",
      "\n",
      "--- VƒÉn b·∫£n SAU KHI DECODE t·ª´ input_ids ---\n",
      "<s> C√¢u h·ªèi: Nhung ham mo co duoc thiet ke va cham khac nhu the nao z? Ng·ªØ c·∫£nh: Roma c√≥ m·ªôt l∆∞·ª£ng l·ªõn h·∫ßm m·ªô c·ªï ho·∫∑c nh·ªØng n∆°i ch√¥n c·∫•t ng·∫ßm trong th√†nh ph·ªë hay g·∫ßn th√†nh ph·ªë, v·ªõi s·ªë l∆∞·ª£ng √≠t nh·∫•t 40, m·ªôt s·ªë v·ª´a ƒë∆∞·ª£c ph√°t hi·ªán ch·ªâ trong v√†i th·∫≠p k·ª∑ g·∫ßn ƒë√¢y. M·∫∑c d√π n·ªïi ti·∫øng nh·∫•t l√† nh·ªØng n∆°i ch√¥n c·∫•t Kit√¥ h·ªØu nh∆∞ng v·∫´n c√≥ m·ªô ngo·∫°i gi√°o v√† ng∆∞·ªùi Do Th√°i, ho·∫∑c ƒë∆∞·ª£c ch√¥n c·∫•t trong h·∫ßm m·ªô ri√™ng bi·ªát ho·∫∑c n·∫±m xen k·∫Ω chung v·ªõi nhau trong m·ªôt khu ƒë·∫•t. Nh·ªØng h·∫ßm m·ªô quy m√¥ l·ªõn ƒë·∫ßu ti√™n ƒë∆∞·ª£c khai qu·∫≠t t·ª´ th·∫ø k·ª∑ th·ª© hai tr·ªü ƒëi. Ban ƒë·∫ßu ch√∫ng ƒë∆∞·ª£c ch·∫°m kh·∫Øc t·ª´ m·ªôt lo·∫°i m·ªÅm t·ª´ tro n√∫i l·ª≠a l√† ƒë√° t√∫p v√† ƒë·∫∑t t·∫°i nh·ªØng v·ªã tr√≠ ngo√†i ranh gi·ªõi c·ªßa th√†nh ph·ªë v√¨ lu·∫≠t La M√£ c·∫•m ch√¥n c·∫•t trong th√†nh ph·ªë. Hi·ªán nay Gi√°o ho√†ng n·∫Øm quy·ªÅn v√† tr√°ch nhi·ªám b·∫£o tr√¨ c√°c h·∫ßm m·ªô. Gi√°o ho√†ng ƒë√£ trao quy·ªÅn cho D√≤ng Sal√™di√™ng Don Bosco trong vi·ªác gi√°m s√°t h·∫ßm m·ªô c·ªßa Th√°nh Callixtus ·ªü ngo·∫°i √¥ Roma.</s></s> Nh·ªØng h·∫ßm m·ªô c·ªï th∆∞·ªùng ƒë∆∞·ª£c thi·∫øt k·∫ø v·ªõi c√°c b√≠ch h·ªça v√† tranh kh·∫£m tinh x·∫£o, m√¥ t·∫£ c√°c c·∫£nh t√¥n gi√°o v√† bi·ªÉu t∆∞·ª£ng vƒÉn h√≥a. C√°c ngh·ªá nh√¢n th·ªùi ƒë√≥ s·ª≠ d·ª•ng k·ªπ thu·∫≠t ch·∫°m kh·∫Øc t·ªâ m·ªâ ƒë·ªÉ t·∫°o ra nh·ªØng h√†nh lang u·ªën l∆∞·ª£n v√† ph√≤ng ch√¥n c·∫•t ph·ª©c</s>\n",
      "\n",
      "=============== M·∫™U 3 ===============\n",
      "S·ªë token g·ªëc (∆∞·ªõc t√≠nh): 303\n",
      "S·ªë token sau khi x·ª≠ l√Ω (gi·ªõi h·∫°n b·ªüi max_len=512): 303\n",
      "‚úÖ  OK: ƒê·ªô d√†i m·∫´u n·∫±m trong gi·ªõi h·∫°n, kh√¥ng b·ªã c·∫Øt.\n",
      "\n",
      "--- VƒÉn b·∫£n G·ªêC  ---\n",
      "C√¢u h·ªèi: Th·ªùi gian ng∆∞·ªùi d√¢n H√†n Qu·ªëc ph·∫£i lao ƒë·ªông trong kho·∫£ng th·ªùi gian 1960-1970 l√† bao l√¢u? Ng·ªØ c·∫£nh: K·∫ø ho·∫°ch ph√°t tri·ªÉn kinh t·∫ø c·ªßa H√†n Qu·ªëc trong th·∫≠p ni√™n 1960-1970 d·ª±a v√†o xu·∫•t kh·∫©u nh·ªù gi√° th√†nh th·∫•p. Chi ph√≠ s·∫£n xu·∫•t ƒë∆∞·ª£c c·ªë t√¨nh h·∫° th·∫•p ƒë·∫øn m·ª©c t·ªëi thi·ªÉu b·∫±ng c√°ch to√†n d√¢n ph·∫£i cam ch·ªãu gian kh·ªï, ti√™u d√πng h·∫øt s·ª©c ti·∫øt ki·ªám, ch√≠nh s√°ch to√†n qu·ªëc th·∫Øt l∆∞ng bu·ªôc b·ª•ng ƒë∆∞·ª£c √°p d·ª•ng. C√°c s·∫£n ph·∫©m xa x·ªâ nh∆∞ m·ªπ ph·∫©m, oto cao c·∫•p, qu·∫ßn √°o th·ªùi trang, tivi m√†u... b·ªã h·∫°n ch·∫ø nh·∫≠p kh·∫©u ·ªü m·ª©c t·ªëi ƒëa. Ng∆∞·ªùi d√¢n l√†m vi·ªác n·∫∑ng nh·ªçc v√† tri·ªÅn mi√™n, nh∆∞ng s·ªëng kham kh·ªï. H√†ng tu·∫ßn m·ªói ng∆∞·ªùi d√¢n ƒë·ªÅu ph·∫£i nh·ªãn ƒÉn m·ªôt b·ªØa, kh√¥ng h√∫t thu·ªëc ngo·∫°i nh·∫≠p, kh√¥ng u·ªëng c√† ph√™. Th·ªùi gian lao ƒë·ªông k√©o d√†i ƒë·∫øn 12-14 ti·∫øng m·ªói ng√†y. ƒêi·ªÅu ki·ªán lao ƒë·ªông k√©m, l∆∞∆°ng r·∫•t th·∫•p. C√°c ngu·ªìn t√†i ch√≠nh c√≥ ƒë∆∞·ª£c nh·ªù ch√≠nh s√°ch ti·∫øt ki·ªám ƒë·∫øn m·ª©c kham kh·ªï l·∫°i ƒë∆∞·ª£c t√°i ƒë·∫ßu t∆∞ v√†o s·∫£n xu·∫•t. Phong tr√†o Saemaeul (c√≤n g·ªçi l√† Phong tr√†o c·ªông ƒë·ªìng c∆∞ d√¢n m·ªõi) c·ªßa Ch√≠nh ph·ªß t·∫≠p trung v√†o ph√°t tri·ªÉn n√¥ng th√¥n H√†n Qu·ªëc b·∫±ng vi·ªác ƒë·ªông vi√™n ng∆∞·ªùi d√¢n lao ƒë·ªông c√¥ng √≠ch, c·∫£i t·∫°o c∆° s·ªü h·∫° t·∫ßng m√† kh√¥ng c·∫ßn ƒë∆∞·ª£c tr·∫£ l∆∞∆°ng. [SEP] Ng∆∞·ªùi d√¢n H√†n Qu·ªëc trong kho·∫£ng th·ªùi gian 1960-1970 ph·∫£i lao ƒë·ªông k√©o d√†i t·ª´ 12-14 ti·∫øng m·ªói ng√†y.\n",
      "\n",
      "--- VƒÉn b·∫£n SAU KHI DECODE t·ª´ input_ids ---\n",
      "<s> C√¢u h·ªèi: Th·ªùi gian ng∆∞·ªùi d√¢n H√†n Qu·ªëc ph·∫£i lao ƒë·ªông trong kho·∫£ng th·ªùi gian 1960-1970 l√† bao l√¢u? Ng·ªØ c·∫£nh: K·∫ø ho·∫°ch ph√°t tri·ªÉn kinh t·∫ø c·ªßa H√†n Qu·ªëc trong th·∫≠p ni√™n 1960-1970 d·ª±a v√†o xu·∫•t kh·∫©u nh·ªù gi√° th√†nh th·∫•p. Chi ph√≠ s·∫£n xu·∫•t ƒë∆∞·ª£c c·ªë t√¨nh h·∫° th·∫•p ƒë·∫øn m·ª©c t·ªëi thi·ªÉu b·∫±ng c√°ch to√†n d√¢n ph·∫£i cam ch·ªãu gian kh·ªï, ti√™u d√πng h·∫øt s·ª©c ti·∫øt ki·ªám, ch√≠nh s√°ch to√†n qu·ªëc th·∫Øt l∆∞ng bu·ªôc b·ª•ng ƒë∆∞·ª£c √°p d·ª•ng. C√°c s·∫£n ph·∫©m xa x·ªâ nh∆∞ m·ªπ ph·∫©m, oto cao c·∫•p, qu·∫ßn √°o th·ªùi trang, tivi m√†u... b·ªã h·∫°n ch·∫ø nh·∫≠p kh·∫©u ·ªü m·ª©c t·ªëi ƒëa. Ng∆∞·ªùi d√¢n l√†m vi·ªác n·∫∑ng nh·ªçc v√† tri·ªÅn mi√™n, nh∆∞ng s·ªëng kham kh·ªï. H√†ng tu·∫ßn m·ªói ng∆∞·ªùi d√¢n ƒë·ªÅu ph·∫£i nh·ªãn ƒÉn m·ªôt b·ªØa, kh√¥ng h√∫t thu·ªëc ngo·∫°i nh·∫≠p, kh√¥ng u·ªëng c√† ph√™. Th·ªùi gian lao ƒë·ªông k√©o d√†i ƒë·∫øn 12-14 ti·∫øng m·ªói ng√†y. ƒêi·ªÅu ki·ªán lao ƒë·ªông k√©m, l∆∞∆°ng r·∫•t th·∫•p. C√°c ngu·ªìn t√†i ch√≠nh c√≥ ƒë∆∞·ª£c nh·ªù ch√≠nh s√°ch ti·∫øt ki·ªám ƒë·∫øn m·ª©c kham kh·ªï l·∫°i ƒë∆∞·ª£c t√°i ƒë·∫ßu t∆∞ v√†o s·∫£n xu·∫•t. Phong tr√†o Saemaeul (c√≤n g·ªçi l√† Phong tr√†o c·ªông ƒë·ªìng c∆∞ d√¢n m·ªõi) c·ªßa Ch√≠nh ph·ªß t·∫≠p trung v√†o ph√°t tri·ªÉn n√¥ng th√¥n H√†n Qu·ªëc b·∫±ng vi·ªác ƒë·ªông vi√™n ng∆∞·ªùi d√¢n lao ƒë·ªông c√¥ng √≠ch, c·∫£i t·∫°o c∆° s·ªü h·∫° t·∫ßng m√† kh√¥ng c·∫ßn ƒë∆∞·ª£c tr·∫£ l∆∞∆°ng.</s></s> Ng∆∞·ªùi d√¢n H√†n Qu·ªëc trong kho·∫£ng th·ªùi gian 1960-1970 ph·∫£i lao ƒë·ªông k√©o d√†i t·ª´ 12-14 ti·∫øng m·ªói ng√†y.</s>\n",
      "\n",
      "=============== M·∫™U 4 ===============\n",
      "S·ªë token g·ªëc (∆∞·ªõc t√≠nh): 345\n",
      "S·ªë token sau khi x·ª≠ l√Ω (gi·ªõi h·∫°n b·ªüi max_len=512): 345\n",
      "‚úÖ  OK: ƒê·ªô d√†i m·∫´u n·∫±m trong gi·ªõi h·∫°n, kh√¥ng b·ªã c·∫Øt.\n",
      "\n",
      "--- VƒÉn b·∫£n G·ªêC  ---\n",
      "C√¢u h·ªèi: Sau khi Nga t·ª´ ch·ªëi b√°n v≈© kh√≠ v√† trong b·ªëi c·∫£nh Trung Qu·ªëc t·ª´ l√¢u ƒë√£ c√≥ l·ªãch s·ª≠ h·ª£p t√°c qu√¢n s·ª± ch·∫∑t ch·∫Ω v·ªõi c√°c n∆∞·ªõc NATO, Trung Qu·ªëc ƒë√£ chuy·ªÉn h∆∞·ªõng sang mua v≈© kh√≠ t·ª´ n∆∞·ªõc n√†o? Ng·ªØ c·∫£nh: Khoa h·ªçc v√† k·ªπ thu·∫≠t trong C√¥ng nghi·ªáp qu·ªëc ph√≤ng c·ªßa C·ªông h√≤a nh√¢n d√¢n Trung Hoa h·∫ßu h·∫øt ƒë∆∞·ª£c ƒë·∫∑t n·ªÅn m√≥ng khi Li√™n X√¥ ƒë·∫ßu t∆∞ m·∫°nh m·∫Ω v√†o Trung Qu·ªëc v√†o nh·ªØng nƒÉm 1950. V√† ph·∫ßn l·ªõn c√°c v≈© kh√≠ quan tr·ªçng c·ªßa Li√™n X√¥ ƒë√£ ƒë∆∞·ª£c c·∫•p gi·∫•y ph√©p ƒë·ªÉ s·∫£n xu·∫•t t·∫°i Trung Qu·ªëc. C≈©ng nh∆∞ Li√™n X√¥ ƒë√£ gi√∫p ƒë·ª° ph√°t tri·ªÉn c√¥ng ngh·ªá h·∫°t nh√¢n v√† v≈© kh√≠ nguy√™n t·ª≠ t·∫°i Trung Qu·ªëc. CHND Trung Hoa c≈©ng ƒë√£ c√≥ ƒë∆∞·ª£c m·ªôt s·ªë c√¥ng ngh·ªá c·ªßa Hoa K·ª≥ khi m·ªëi quan h·ªá gi·ªØa hai n∆∞·ªõc tr·ªü n√™n n·ªìng ·∫•m v√†o nh·ªØng nƒÉm 1970. C≈©ng nh∆∞ Trung Qu·ªëc b·∫Øt ƒë·∫ßu sao ch√©p nh·ªØng v≈© kh√≠ m√† m√¨nh mua ƒë∆∞·ª£c t·ª´ ph∆∞∆°ng T√¢y nh∆∞ng kh√¥ng nhi·ªÅu do c√°c n∆∞·ªõc ph∆∞∆°ng T√¢y th·∫≠n tr·ªçng h∆°n trong vi·ªác mua b√°n v≈© kh√≠ v·ªõi Trung Qu·ªëc c≈©ng nh∆∞ b·ªã c·∫•m v·∫≠n v≈© kh√≠ v√†o nƒÉm 1989. ƒê·∫øn nh·ªØng nƒÉm 1990 th√¨ Trung Qu·ªëc b·∫Øt ƒë·∫ßu sao ch√©p quy m√¥ l·ªõn c√°c v≈© kh√≠ hi·ªán ƒë·∫°i mua ƒë∆∞·ª£c t·ª´ Nga. C√≤n khi Nga t·ª´ ch·ªëi b√°n c√°c lo·∫°i v≈© kh√≠ c·ªßa m√¨nh th√¨ Trung Qu·ªëc chuy·ªÉn sang mua c·ªßa Ukraina v·ªën c≈©ng s·ªü h·ªØu nhi·ªÅu lo·∫°i v≈© kh√≠ hi·ªán ƒë·∫°i t·ª´ th·ªùi Li√™n X√¥. Hi·ªán t·∫°i th√¨ Trung Qu·ªëc ƒëang t√≠ch c·ª±c sao ch√©p c√°c lo·∫°i v≈© kh√≠ c·ªßa ph∆∞∆°ng T√¢y mua ƒë∆∞·ª£c t·ª´ Israel. [SEP] Trung Qu·ªëc ƒë√£ chuy·ªÉn sang h·ª£p t√°c mua v≈© kh√≠ t·ª´ Brazil, qu·ªëc gia kh√¥ng c√≥ l·ªãch s·ª≠ h·ª£p t√°c qu√¢n s·ª± ch·∫∑t ch·∫Ω v·ªõi Trung Qu·ªëc v√† kh√¥ng n·∫±m trong li√™n minh NATO.\n",
      "\n",
      "--- VƒÉn b·∫£n SAU KHI DECODE t·ª´ input_ids ---\n",
      "<s> C√¢u h·ªèi: Sau khi Nga t·ª´ ch·ªëi b√°n v≈© kh√≠ v√† trong b·ªëi c·∫£nh Trung Qu·ªëc t·ª´ l√¢u ƒë√£ c√≥ l·ªãch s·ª≠ h·ª£p t√°c qu√¢n s·ª± ch·∫∑t ch·∫Ω v·ªõi c√°c n∆∞·ªõc NATO, Trung Qu·ªëc ƒë√£ chuy·ªÉn h∆∞·ªõng sang mua v≈© kh√≠ t·ª´ n∆∞·ªõc n√†o? Ng·ªØ c·∫£nh: Khoa h·ªçc v√† k·ªπ thu·∫≠t trong C√¥ng nghi·ªáp qu·ªëc ph√≤ng c·ªßa C·ªông h√≤a nh√¢n d√¢n Trung Hoa h·∫ßu h·∫øt ƒë∆∞·ª£c ƒë·∫∑t n·ªÅn m√≥ng khi Li√™n X√¥ ƒë·∫ßu t∆∞ m·∫°nh m·∫Ω v√†o Trung Qu·ªëc v√†o nh·ªØng nƒÉm 1950. V√† ph·∫ßn l·ªõn c√°c v≈© kh√≠ quan tr·ªçng c·ªßa Li√™n X√¥ ƒë√£ ƒë∆∞·ª£c c·∫•p gi·∫•y ph√©p ƒë·ªÉ s·∫£n xu·∫•t t·∫°i Trung Qu·ªëc. C≈©ng nh∆∞ Li√™n X√¥ ƒë√£ gi√∫p ƒë·ª° ph√°t tri·ªÉn c√¥ng ngh·ªá h·∫°t nh√¢n v√† v≈© kh√≠ nguy√™n t·ª≠ t·∫°i Trung Qu·ªëc. CHND Trung Hoa c≈©ng ƒë√£ c√≥ ƒë∆∞·ª£c m·ªôt s·ªë c√¥ng ngh·ªá c·ªßa Hoa K·ª≥ khi m·ªëi quan h·ªá gi·ªØa hai n∆∞·ªõc tr·ªü n√™n n·ªìng ·∫•m v√†o nh·ªØng nƒÉm 1970. C≈©ng nh∆∞ Trung Qu·ªëc b·∫Øt ƒë·∫ßu sao ch√©p nh·ªØng v≈© kh√≠ m√† m√¨nh mua ƒë∆∞·ª£c t·ª´ ph∆∞∆°ng T√¢y nh∆∞ng kh√¥ng nhi·ªÅu do c√°c n∆∞·ªõc ph∆∞∆°ng T√¢y th·∫≠n tr·ªçng h∆°n trong vi·ªác mua b√°n v≈© kh√≠ v·ªõi Trung Qu·ªëc c≈©ng nh∆∞ b·ªã c·∫•m v·∫≠n v≈© kh√≠ v√†o nƒÉm 1989. ƒê·∫øn nh·ªØng nƒÉm 1990 th√¨ Trung Qu·ªëc b·∫Øt ƒë·∫ßu sao ch√©p quy m√¥ l·ªõn c√°c v≈© kh√≠ hi·ªán ƒë·∫°i mua ƒë∆∞·ª£c t·ª´ Nga. C√≤n khi Nga t·ª´ ch·ªëi b√°n c√°c lo·∫°i v≈© kh√≠ c·ªßa m√¨nh th√¨ Trung Qu·ªëc chuy·ªÉn sang mua c·ªßa Ukraina v·ªën c≈©ng s·ªü h·ªØu nhi·ªÅu lo·∫°i v≈© kh√≠ hi·ªán ƒë·∫°i t·ª´ th·ªùi Li√™n X√¥. Hi·ªán t·∫°i th√¨ Trung Qu·ªëc ƒëang t√≠ch c·ª±c sao ch√©p c√°c lo·∫°i v≈© kh√≠ c·ªßa ph∆∞∆°ng T√¢y mua ƒë∆∞·ª£c t·ª´ Israel.</s></s> Trung Qu·ªëc ƒë√£ chuy·ªÉn sang h·ª£p t√°c mua v≈© kh√≠ t·ª´ Brazil, qu·ªëc gia kh√¥ng c√≥ l·ªãch s·ª≠ h·ª£p t√°c qu√¢n s·ª± ch·∫∑t ch·∫Ω v·ªõi Trung Qu·ªëc v√† kh√¥ng n·∫±m trong li√™n minh NATO.</s>\n",
      "\n",
      "===========================================\n",
      "Ki·ªÉm tra ho√†n t·∫•t. H√£y so s√°nh vƒÉn b·∫£n tr√™n ƒë·ªÉ xem c√≥ s·ª± kh√°c bi·ªát ·ªü cu·ªëi chu·ªói kh√¥ng.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Ki·ªÉm tra chi ti·∫øt 5 m·∫´u ƒë·∫ßu ti√™n ƒë·ªÉ so s√°nh tr∆∞·ªõc v√† sau khi x·ª≠ l√Ω ---\")\n",
    "\n",
    "# L·∫•y 5 m·∫´u ƒë·∫ßu ti√™n t·ª´ DataFrame g·ªëc ƒë·ªÉ so s√°nh\n",
    "num_samples_to_check = 5\n",
    "for i in range(num_samples_to_check):\n",
    "    print(f\"\\n=============== M·∫™U {i} ===============\")\n",
    "\n",
    "    # 1. L·∫•y d·ªØ li·ªáu g·ªëc t·ª´ DataFrame\n",
    "    original_premise = train_df[\"premise\"].iloc[i]\n",
    "    original_hypothesis = train_df[\"hypothesis\"].iloc[i]\n",
    "    # N·ªëi 2 chu·ªói l·∫°i gi·ªëng c√°ch tokenizer s·∫Ω th·∫•y ch√∫ng\n",
    "    original_combined_text = original_premise + \" [SEP] \" + original_hypothesis\n",
    "\n",
    "    # 2. L·∫•y d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω t·ª´ Dataset\n",
    "    processed_sample = train_dataset[i]\n",
    "    processed_input_ids = processed_sample[\"input_ids\"]\n",
    "\n",
    "    # 3. Gi·∫£i m√£ (decode) c√°c input_ids ƒë√£ x·ª≠ l√Ω tr·ªü l·∫°i th√†nh vƒÉn b·∫£n\n",
    "    decoded_text = tokenizer.decode(processed_input_ids, skip_special_tokens=False)\n",
    "\n",
    "    # 4. So s√°nh v√† in k·∫øt qu·∫£\n",
    "    original_token_count = len(tokenizer.encode(original_premise, original_hypothesis))\n",
    "    processed_token_count = len(processed_input_ids)\n",
    "\n",
    "    print(f\"S·ªë token g·ªëc (∆∞·ªõc t√≠nh): {original_token_count}\")\n",
    "    print(\n",
    "        f\"S·ªë token sau khi x·ª≠ l√Ω (gi·ªõi h·∫°n b·ªüi max_len={cfg.MAX_LENGTH}): {processed_token_count}\"\n",
    "    )\n",
    "\n",
    "    if original_token_count > cfg.MAX_LENGTH:\n",
    "        print(\"‚ö†Ô∏è  C·∫¢NH B√ÅO: M·∫´u n√†y ƒë√£ b·ªã c·∫Øt b·ªõt (truncated)!\")\n",
    "    else:\n",
    "        print(\"‚úÖ  OK: ƒê·ªô d√†i m·∫´u n·∫±m trong gi·ªõi h·∫°n, kh√¥ng b·ªã c·∫Øt.\")\n",
    "\n",
    "    print(\"\\n--- VƒÉn b·∫£n G·ªêC  ---\")\n",
    "    print(original_combined_text)\n",
    "\n",
    "    print(\"\\n--- VƒÉn b·∫£n SAU KHI DECODE t·ª´ input_ids ---\")\n",
    "    print(decoded_text)\n",
    "\n",
    "print(\"\\n===========================================\")\n",
    "print(\n",
    "    \"Ki·ªÉm tra ho√†n t·∫•t. H√£y so s√°nh vƒÉn b·∫£n tr√™n ƒë·ªÉ xem c√≥ s·ª± kh√°c bi·ªát ·ªü cu·ªëi chu·ªói kh√¥ng.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2739f76b",
   "metadata": {},
   "source": [
    "## 4. Thi·∫øt l·∫≠p Hu·∫•n luy·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62005cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 14:07:20 - [INFO] - B∆∞·ªõc 4: Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng hu·∫•n luy·ªán v√† ki·∫øn tr√∫c model...\n",
      "2025-10-17 14:07:20 - [INFO] - S·ª≠ d·ª•ng thi·∫øt b·ªã: cuda\n",
      "2025-10-17 14:07:20 - [INFO] - ‚úÖ T√¨m th·∫•y 1 GPU(s).\n",
      "2025-10-17 14:07:20 - [INFO] - ‚úÖ ƒêang s·ª≠ d·ª•ng GPU: NVIDIA GeForce RTX 5070 Ti\n",
      "2025-10-17 14:07:20 - [INFO] - Ki·∫øn tr√∫c c·ªßa m√¥ h√¨nh:\n",
      "XLMRobertaForSequenceClassification(\n",
      "  (roberta): XLMRobertaModel(\n",
      "    (embeddings): XLMRobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): XLMRobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-23): 24 x XLMRobertaLayer(\n",
      "          (attention): XLMRobertaAttention(\n",
      "            (self): XLMRobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): XLMRobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): XLMRobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): XLMRobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): XLMRobertaClassificationHead(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): XLMRobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch  # ƒê·∫£m b·∫£o ƒë√£ import torch\n",
    "\n",
    "logger.info(\"B∆∞·ªõc 4: Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng hu·∫•n luy·ªán v√† ki·∫øn tr√∫c model...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Log th√¥ng tin thi·∫øt b·ªã (GPU/CPU) ---\n",
    "logger.info(f\"S·ª≠ d·ª•ng thi·∫øt b·ªã: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    logger.info(f\"‚úÖ T√¨m th·∫•y {gpu_count} GPU(s).\")\n",
    "    logger.info(f\"‚úÖ ƒêang s·ª≠ d·ª•ng GPU: {gpu_name}\")\n",
    "else:\n",
    "    logger.warning(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y GPU, s·ª≠ d·ª•ng CPU. Qu√° tr√¨nh training s·∫Ω r·∫•t ch·∫≠m.\")\n",
    "\n",
    "# --- B·∫ÆT ƒê·∫¶U PH·∫¶N TH√äM M·ªöI ---\n",
    "# Chuy·ªÉn to√†n b·ªô ki·∫øn tr√∫c model th√†nh d·∫°ng string ƒë·ªÉ ƒë∆∞a v√†o logger\n",
    "model_architecture_string = str(model)\n",
    "\n",
    "# Ghi log ki·∫øn tr√∫c model\n",
    "logger.info(f\"Ki·∫øn tr√∫c c·ªßa m√¥ h√¨nh:\\n{model_architecture_string}\")\n",
    "# --- K·∫æT TH√öC PH·∫¶N TH√äM M·ªöI ---\n",
    "\n",
    "# Di chuy·ªÉn model ƒë·∫øn device ƒë√£ ch·ªçn\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8449d146",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=cfg.LEARNING_RATE,\n",
    "    weight_decay=cfg.WEIGHT_DECAY,\n",
    "    eps=cfg.EPSILON,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a39f57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 14:07:21 - [INFO] - Scheduler will run for 3500 total steps (350 per epoch)\n",
      "2025-10-17 14:07:21 - [INFO] - S·ª≠ d·ª•ng scheduler chung: constant_with_warmup\n",
      "2025-10-17 14:07:21 - [INFO] - Warmup steps: 350\n",
      "2025-10-17 14:07:21 - [INFO] - S·ª≠ d·ª•ng scheduler chung: constant_with_warmup\n",
      "2025-10-17 14:07:21 - [INFO] - Warmup steps: 350\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    get_scheduler,\n",
    "    get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "num_update_steps_per_epoch = math.ceil(len(train_loader) / gradient_accumulation_steps)\n",
    "num_training_steps = num_update_steps_per_epoch * cfg.EPOCHS\n",
    "logger.info(\n",
    "    \"Scheduler will run for %s total steps (%s per epoch)\",\n",
    "    num_training_steps,\n",
    "    num_update_steps_per_epoch,\n",
    ")\n",
    "\n",
    "if cfg.TOTAL_STEP_SCALE <= 0:\n",
    "    warmup_steps = 0\n",
    "elif cfg.TOTAL_STEP_SCALE <= 1:\n",
    "    warmup_steps = max(1, int(cfg.TOTAL_STEP_SCALE * num_training_steps))\n",
    "else:\n",
    "    warmup_steps = min(int(cfg.TOTAL_STEP_SCALE), num_training_steps)\n",
    "\n",
    "# <<< TH√äM KH·ªêI L·ªÜNH IF ƒê·ªÇ X·ª¨ L√ù TR∆Ø·ªúNG H·ª¢P ƒê·∫∂C BI·ªÜT\n",
    "if cfg.SCHEDULER_TYPE == \"cosine_with_restarts\":\n",
    "    logger.info(\n",
    "        f\"S·ª≠ d·ª•ng scheduler chuy√™n d·ª•ng: cosine_with_hard_restarts_schedule_with_warmup v·ªõi {cfg.NUM_CYCLES} chu k·ª≥.\"\n",
    "    )\n",
    "    scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=num_training_steps,\n",
    "        num_cycles=cfg.NUM_CYCLES,  # <-- Tham s·ªë chuy√™n bi·ªát ho·∫°t ƒë·ªông ·ªü ƒë√¢y!\n",
    "    )\n",
    "else:\n",
    "    # Gi·ªØ l·∫°i h√†m get_scheduler chung cho t·∫•t c·∫£ c√°c lo·∫°i scheduler kh√°c\n",
    "    logger.info(f\"S·ª≠ d·ª•ng scheduler chung: {cfg.SCHEDULER_TYPE}\")\n",
    "    scheduler = get_scheduler(\n",
    "        cfg.SCHEDULER_TYPE,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "logger.info(\"Warmup steps: %s\", warmup_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f73d99dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 14:07:21 - [INFO] - S·ª≠ d·ª•ng Class Weights & Label smoothing cho h√†m loss.\n"
     ]
    }
   ],
   "source": [
    "# Chuy·ªÉn class weights t·ª´ config th√†nh tensor v√† ƒë∆∞a l√™n device\n",
    "if cfg.CLASS_WEIGHTS:\n",
    "    logger.info(\"S·ª≠ d·ª•ng Class Weights & Label smoothing cho h√†m loss.\")\n",
    "    class_weights_tensor = torch.tensor(cfg.CLASS_WEIGHTS, dtype=torch.float).to(device)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(\n",
    "        weight=class_weights_tensor,\n",
    "        label_smoothing=cfg.LABEL_SMOOTHING,\n",
    "    ).to(device)\n",
    "else:\n",
    "    logger.info(\"S·ª≠ d·ª•ng CrossEntropyLoss th√¥ng th∆∞·ªùng (kh√¥ng c√≥ tr·ªçng s·ªë).\")\n",
    "    loss_fn = torch.nn.CrossEntropyLoss().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e5a7d7",
   "metadata": {},
   "source": [
    "## 5. V√≤ng l·∫∑p Hu·∫•n luy·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275d4c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 14:07:21 - [INFO] - --- Epoch 1/10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e8cd278f63444e0bf29a755522ce342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/1400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 14:11:45 - [INFO] - Loss trung b√¨nh tr√™n t·∫≠p train: 1.1033\n",
      "2025-10-17 14:11:45 - [INFO] - Current Learning Rate: 8.00e-06\n",
      "2025-10-17 14:11:45 - [INFO] - B·∫Øt ƒë·∫ßu ƒë√°nh gi√° tr√™n t·∫≠p validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e667fedfde8d42da820d27221e01c977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 14:12:06 - [INFO] - Validation Loss: 1.0034\n",
      "2025-10-17 14:12:06 - [INFO] - Validation Accuracy: 0.3943\n",
      "2025-10-17 14:12:06 - [INFO] - Validation Macro-F1: 0.2649\n",
      "/home/guest/.conda/envs/cs221/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/guest/.conda/envs/cs221/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/guest/.conda/envs/cs221/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "2025-10-17 14:12:06 - [INFO] - Classification Report tr√™n t·∫≠p validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no     0.8625    0.1537    0.2609       449\n",
      "   extrinsic     0.0000    0.0000    0.0000       461\n",
      "   intrinsic     0.3659    0.9857    0.5337       490\n",
      "\n",
      "    accuracy                         0.3943      1400\n",
      "   macro avg     0.4095    0.3798    0.2649      1400\n",
      "weighted avg     0.4047    0.3943    0.2705      1400\n",
      "\n",
      "2025-10-17 14:12:06 - [INFO] - üéâ Macro-F1 c·∫£i thi·ªán. ƒêang l∆∞u model t·ªët nh·∫•t v√†o '/home/guest/Projects/CS221/models/xlm-roberta-large-tuned'...\n",
      "2025-10-17 14:12:08 - [INFO] - L∆∞u model th√†nh c√¥ng.\n",
      "2025-10-17 14:12:08 - [INFO] - --- Epoch 2/10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5774c4d874da4d3db1076d94f281002a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/1400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 14:16:22 - [INFO] - Loss trung b√¨nh tr√™n t·∫≠p train: 0.8490\n",
      "2025-10-17 14:16:22 - [INFO] - Current Learning Rate: 8.00e-06\n",
      "2025-10-17 14:16:22 - [INFO] - B·∫Øt ƒë·∫ßu ƒë√°nh gi√° tr√™n t·∫≠p validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4c88908d0441c5ac34aba02992d1a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 14:16:43 - [INFO] - Validation Loss: 0.8177\n",
      "2025-10-17 14:16:43 - [INFO] - Validation Accuracy: 0.7186\n",
      "2025-10-17 14:16:43 - [INFO] - Validation Macro-F1: 0.7214\n",
      "2025-10-17 14:16:43 - [INFO] - Classification Report tr√™n t·∫≠p validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no     0.8184    0.6526    0.7261       449\n",
      "   extrinsic     0.7701    0.7484    0.7591       461\n",
      "   intrinsic     0.6195    0.7510    0.6790       490\n",
      "\n",
      "    accuracy                         0.7186      1400\n",
      "   macro avg     0.7360    0.7173    0.7214      1400\n",
      "weighted avg     0.7329    0.7186    0.7205      1400\n",
      "\n",
      "2025-10-17 14:16:43 - [INFO] - üéâ Macro-F1 c·∫£i thi·ªán. ƒêang l∆∞u model t·ªët nh·∫•t v√†o '/home/guest/Projects/CS221/models/xlm-roberta-large-tuned'...\n",
      "2025-10-17 14:16:45 - [INFO] - L∆∞u model th√†nh c√¥ng.\n",
      "2025-10-17 14:16:45 - [INFO] - --- Epoch 3/10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62038eb9e3544a5092f8182c7fc9d25f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/1400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_macro_f1 = 0.0\n",
    "patience_counter = 0  # bien dem => early stopped khi f1 ko tang them => overfitting\n",
    "\n",
    "for epoch in range(cfg.EPOCHS):\n",
    "    logger.info(f\"--- Epoch {epoch + 1}/{cfg.EPOCHS} ---\")\n",
    "\n",
    "    avg_train_loss = train_one_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        epoch + 1,\n",
    "        cfg.EPOCHS,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    )\n",
    "    logger.info(f\"Loss trung b√¨nh tr√™n t·∫≠p train: {avg_train_loss:.4f}\")\n",
    "\n",
    "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    logger.info(\n",
    "        f\"Current Learning Rate: {current_lr:.2e}\"\n",
    "    )  # D√πng ƒë·ªãnh d·∫°ng khoa h·ªçc e.g., 8.00e-06\n",
    "\n",
    "    # ƒê√°nh gi√° tr√™n t·∫≠p validation\n",
    "    logger.info(\"B·∫Øt ƒë·∫ßu ƒë√°nh gi√° tr√™n t·∫≠p validation...\")\n",
    "    val_labels, val_preds, avg_val_loss = evaluate(model, val_loader, loss_fn, device)\n",
    "\n",
    "    accuracy = accuracy_score(val_labels, val_preds)\n",
    "    macro_f1 = f1_score(val_labels, val_preds, average=\"macro\")\n",
    "\n",
    "    logger.info(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    logger.info(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "    logger.info(f\"Validation Macro-F1: {macro_f1:.4f}\")\n",
    "\n",
    "    # # In classification report chi ti·∫øt\n",
    "    target_names = [cfg.ID2LABEL[i] for i in range(len(cfg.LABEL_MAP))]\n",
    "\n",
    "    # In classification report chi ti·∫øt (c√≥ th·ªÉ gi·ªØ l·∫°i print ho·∫∑c log t·ª´ng d√≤ng)\n",
    "    report = classification_report(\n",
    "        val_labels,\n",
    "        val_preds,\n",
    "        target_names=[cfg.ID2LABEL[i] for i in range(len(cfg.LABEL_MAP))],\n",
    "        digits=4,\n",
    "    )\n",
    "    logger.info(f\"Classification Report tr√™n t·∫≠p validation:\\n{report}\")\n",
    "\n",
    "    # L∆∞u l·∫°i model t·ªët nh·∫•t d·ª±a tr√™n Macro-F1\n",
    "    if macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = macro_f1\n",
    "        patience_counter = 0  # << RESET B·ªò ƒê·∫æM\n",
    "\n",
    "        logger.info(\n",
    "            f\"üéâ Macro-F1 c·∫£i thi·ªán. ƒêang l∆∞u model t·ªët nh·∫•t v√†o '{cfg.MODEL_OUTPUT_DIR}'...\"\n",
    "        )\n",
    "        if not os.path.exists(cfg.MODEL_OUTPUT_DIR):\n",
    "            os.makedirs(cfg.MODEL_OUTPUT_DIR)\n",
    "\n",
    "        model.save_pretrained(cfg.MODEL_OUTPUT_DIR)\n",
    "        tokenizer.save_pretrained(cfg.MODEL_OUTPUT_DIR)\n",
    "        logger.info(\"L∆∞u model th√†nh c√¥ng.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        logger.warning(\n",
    "            f\"Macro-F1 kh√¥ng c·∫£i thi·ªán. Patience: {patience_counter}/{cfg.PATIENCE_LIMIT}\"\n",
    "        )\n",
    "        if patience_counter >= cfg.PATIENCE_LIMIT:\n",
    "            logger.info(\"Early stopping! D·ª´ng hu·∫•n luy·ªán.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1843cf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 14:06:49 - [INFO] - üèÅ Qu√° tr√¨nh hu·∫•n luy·ªán ho√†n t·∫•t.\n",
      "2025-10-17 14:06:49 - [INFO] - Model t·ªët nh·∫•t v·ªõi Macro-F1 = 0.7799 ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i '/home/guest/Projects/CS221/models/xlm-roberta-large-tuned'\n",
      "2025-10-17 14:06:49 - [INFO] - Model t·ªët nh·∫•t v·ªõi Macro-F1 = 0.7799 ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i '/home/guest/Projects/CS221/models/xlm-roberta-large-tuned'\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"üèÅ Qu√° tr√¨nh hu·∫•n luy·ªán ho√†n t·∫•t.\")\n",
    "logger.info(\n",
    "    f\"Model t·ªët nh·∫•t v·ªõi Macro-F1 = {best_macro_f1:.4f} ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i '{cfg.MODEL_OUTPUT_DIR}'\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451f5a5e",
   "metadata": {},
   "source": [
    "# Ph√¢n ph·ªëi k·∫øt qu·∫£ ƒë√∫ng/sai theo t·ª´ng l·ªõp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc9859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 14:06:52 - [INFO] - Ph√¢n ph·ªëi k·∫øt qu·∫£ tr√™n t·ª´ng l·ªõp:\n",
      "  true_label  correct  incorrect  total correct_rate incorrect_rate\n",
      "0  extrinsic      341        120    461       73.97%         26.03%\n",
      "1  intrinsic      368        122    490       75.10%         24.90%\n",
      "2         no      380         69    449       84.63%         15.37%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B·∫£ng ph√¢n ph·ªëi k·∫øt qu·∫£ tr√™n t·ª´ng l·ªõp:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_label</th>\n",
       "      <th>correct</th>\n",
       "      <th>incorrect</th>\n",
       "      <th>total</th>\n",
       "      <th>correct_rate</th>\n",
       "      <th>incorrect_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>extrinsic</td>\n",
       "      <td>341</td>\n",
       "      <td>120</td>\n",
       "      <td>461</td>\n",
       "      <td>73.97%</td>\n",
       "      <td>26.03%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>intrinsic</td>\n",
       "      <td>368</td>\n",
       "      <td>122</td>\n",
       "      <td>490</td>\n",
       "      <td>75.10%</td>\n",
       "      <td>24.90%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no</td>\n",
       "      <td>380</td>\n",
       "      <td>69</td>\n",
       "      <td>449</td>\n",
       "      <td>84.63%</td>\n",
       "      <td>15.37%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  true_label  correct  incorrect  total correct_rate incorrect_rate\n",
       "0  extrinsic      341        120    461       73.97%         26.03%\n",
       "1  intrinsic      368        122    490       75.10%         24.90%\n",
       "2         no      380         69    449       84.63%         15.37%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_label_names = [cfg.ID2LABEL[label_id] for label_id in val_labels]\n",
    "pred_label_names = [cfg.ID2LABEL[pred_id] for pred_id in val_preds]\n",
    "evaluation_df = pd.DataFrame(\n",
    "    {\n",
    "        \"true_label\": val_label_names,\n",
    "        \"predicted_label\": pred_label_names,\n",
    "    }\n",
    ")\n",
    "evaluation_df[\"status\"] = evaluation_df.apply(\n",
    "    lambda row: (\n",
    "        \"correct\" if row[\"true_label\"] == row[\"predicted_label\"] else \"incorrect\"\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "distribution_table = (\n",
    "    evaluation_df.groupby([\"true_label\", \"status\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .rename_axis(None, axis=1)\n",
    "    .reset_index()\n",
    "    .sort_values(\"true_label\")\n",
    ")\n",
    "\n",
    "# 1. Th√™m c·ªôt 'total' b·∫±ng c√°ch c·ªông c·ªôt 'correct' v√† 'incorrect'\n",
    "distribution_table[\"total\"] = (\n",
    "    distribution_table[\"correct\"] + distribution_table[\"incorrect\"]\n",
    ")\n",
    "\n",
    "# 2. Th√™m c·ªôt t·ªâ l·ªá ƒë√∫ng (correct_rate)\n",
    "distribution_table[\"correct_rate\"] = (\n",
    "    distribution_table[\"correct\"] / distribution_table[\"total\"]\n",
    ")\n",
    "\n",
    "# 3. Th√™m c·ªôt t·ªâ l·ªá sai (incorrect_rate)\n",
    "distribution_table[\"incorrect_rate\"] = (\n",
    "    distribution_table[\"incorrect\"] / distribution_table[\"total\"]\n",
    ")\n",
    "\n",
    "# (T√πy ch·ªçn) Format c√°c c·ªôt t·ªâ l·ªá th√†nh d·∫°ng ph·∫ßn trƒÉm cho d·ªÖ ƒë·ªçc\n",
    "distribution_table[\"correct_rate\"] = distribution_table[\"correct_rate\"].map(\n",
    "    \"{:.2%}\".format\n",
    ")\n",
    "distribution_table[\"incorrect_rate\"] = distribution_table[\"incorrect_rate\"].map(\n",
    "    \"{:.2%}\".format\n",
    ")\n",
    "\n",
    "# In ra b·∫£ng k·∫øt qu·∫£\n",
    "logger.info(f\"Ph√¢n ph·ªëi k·∫øt qu·∫£ tr√™n t·ª´ng l·ªõp:\\n{distribution_table.to_string()}\")\n",
    "\n",
    "# Trong notebook, d√πng display() s·∫Ω cho b·∫£ng ƒë·∫πp h∆°n\n",
    "print(\"B·∫£ng ph√¢n ph·ªëi k·∫øt qu·∫£ tr√™n t·ª´ng l·ªõp:\")\n",
    "display(distribution_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs221",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
