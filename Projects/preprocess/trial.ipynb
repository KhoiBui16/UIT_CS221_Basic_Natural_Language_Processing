{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e17326c9",
   "metadata": {},
   "source": [
    "# Correction Vietnamese local strict spelling \n",
    "\n",
    "- Tích hợp hàm trích xuất danh từ riêng và số thông minh hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92736a38",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bc3c342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "\n",
    "import signal\n",
    "import logging\n",
    "import pandas as pd\n",
    "import torch\n",
    "import unicodedata\n",
    "import difflib\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GenerationConfig\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.notebook import tqdm\n",
    "from huggingface_hub import snapshot_download\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604e4d21",
   "metadata": {},
   "source": [
    "# Suppress Transformers Warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6917915",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger(\"transformers.generation.utils\").setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bf91c3",
   "metadata": {},
   "source": [
    "# LOAD .env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb792ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dotenv_path: /home/guest/Projects/DSC2025/BAN/preprocess/../envs/.env\n"
     ]
    }
   ],
   "source": [
    "dotenv_path = os.path.join(os.getcwd(), \"..\", \"envs\", \".env\")\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path)\n",
    "else:\n",
    "    load_dotenv()\n",
    "    \n",
    "print(f\"dotenv_path: {dotenv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25697001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!\n"
     ]
    }
   ],
   "source": [
    "HF_TOKEN = (\n",
    "    os.getenv(\"HUGGING_FACE_TOKEN\") or\n",
    "    os.getenv(\"HUGGINGFACE_TOKEN\") or\n",
    "    os.getenv(\"HF_TOKEN\") or\n",
    "    os.getenv(\"HUNGGING_FACE_TOKEN\") or\n",
    "    None\n",
    ")\n",
    "\n",
    "if HF_TOKEN:\n",
    "    print(\"OK!\")\n",
    "else:\n",
    "    print(\"Add HF key to .env\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f7ee783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public_test_correction_folder_path: ./vihallu_public_test_correction\n",
      "debug_correction_villua_folder_path: ./debug_vihallu_public_test_correction\n"
     ]
    }
   ],
   "source": [
    "public_test_correction_folder = \"./vihallu_public_test_correction\"\n",
    "os.makedirs(public_test_correction_folder, exist_ok=True)\n",
    "print(f\"public_test_correction_folder_path: {public_test_correction_folder}\")\n",
    "\n",
    "debug_correction_villua_folder = \"./debug_vihallu_public_test_correction\"\n",
    "os.makedirs(debug_correction_villua_folder, exist_ok=True)\n",
    "print(f\"debug_correction_villua_folder_path: {debug_correction_villua_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c58ba2",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90ba0c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"VietnamAIHub/Vietnamese_llama2_7B_8K_SFT_General_domain\"\n",
    "INPUT_CSV = \"./test.csv\"\n",
    "OUTPUT_CSV =  os.path.join(public_test_correction_folder, \"fixed-text-correction.csv\")\n",
    "COLUMNS_TO_FIX = ['text'] #['context', 'prompt', 'response']\n",
    "\n",
    "MAX_INPUT_LENGTH = 2048\n",
    "WORD_COUNT_TOLERANCE = 1\n",
    "ACCEPT_SIMILARITY_THRESHOLD = 0.88\n",
    "LENGTH_CHANGE_ALLOWED_RATIO = 0.15 \n",
    "STRICT_BASE_SIMILARITY      = 0.97\n",
    "LENIENT_BASE_SIMILARITY     = 0.94      \n",
    "\n",
    "\n",
    "DEBUG_LOG_CSV =  os.path.join(debug_correction_villua_folder, \"debug_text_corrections.csv\")\n",
    "SAMPLE_DEBUG_N = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca64caa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0\n",
      "+ Original: Nguồn thhu chinh của thuê truc tiếp la nhủg ai?\n",
      "-> Correction: Nguồn thu chính của thuế trực tiếp là những ai?\n",
      "\n",
      "index: 1\n",
      "+ Original: Ý nhĩa cũa viẹc tổ chưc cuôc thi Intervision là gì z?\n",
      "-> Correction: Ý nghĩa của việc tổ chức cuộc thi Intervision là gì?\n",
      "\n",
      "index: 2\n",
      "+ Original: thủ đô ha nội là trung tâm văn hóa\n",
      "-> Correction: thủ đô Hà Nội là trung tâm văn hóa\n",
      "\n",
      "index: 3\n",
      "+ Original: ...không đứng đầu thế giới về dự trữ ngoại tệ?\n",
      "-> Correction: ...không đứng đầu thế giới về dự trữ ngoại tệ?\n",
      "\n",
      "index: 4\n",
      "+ Original: Các nhà khoa học muốn tìm đến S.J thì phải đến đâu?\n",
      "-> Correction: Các nhà khoa học muốn tìm đến S.J thì phải đến đâu?\n",
      "\n",
      "index: 5\n",
      "+ Original: ...cuộc thi hát nổi tiếng thường niên...\n",
      "-> Correction: ...cuộc thi hát nổi tiếng thường niên...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FEW_SHOTS = [\n",
    "    (\"Nguồn thhu chinh của thuê truc tiếp la nhủg ai?\", \"Nguồn thu chính của thuế trực tiếp là những ai?\"),\n",
    "    (\"Ý nhĩa cũa viẹc tổ chưc cuôc thi Intervision là gì z?\", \"Ý nghĩa của việc tổ chức cuộc thi Intervision là gì?\"),\n",
    "    (\"thủ đô ha nội là trung tâm văn hóa\", \"thủ đô Hà Nội là trung tâm văn hóa\"),\n",
    "    (\"...không đứng đầu thế giới về dự trữ ngoại tệ?\", \"...không đứng đầu thế giới về dự trữ ngoại tệ?\"),\n",
    "    (\"Các nhà khoa học muốn tìm đến S.J thì phải đến đâu?\", \"Các nhà khoa học muốn tìm đến S.J thì phải đến đâu?\"),\n",
    "    (\"...cuộc thi hát nổi tiếng thường niên...\", \"...cuộc thi hát nổi tiếng thường niên...\"),\n",
    "]\n",
    "\n",
    "for idx, few_shot in enumerate(FEW_SHOTS):\n",
    "    print(f\"index: {idx}\\n+ Original: {few_shot[0]}\\n-> Correction: {few_shot[1]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294f3a57",
   "metadata": {},
   "source": [
    "# Device info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f36dbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "\n",
      "=== CONFIG: ===\n",
      "Model: VietnamAIHub/Vietnamese_llama2_7B_8K_SFT_General_domain\n",
      "INPUT CSV: ./test.csv\n",
      "OUTPUT CSV: ./vihallu_public_test_correction/fixed-text-correction.csv\n",
      "DEBUG_LOG_CSV: ./debug_vihallu_public_test_correction/debug_text_corrections.csv\n",
      "\n",
      "=== Columns to fix: ['text'] ===\n",
      "MAX_INPUT_LENGTH: 2048\n",
      "WORD_COUNT_TOLERANCE: 1\n",
      "SMAMPLE_DEBUG_N:20\n",
      "\n",
      "=== DEBUG: Thresholds (Final Optimized) ===\n",
      "STRICT (prompt, response): BASE_SIMILARITY >= 0.97\n",
      "LENGTH_CHANGE_ALLOWED_RATIO: 0.15\n",
      "LENIENT (context): BASE_SIMILARITY >= 0.94\n",
      "GENERAL: ACCEPT_SIMILARITY >= 0.88\n",
      "====================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"\\n=== CONFIG: ===\")\n",
    "print(f\"Model: {MODEL}\")\n",
    "print(f\"INPUT CSV: {INPUT_CSV}\")\n",
    "print(f\"OUTPUT CSV: {OUTPUT_CSV}\")\n",
    "print(f\"DEBUG_LOG_CSV: {DEBUG_LOG_CSV}\")\n",
    "\n",
    "print(f\"\\n=== Columns to fix: {COLUMNS_TO_FIX} ===\")\n",
    "print(f\"MAX_INPUT_LENGTH: {MAX_INPUT_LENGTH}\")\n",
    "print(f\"WORD_COUNT_TOLERANCE: {WORD_COUNT_TOLERANCE}\")\n",
    "print(f\"SMAMPLE_DEBUG_N:{SAMPLE_DEBUG_N}\")\n",
    "\n",
    "print(\"\\n=== DEBUG: Thresholds (Final Optimized) ===\")\n",
    "print(f\"STRICT (prompt, response): BASE_SIMILARITY >= {STRICT_BASE_SIMILARITY}\")\n",
    "print(f\"LENGTH_CHANGE_ALLOWED_RATIO: {LENGTH_CHANGE_ALLOWED_RATIO}\")\n",
    "print(f\"LENIENT (context): BASE_SIMILARITY >= {LENIENT_BASE_SIMILARITY}\")\n",
    "print(f\"GENERAL: ACCEPT_SIMILARITY >= {ACCEPT_SIMILARITY_THRESHOLD}\")\n",
    "print(\"====================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b319cc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(INPUT_CSV): \n",
    "    raise FileNotFoundError(f\"Không tìm thấy file '{INPUT_CSV}'.\")\n",
    "\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "df_global = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed8523aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bộ chứa cho các row đã được xử lý (chỉ những row này sẽ được lưu vào OUTPUT_CSV)\n",
    "processed_rows = []\n",
    "processed_indices = set()\n",
    "# STOP flag (sẽ được set True bởi signal handler)\n",
    "stop_requested = False\n",
    "\n",
    "\n",
    "# hàm lưu hiện trạng (chỉ lưu các cột index, context, prompt, response, predict_label)\n",
    "def save_processed_rows_and_exit():\n",
    "    if not processed_rows:\n",
    "        print(\"No processed rows to save.\")\n",
    "        return\n",
    "    output_df = pd.DataFrame(processed_rows)\n",
    "\n",
    "\n",
    "    # đảm bảo đủ các cột cần thiết\n",
    "    required_cols = [\"index\", \"context\", \"prompt\", \"response\", \"predict_label\"]\n",
    "    for c in required_cols:\n",
    "        if c not in output_df.columns:\n",
    "            output_df[c] = \"\"\n",
    "\n",
    "\n",
    "    output_df = output_df[required_cols]\n",
    "    output_df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Successfully saved {len(output_df)} processed rows to {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bcb2cb",
   "metadata": {},
   "source": [
    "## Ctrl+C handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17b0701c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected IPython environment — relying on KeyboardInterrupt for interruption handling.\n"
     ]
    }
   ],
   "source": [
    "def signal_handler(sig, frame):\n",
    "    global stop_requested\n",
    "    print(\"\\n\\n[Ctrl+C] Request received — will stop after current row and save processed rows...\")\n",
    "    stop_requested = True\n",
    "\n",
    "\n",
    "if 'ipykernel' not in sys.modules and 'IPython' not in sys.modules:\n",
    "    signal.signal(signal.SIGINT, signal_handler)\n",
    "else:\n",
    "    print(\"Detected IPython environment — relying on KeyboardInterrupt for interruption handling.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861d913c",
   "metadata": {},
   "source": [
    "## tokenizer/model setup -> set up 8-bit if not fallback to origin 16-bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ca3cd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_kwargs = {\n",
    "    \"use_fast\": False, \n",
    "    \"token\": HF_TOKEN, \n",
    "    \"padding_side\": \"left\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b350b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer_local_or_remote(model_id):\n",
    "    try:\n",
    "        tok = AutoTokenizer.from_pretrained(model_id, local_files_only=True, **{k: v for k, v in tokenizer_kwargs.items() if k != \"token\"})\n",
    "    except Exception:\n",
    "        tok = AutoTokenizer.from_pretrained(model_id, **tokenizer_kwargs)\n",
    "    if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
    "    return tok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512f3a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_local_or_snapshot(model_id):\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub\")\n",
    "    print(f\"Cache huggingface path: {cache_dir}\")\n",
    "    loading_strategies = [\n",
    "        # {\"name\": \"8bit\", \"kwargs\": {\"quantization_config\": BitsAndBytesConfig(load_in_8bit=True), \"device_map\": \"auto\", \"token\": HF_TOKEN, \"cache_dir\": cache_dir}},\n",
    "        {\"name\": \"4bit\", \"kwargs\": {\"quantization_config\": BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16), \"device_map\": \"auto\", \"token\": HF_TOKEN, \"cache_dir\": cache_dir}},\n",
    "        {\"name\": \"bfloat16\", \"kwargs\": {\"torch_dtype\": torch.bfloat16, \"device_map\": \"auto\", \"token\": HF_TOKEN, \"cache_dir\": cache_dir}},\n",
    "    ]\n",
    "    \n",
    "    last_e = None\n",
    "    for strat in loading_strategies:    \n",
    "        try:\n",
    "            print(f\"Trying to load with config: {strat['name']}\")\n",
    "            m = AutoModelForCausalLM.from_pretrained(model_id, **strat[\"kwargs\"])\n",
    "            print(f\"Successfully loaded with config: {strat['name']}\")\n",
    "            model_path = (\n",
    "                snapshot_download(model_id, token=HF_TOKEN, cache_dir=cache_dir)\n",
    "                if HF_TOKEN\n",
    "                else cache_dir\n",
    "            )\n",
    "            return m, model_path\n",
    "        except Exception as e:\n",
    "            print(f\"Failed with config: {strat['name']} - {type(e).__name__}: {e}\")\n",
    "            last_e = e\n",
    "    raise RuntimeError(\"Cannot load model\") from last_e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a934c1e3",
   "metadata": {},
   "source": [
    "## Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b06baa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaTokenizer(name_or_path='VietnamAIHub/Vietnamese_llama2_7B_8K_SFT_General_domain', vocab_size=32000, model_max_length=8192, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t32000: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = load_tokenizer_local_or_remote(MODEL)\n",
    "tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e0cc05",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f3124e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Cache huggingface path: /home/guest/.cache/huggingface/hub\n",
      "Trying to load with config: 8bit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaca0d7bc7284bd3a63b4c9ae74647f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded with config: 8bit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c631e9ccbe4f2db5d13625657f3730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading model...\")\n",
    "model, model_source = load_model_local_or_snapshot(MODEL)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6c8f1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "Model loaded from: /home/guest/.cache/huggingface/hub/models--VietnamAIHub--Vietnamese_llama2_7B_8K_SFT_General_domain/snapshots/12466d4619deed5fb5972760c082d7382151376c\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    model_device = next(model.parameters()).device\n",
    "except: \n",
    "    model_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "print(f\"Model device: {model_device}\")\n",
    "print(f\"Model loaded from: {model_source}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79df2a41",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6bcaea",
   "metadata": {},
   "source": [
    "## Remove diaccritics and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85f60a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_diacritics_and_punct(s: str):\n",
    "    s = unicodedata.normalize('NFD', s)\n",
    "    s = ''.join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    s = unicodedata.normalize('NFC', s)\n",
    "    s = re.sub(r'[^\\w\\s]', '', s)\n",
    "    return s.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a198a1",
   "metadata": {},
   "source": [
    "## Nomalize spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b3876cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_spaces(s: str): \n",
    "    return re.sub(r'\\s+', ' ', s).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880ff441",
   "metadata": {},
   "source": [
    "# Calculate String Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "913ca251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_similarity(a, b): \n",
    "    return difflib.SequenceMatcher(None, a, b).ratio()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ec1b89",
   "metadata": {},
   "source": [
    "## Extract number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0df84343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_numbers(text: str):\n",
    "    \"\"\"Trích xuất tất cả các chuỗi số, bao gồm cả số có dấu phẩy và dấu chấm.\"\"\"\n",
    "    return set(re.findall(r'\\d+[,.]?\\d*', text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b7f225",
   "metadata": {},
   "source": [
    "## Extract proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bfb835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_proper_nouns(text: str):\n",
    "    \"\"\"\n",
    "    Trích xuất các cụm danh từ riêng tiềm năng (chuỗi các từ viết hoa liền nhau).\n",
    "    Hỗ trợ đầy đủ ký tự tiếng Việt và các trường hợp có dấu '.' hoặc '-'.\n",
    "    \"\"\"\n",
    "    vietnamese_uppercase = \"A-ZÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚÝĂĐĨŨƠƯẠẢẤẦẨẪẬẮẰẲẴẶẸẺẼẾỀỂỄỆỈỊỌỎỐỒỔỖỘỚỜỞỠỢỤỦỨỪỬỮỰỲỴỶỸ\"\n",
    "    pattern = r'\\b([' + vietnamese_uppercase + r'][\\w\\.-]+' + r'(?:\\s+[' + vietnamese_uppercase + r'][\\w\\.-]+)*)\\b'\n",
    "    found_nouns = set(re.findall(pattern, text))\n",
    "    common_acronyms = {'AI', 'LLM', 'UIT', 'SED', 'SKLP', 'GDP', 'UNESCO'}\n",
    "    return {noun for noun in found_nouns if noun not in common_acronyms}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3671183",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecda240",
   "metadata": {},
   "source": [
    "## Prompt & Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63b2fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_correction_prompt_llama2(original_text):\n",
    "    fs_examples = \"\\n\".join([f\"Văn bản gốc: \\\"{o}\\\"\\nVăn bản đã sửa: \\\"{c}\\\"\" for o, c in FEW_SHOTS])\n",
    "    system_prompt = '''\n",
    "Bạn là một chuyên gia sửa lỗi chính tả tiếng Việt.  \n",
    "Nhiệm vụ của bạn là kiểm tra và sửa lỗi chính tả của văn bản được cung cấp theo các quy tắc sau:\n",
    "\n",
    "**QUY TẮC SỐ 1 (QUAN TRỌNG NHẤT):**  \n",
    "Chỉ thay đổi những từ bị sai chính tả. Giữ nguyên các từ có nghĩa chuyên biệt (ví dụ: “ngoại tệ”) và giữ nguyên các từ viết tắt không rõ nghĩa (ví dụ: “S.J”).\n",
    "\n",
    "**QUY TẮC SỐ 2:**  \n",
    "Chỉ sửa lỗi chính tả, lỗi gõ phím, lỗi dấu thanh và lỗi viết hoa.\n",
    "\n",
    "**QUY TẮC SỐ 3:**  \n",
    "Nếu văn bản đã đúng, hãy lặp lại y hệt.\n",
    "\n",
    "**QUY TẮC SỐ 4:**  \n",
    "Không sửa dữ liệu số.\n",
    "'''.strip()\n",
    "    user_prompt = (\n",
    "        \"Các ví dụ:\\n\"\n",
    "        f\"{fs_examples}\\n\\n\"\n",
    "        \"Sửa văn bản sau. Chỉ trả về văn bản đã sửa.\\n\"\n",
    "        f\"Văn bản gốc: \\\"{original_text}\\\"\"\n",
    "    )\n",
    "    return f\"<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{user_prompt} [/INST] Văn bản đã sửa: \\\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28996f7",
   "metadata": {},
   "source": [
    "## Generate from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a948217b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_model(prompt, original_text_for_token_limit):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_INPUT_LENGTH)\n",
    "    try: inputs = {k: v.to(model_device) for k, v in inputs.items()}\n",
    "    except Exception: pass\n",
    "    \n",
    "    input_token_length = len(tokenizer.encode(original_text_for_token_limit, add_special_tokens=False))\n",
    "    \n",
    "    dynamic_min_new_tokens = int(input_token_length * 0.8)\n",
    "    dynamic_max_new_tokens = int(input_token_length * 1.5) + 50\n",
    "    \n",
    "    gen_cfg = GenerationConfig(\n",
    "        min_new_tokens=dynamic_min_new_tokens,\n",
    "        max_new_tokens=dynamic_max_new_tokens,\n",
    "        do_sample=False, \n",
    "        num_beams=3, \n",
    "        repetition_penalty=1.1, \n",
    "        early_stopping=False,\n",
    "        pad_token_id=tokenizer.eos_token_id, \n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        out = model.generate(**inputs, generation_config=gen_cfg)\n",
    "    full_decoded = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    \n",
    "    if \"[/INST]\" in full_decoded:\n",
    "        response_part = full_decoded.split(\"[/INST]\")[-1].strip()\n",
    "        if response_part.lower().startswith(\"văn bản đã sửa:\"):\n",
    "            response_part = response_part[len(\"Văn bản đã sửa:\"):].strip()\n",
    "        corrected = response_part.split('\\n')[0].strip()\n",
    "        corrected = re.sub(r'^[\"\\']|[\"\\']$', '', corrected)\n",
    "        \n",
    "        if corrected.endswith('\"'): \n",
    "            corrected = corrected[:-1]\n",
    "        return corrected\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beb5a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dì hai lăng lọi đừng xa đi tới thăm chú Tư'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Dì hai lăng lọi đường xa đi tới thăm chú Tư\"\n",
    "generate_from_model(make_correction_prompt_llama2(text),text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca3a110",
   "metadata": {},
   "source": [
    "# Correction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "395d8a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "correction_cache = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81229984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_vietnamese_spelling(text, row_id, col_name, debug_print=False):\n",
    "    original_text = \"\" if pd.isna(text) else str(text).strip()\n",
    "    if not original_text: \n",
    "        return text\n",
    "    \n",
    "    cache_key = (col_name, original_text)\n",
    "    if cache_key in correction_cache: \n",
    "        return correction_cache[cache_key]\n",
    "    \n",
    "    prompt = make_correction_prompt_llama2(original_text)\n",
    "    corrected_text = generate_from_model(prompt, original_text_for_token_limit=original_text)\n",
    "    \n",
    "    accepted = False\n",
    "    reason = \"not_changed\"\n",
    "    final_text = original_text\n",
    "    base_sim = 1.0\n",
    "    \n",
    "    # Chỉ check khi có sự thay đổi\n",
    "    if corrected_text and corrected_text != original_text:\n",
    "        passes_initial_checks = False\n",
    "        \n",
    "        current_base_threshold = STRICT_BASE_SIMILARITY if col_name in ['prompt', 'response'] else LENIENT_BASE_SIMILARITY\n",
    "        orig_words = normalize_spaces(original_text).split()\n",
    "        corr_words = normalize_spaces(corrected_text).split()\n",
    "        \n",
    "        # Các điều kiện để accept hay reject cho llm generate để correct spelling\n",
    "        base_sim = string_similarity(remove_diacritics_and_punct(original_text), remove_diacritics_and_punct(corrected_text))\n",
    "        \n",
    "        if abs(len(orig_words) - len(corr_words)) > WORD_COUNT_TOLERANCE:\n",
    "            reason = f\"word_count_mismatch: original_words:{len(orig_words)} vs correction_word: {len(corr_words)}\"\n",
    "        elif base_sim < current_base_threshold:\n",
    "            reason = f\"base_similarity_too_low_{base_sim:.2f}_(threshold:{current_base_threshold})\"\n",
    "        elif len(original_text) > 0 and abs(len(corrected_text) - len(original_text)) / len(original_text) > LENGTH_CHANGE_ALLOWED_RATIO:\n",
    "            reason = \"length_changed_too_much\"\n",
    "        elif string_similarity(original_text, corrected_text) < ACCEPT_SIMILARITY_THRESHOLD:\n",
    "            reason = f\"low_similarity_{string_similarity(original_text, corrected_text):.2f}\"\n",
    "        elif original_text.endswith('?') and not corrected_text.endswith('?'):\n",
    "            reason = \"question_mark_removed\"\n",
    "        else:\n",
    "            passes_initial_checks = True\n",
    "        \n",
    "        # Nếu qua các điều kiện trên -> check thêm các lưới dự phòng => để tránh model generate ra những danh từ và số mới => dữ liệu càng bị hallucination\n",
    "        if passes_initial_checks:\n",
    "            original_numbers = extract_numbers(original_text)\n",
    "            corrected_numbers = extract_numbers(corrected_text)\n",
    "            original_nouns = extract_proper_nouns(original_text)\n",
    "            corrected_nouns = extract_proper_nouns(corrected_text)\n",
    "            \n",
    "            # Kiểm tra sự tương đương tuyệt đối của các \"sự thật\"\n",
    "            if original_numbers != corrected_numbers:\n",
    "                reason = f\"numbers_altered: {original_numbers.symmetric_difference(corrected_numbers)}\"\n",
    "                passes_initial_checks = False\n",
    "            elif original_nouns != corrected_nouns:\n",
    "                reason = f\"proper_nouns_altered: {original_nouns.symmetric_difference(corrected_nouns)}\"\n",
    "                passes_initial_checks = False\n",
    "\n",
    "        if passes_initial_checks:\n",
    "            accepted = True\n",
    "            reason = \"accepted_change\"\n",
    "            final_text = corrected_text\n",
    "            \n",
    "    if debug_print:\n",
    "        print(\"\\n--- DEBUG SAMPLE ---\")\n",
    "        print(f\"### ID: {row_id} | COLUMN: {col_name}\")\n",
    "        print(f\"+   ORIGINAL: \\n{original_text}\")\n",
    "        print(f\"\\n+ CORRECTED (model returned): \\n{corrected_text}\")\n",
    "        \n",
    "        if corrected_text and corrected_text != original_text:\n",
    "            print(\"\\n--- METRICS & THRESHOLDS ---\")\n",
    "            print(f\"  - Base Similarity: {base_sim:.4f}\")\n",
    "            print(f\"  - Direct Similarity: {string_similarity(original_text, corrected_text):.4f}\")\n",
    "            \n",
    "            print(\"\\n--- FACT CHECKING ---\")\n",
    "            print(f\"  - Original Numbers (len: {len(extract_numbers(original_text))}): \\n{extract_numbers(original_text)}\")\n",
    "            print(f\"  - Corrected Numbers (len: {len(extract_numbers(corrected_text))}): \\n{extract_numbers(corrected_text)}\")\n",
    "            print(f\"  - Original Nouns (len: {len(extract_proper_nouns(original_text))}): \\n{extract_proper_nouns(original_text)}\")\n",
    "            print(f\"  - Corrected Nouns (len: {len(extract_proper_nouns(corrected_text))}): \\n{extract_proper_nouns(corrected_text)}\")\n",
    "        else:\n",
    "            print(\"\\n--- METRICS & THRESHOLDS ---\")\n",
    "            print(\"No change proposed by model.\")\n",
    "\n",
    "        print(\"\\n--- DECISION ---\")\n",
    "        print(f\"RESULT: {'ACCEPTED' if accepted else 'REJECTED'}\")\n",
    "        if not accepted:\n",
    "            print(f\"REASON: {reason}\")\n",
    "        print(f\"FINAL TEXT: \\n{final_text}\")\n",
    "        print(\"-\" * 10)\n",
    "        \n",
    "    try:\n",
    "        df_row = pd.DataFrame([{\n",
    "            \"id\": row_id, \n",
    "            \"column\": col_name, \n",
    "            \"original\": original_text, \n",
    "            \"corrected_model\": corrected_text, \n",
    "            \"final_text\": final_text, \n",
    "            \"accepted\": accepted, \n",
    "            \"reason\": reason, \n",
    "            \"similarity\": string_similarity(original_text, corrected_text) if corrected_text else 1.0, \n",
    "            \"base_similarity\": base_sim\n",
    "        }])\n",
    "        \n",
    "        log_header = not os.path.exists(DEBUG_LOG_CSV)\n",
    "        df_row.to_csv(DEBUG_LOG_CSV, index=False, mode='a', header=log_header, encoding=\"utf-8-sig\")\n",
    "    except Exception as e:\n",
    "        if debug_print: print(f\"Warning: cannot write debug csv: {e}\")\n",
    "        \n",
    "    correction_cache[cache_key] = final_text\n",
    "    return final_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479b5985",
   "metadata": {},
   "source": [
    "# Main Loop for correcting Vietnamese's spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8af922b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44638705e79643a5a1109ece21089f60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DEBUG SAMPLE ---\n",
      "### ID: 1 | COLUMN: text\n",
      "+   ORIGINAL: \n",
      "\"Trước năm 1929, khu vực lãnh thổ Thành Vatican thuộc khu vực nào trong thành phố Roma?\"\n",
      "\n",
      "+ CORRECTED (model returned): \n",
      "Trước năm 1929, khu vực lãnh thổ Thành Vatican thuộc khu vực nào trong thành phố Rome?\n",
      "\n",
      "--- METRICS & THRESHOLDS ---\n",
      "  - Base Similarity: 0.9881\n",
      "  - Direct Similarity: 0.9770\n",
      "\n",
      "--- FACT CHECKING ---\n",
      "  - Original Numbers (len: 1): \n",
      "{'1929,'}\n",
      "  - Corrected Numbers (len: 1): \n",
      "{'1929,'}\n",
      "  - Original Nouns (len: 3): \n",
      "{'Thành Vatican', 'Roma', 'Trước'}\n",
      "  - Corrected Nouns (len: 3): \n",
      "{'Rome', 'Thành Vatican', 'Trước'}\n",
      "\n",
      "--- DECISION ---\n",
      "RESULT: REJECTED\n",
      "REASON: proper_nouns_altered: {'Rome', 'Roma'}\n",
      "FINAL TEXT: \n",
      "\"Trước năm 1929, khu vực lãnh thổ Thành Vatican thuộc khu vực nào trong thành phố Roma?\"\n",
      "----------\n",
      "\n",
      "--- DEBUG SAMPLE ---\n",
      "### ID: 2 | COLUMN: text\n",
      "+   ORIGINAL: \n",
      "\"Tôi Nhìn thấy những thứ Khôn có\"\n",
      "\n",
      "+ CORRECTED (model returned): \n",
      "Tôi Nhìn thấy những thứ Khôn có\n",
      "\n",
      "--- METRICS & THRESHOLDS ---\n",
      "  - Base Similarity: 1.0000\n",
      "  - Direct Similarity: 0.9730\n",
      "\n",
      "--- FACT CHECKING ---\n",
      "  - Original Numbers (len: 0): \n",
      "set()\n",
      "  - Corrected Numbers (len: 0): \n",
      "set()\n",
      "  - Original Nouns (len: 2): \n",
      "{'Tôi Nhi', 'Khôn'}\n",
      "  - Corrected Nouns (len: 2): \n",
      "{'Tôi Nhi', 'Khôn'}\n",
      "\n",
      "--- DECISION ---\n",
      "RESULT: ACCEPTED\n",
      "FINAL TEXT: \n",
      "Tôi Nhìn thấy những thứ Khôn có\n",
      "----------\n",
      "\n",
      "--- DEBUG SAMPLE ---\n",
      "### ID: 3 | COLUMN: text\n",
      "+   ORIGINAL: \n",
      "\"Dì hai lăng lọi đường xa đi tới thăm chú Tư\"\n",
      "\n",
      "+ CORRECTED (model returned): \n",
      "Dì hai lăng lọi đường xa đi tới thăm chú Tư\n",
      "\n",
      "--- METRICS & THRESHOLDS ---\n",
      "  - Base Similarity: 1.0000\n",
      "  - Direct Similarity: 0.9796\n",
      "\n",
      "--- FACT CHECKING ---\n",
      "  - Original Numbers (len: 0): \n",
      "set()\n",
      "  - Corrected Numbers (len: 0): \n",
      "set()\n",
      "  - Original Nouns (len: 2): \n",
      "{'Tư', 'Di'}\n",
      "  - Corrected Nouns (len: 2): \n",
      "{'Tư', 'Di'}\n",
      "\n",
      "--- DECISION ---\n",
      "RESULT: ACCEPTED\n",
      "FINAL TEXT: \n",
      "Dì hai lăng lọi đường xa đi tới thăm chú Tư\n",
      "----------\n",
      "\n",
      "--- DEBUG SAMPLE ---\n",
      "### ID: 4 | COLUMN: text\n",
      "+   ORIGINAL: \n",
      "\"George Washington (22 tháng 2 năm 1732 – 14 tháng 12 năm 1799) (phiên âm: Gioóc-giơ Qua-sinh-tơn)\"\n",
      "\n",
      "+ CORRECTED (model returned): \n",
      "George Washington (22 tháng 2 năm 1732 – 14 tháng 12 năm 179) (phiên âm: Gioóc-giơ Qua-sinh-tơn)\n",
      "\n",
      "--- METRICS & THRESHOLDS ---\n",
      "  - Base Similarity: 0.9943\n",
      "  - Direct Similarity: 0.9846\n",
      "\n",
      "--- FACT CHECKING ---\n",
      "  - Original Numbers (len: 6): \n",
      "{'22', '1732', '2', '1799', '12', '14'}\n",
      "  - Corrected Numbers (len: 6): \n",
      "{'22', '179', '1732', '2', '12', '14'}\n",
      "  - Original Nouns (len: 2): \n",
      "{'Gioóc-giơ Qua-sinh-tơn', 'George Washington'}\n",
      "  - Corrected Nouns (len: 2): \n",
      "{'Gioóc-giơ Qua-sinh-tơn', 'George Washington'}\n",
      "\n",
      "--- DECISION ---\n",
      "RESULT: REJECTED\n",
      "REASON: numbers_altered: {'179', '1799'}\n",
      "FINAL TEXT: \n",
      "\"George Washington (22 tháng 2 năm 1732 – 14 tháng 12 năm 1799) (phiên âm: Gioóc-giơ Qua-sinh-tơn)\"\n",
      "----------\n",
      "\n",
      "--- DEBUG SAMPLE ---\n",
      "### ID: 5 | COLUMN: text\n",
      "+   ORIGINAL: \n",
      "\"Tôi tên là huy, rất vuii được gập bạn\n",
      "\n",
      "+ CORRECTED (model returned): \n",
      "Tôi tên là Huỳnh, rất vui được gặp bạn\n",
      "\n",
      "--- METRICS & THRESHOLDS ---\n",
      "  - Base Similarity: 0.9589\n",
      "  - Direct Similarity: 0.7160\n",
      "\n",
      "--- FACT CHECKING ---\n",
      "  - Original Numbers (len: 0): \n",
      "set()\n",
      "  - Corrected Numbers (len: 0): \n",
      "set()\n",
      "  - Original Nouns (len: 1): \n",
      "{'Tôi'}\n",
      "  - Corrected Nouns (len: 2): \n",
      "{'Tôi', 'Huỳnh'}\n",
      "\n",
      "--- DECISION ---\n",
      "RESULT: REJECTED\n",
      "REASON: low_similarity_0.72\n",
      "FINAL TEXT: \n",
      "\"Tôi tên là huy, rất vuii được gập bạn\n",
      "----------\n",
      "\n",
      "--- DEBUG SAMPLE ---\n",
      "### ID: 4 | COLUMN: text\n",
      "+   ORIGINAL: \n",
      "\"Lãnh thổ Thành Vatican là bộ phận của Mons Vaticanus (Đồi Vatican), và của phạm vi Vatican cũ lân cận, tại đây có Vương cung thánh đường Thánh Pedro, Điện Tông Tòa, Nhà nguyện Sistina, và các bảo tàng cùng nhiều công trình khác. Khu vực là bộ phận của rione Borgo thuộc Roma cho đến năm 1929. Thành Vatican tách khỏi thành phố, được bảo vệ trong các tường thời Giáo hoàng Leo IV, sau đó mở rộng ra các tường được xây thời Paulus III/Pius IV/Urbanus VIII như hiện nay. Hiệp ước Laterano vào năm 1929 tạo ra nhà nước Vatican, ranh giới của lãnh thổ đề xuất chịu ảnh hưởng bởi thực tế là phần lớn chúng đều nằm trong vòng tường này. Tại một số đoạn biên giới không có tường, và giới hạn của các toà nhà nhất định tạo thành bộ phận của biên giới, và một đoạn nhỏ biên giới là một bức tường xây vào thời hiện đại. Trong lãnh thổ Vatican có Quảng trường Thánh Pedro, tách khỏi lãnh thổ Ý chỉ qua một vạch màu trắng dọc giới hạn của quảng trường, và giáp với Piazza Pio XII. Quảng trường Thánh Pedro nối liền với đại lộ Via della Conciliazione, đại lộ này chạy thẳng đến bờ sông Tevere. Lối vào lớn này do các kiến trúc sư Piacentini và Spaccarelli thiết kế, theo mong muốn của Benito Mussolini và được thoả thuận với giáo hội sau khi ký kết Hiệp định Laterano. Cũng theo hiệp định này, một số tài sản của Toà Thánh nằm trong lãnh thổ Ý, đáng chú ý nhất là Cung điện Giáo hoàng Castel Gandolfo và các đại vương cung thánh đường, chúng được hưởng vị thế đặc quyền ngoài lãnh thổ giống như các đại sứ quán nước ngoài. \"\n",
      "\n",
      "+ CORRECTED (model returned): \n",
      "Lãnh thổ Thành Vatican là bộ phận của Mons Vaticanus (Đồi Vatican), và của phạm vi Vatican cũ lân cận, tại đây có Vương cung thánh đường Thánh Pedro, Điện Tông Tòa, Nhà nguyện Sistina, và các bảo tàng cùng nhiều công trình khác. Khu vực là bộ phận của rione Borgo thuộc Roma cho đến năm 1929. Thành Vatican tách khỏi thành phố, được bảo vệ trong các tường thời Giáo hoàng Leo IV, sau đó mở rộng ra các tường được xây thời Paulus III/Pius IV/Urban VIII như hiện nay. Hiệp ước Laterano vào năm 1929 tạo ra nhà nước Vatican, ranh giới của lãnh thổ đề xuất chịu ảnh hưởng bởi thực tế là phần lớn chúng đều nằm trong vòng tường này. Tại một số đoạn biên giới không có tường, và giới hạn của các toà nhà nhất định tạo thành bộ phận của biên giới, và một đoạn nhỏ biên giới là một bức tường xây vào thời hiện đại. Trong lãnh thổ Vatican có Quảng trường Thánh Pedro, tách khỏi lãnh thổ Ý chỉ qua một vạch màu trắng dọc giới hạn của quảng trường, và giáp với Piazza Pio XII. Quảng trường Thánh Pedro nối liền với đại lộ Via della Conciliazione, đại lộ này chạy thẳng đến bờ sông Tevere. Lối vào lớn này do các kiến trúc sư Piacentini và Spaccarelli thiết kế, theo mong muốn của Benito Mussolini và được thoả thuận với giáo hội sau khi ký kết Hiệp định Laterano. Cũng theo hiệp định này, một số tài sản của Toà Thánh nằm trong lãnh thổ Ý, đáng chú ý nhất là Cung điện Giáo hoàng Castel Gandolfo và các đại vương cung thánh đường, chúng được hưởng vị thế đặc quyền ngoài lãnh thổ giống như các đại sứ quán nước ngoài.\n",
      "\n",
      "--- METRICS & THRESHOLDS ---\n",
      "  - Base Similarity: 0.9990\n",
      "  - Direct Similarity: 0.9983\n",
      "\n",
      "--- FACT CHECKING ---\n",
      "  - Original Numbers (len: 2): \n",
      "{'1929', '1929.'}\n",
      "  - Corrected Numbers (len: 2): \n",
      "{'1929', '1929.'}\n",
      "  - Original Nouns (len: 34): \n",
      "{'Borgo', 'Piazza Pio XII. Quảng', 'Tại', 'Khu', 'Sistina', 'Toà Thánh', 'Leo IV', 'Trong', 'Benito Mussolini', 'Đồi Vatican', 'Laterano', 'Thánh Pedro', 'Urbanus VIII', 'Thành Vatican', 'Hiệp', 'Cung', 'Mons Vaticanus', 'Lãnh', 'Piacentini', 'Quảng', 'Conciliazione', 'Laterano. Cũng', 'Spaccarelli', 'Vương', 'Tevere. Lối', 'Via', 'Paulus III', 'Roma', 'Pius IV', 'Điện Tông Tòa', 'Nhà', 'Vatican', 'Castel Gandolfo', 'Giáo'}\n",
      "  - Corrected Nouns (len: 34): \n",
      "{'Borgo', 'Piazza Pio XII. Quảng', 'Tại', 'Khu', 'Sistina', 'Toà Thánh', 'Leo IV', 'Trong', 'Benito Mussolini', 'Đồi Vatican', 'Laterano', 'Thánh Pedro', 'Thành Vatican', 'Hiệp', 'Cung', 'Mons Vaticanus', 'Lãnh', 'Piacentini', 'Quảng', 'Conciliazione', 'Laterano. Cũng', 'Spaccarelli', 'Vương', 'Tevere. Lối', 'Via', 'Paulus III', 'Roma', 'Pius IV', 'Điện Tông Tòa', 'Nhà', 'Vatican', 'Castel Gandolfo', 'Giáo', 'Urban VIII'}\n",
      "\n",
      "--- DECISION ---\n",
      "RESULT: REJECTED\n",
      "REASON: proper_nouns_altered: {'Urbanus VIII', 'Urban VIII'}\n",
      "FINAL TEXT: \n",
      "\"Lãnh thổ Thành Vatican là bộ phận của Mons Vaticanus (Đồi Vatican), và của phạm vi Vatican cũ lân cận, tại đây có Vương cung thánh đường Thánh Pedro, Điện Tông Tòa, Nhà nguyện Sistina, và các bảo tàng cùng nhiều công trình khác. Khu vực là bộ phận của rione Borgo thuộc Roma cho đến năm 1929. Thành Vatican tách khỏi thành phố, được bảo vệ trong các tường thời Giáo hoàng Leo IV, sau đó mở rộng ra các tường được xây thời Paulus III/Pius IV/Urbanus VIII như hiện nay. Hiệp ước Laterano vào năm 1929 tạo ra nhà nước Vatican, ranh giới của lãnh thổ đề xuất chịu ảnh hưởng bởi thực tế là phần lớn chúng đều nằm trong vòng tường này. Tại một số đoạn biên giới không có tường, và giới hạn của các toà nhà nhất định tạo thành bộ phận của biên giới, và một đoạn nhỏ biên giới là một bức tường xây vào thời hiện đại. Trong lãnh thổ Vatican có Quảng trường Thánh Pedro, tách khỏi lãnh thổ Ý chỉ qua một vạch màu trắng dọc giới hạn của quảng trường, và giáp với Piazza Pio XII. Quảng trường Thánh Pedro nối liền với đại lộ Via della Conciliazione, đại lộ này chạy thẳng đến bờ sông Tevere. Lối vào lớn này do các kiến trúc sư Piacentini và Spaccarelli thiết kế, theo mong muốn của Benito Mussolini và được thoả thuận với giáo hội sau khi ký kết Hiệp định Laterano. Cũng theo hiệp định này, một số tài sản của Toà Thánh nằm trong lãnh thổ Ý, đáng chú ý nhất là Cung điện Giáo hoàng Castel Gandolfo và các đại vương cung thánh đường, chúng được hưởng vị thế đặc quyền ngoài lãnh thổ giống như các đại sứ quán nước ngoài. \"\n",
      "----------\n",
      "Successfully saved 6 processed rows to ./vihallu_public_test_correction/fixed-text-correction.csv\n",
      "\n",
      "--- Summary ---\n",
      "Đã xử lý tổng: 6 hàng (trong tổng 6).\n",
      "Có 2 ids thay đổi.\n"
     ]
    }
   ],
   "source": [
    "corrected_ids = set()\n",
    "if os.path.exists(DEBUG_LOG_CSV): \n",
    "    os.remove(DEBUG_LOG_CSV)\n",
    "\n",
    "try:\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing\", leave=True, dynamic_ncols=True):\n",
    "        # nếu signal handler đã set stop_requested, thoát vòng lặp an toàn\n",
    "        if stop_requested:\n",
    "            print(\"\\nStop requested — breaking main loop now.\")\n",
    "            break\n",
    "        \n",
    "        current_id = row.get('id', index)\n",
    "        row_copy = row.copy()\n",
    "        row_was_processed = False\n",
    "        \n",
    "        for col in COLUMNS_TO_FIX:\n",
    "            if col not in row or pd.isna(row[col]): \n",
    "                continue\n",
    "            \n",
    "            original_text = row[col]\n",
    "            debug_flag = (index < SAMPLE_DEBUG_N)\n",
    "            \n",
    "            corrected_text = correct_vietnamese_spelling(original_text, current_id, col, debug_print=debug_flag)\n",
    "            \n",
    "            # đánh dấu row này đã được xử lý (đã cố gắng sửa ít nhất 1 cột)\n",
    "            row_was_processed = True\n",
    "            \n",
    "            if str(original_text) != str(corrected_text):\n",
    "                corrected_ids.add(current_id)\n",
    "                row_copy[col] = corrected_text\n",
    "        \n",
    "        # nếu đã xử lý (đã thử sửa ít nhất 1 cột), lưu row vào processed_rows\n",
    "        if row_was_processed:\n",
    "            processed_indices.add(index)\n",
    "\n",
    "            # chuẩn hóa trường predict_label nếu không tồn tại\n",
    "            if 'predict_label' not in row_copy.index:\n",
    "                row_copy['predict_label'] = ''\n",
    "\n",
    "\n",
    "            processed_rows.append({\n",
    "                'index': index,\n",
    "                'context': row_copy.get('context', ''),\n",
    "                'prompt': row_copy.get('prompt', ''),\n",
    "                'response': row_copy.get('response', ''),\n",
    "                'predict_label': row_copy.get('predict_label', '')\n",
    "            })\n",
    "                \n",
    "except KeyboardInterrupt:\n",
    "    # trong notebook: Ctrl+C\n",
    "    print(\"\\nNgắt bởi người dùng. Đang lưu các hàng đã xử lý && giải phóng RAM/VRAM...\")\n",
    "    try:\n",
    "        save_processed_rows_and_exit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error while saving: {e}\")\n",
    "\n",
    "except Exception as e: \n",
    "    print(f\"\\nAn error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Khi hoàn tất (không có Ctrl+C), cũng chỉ lưu các hàng đã được xử lý\n",
    "    save_processed_rows_and_exit()\n",
    "\n",
    "\n",
    "    print(\"\\n--- Summary ---\")\n",
    "    total_rows = df.shape[0]\n",
    "    changed = len(corrected_ids)\n",
    "    print(f\"Đã xử lý tổng: {len(processed_rows)} hàng (trong tổng {total_rows}).\")\n",
    "    print(f\"Có {changed} ids thay đổi.\")\n",
    "\n",
    "\n",
    "    # luôn giải phóng bộ nhớ ở cuối\n",
    "    try:\n",
    "        del df, df_global, corrected_ids\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
