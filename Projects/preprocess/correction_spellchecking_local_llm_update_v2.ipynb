{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e17326c9",
   "metadata": {},
   "source": [
    "# Correction Vietnamese local strict spelling \n",
    "\n",
    "- Tích hợp hàm trích xuất danh từ riêng và số thông minh hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92736a38",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bc3c342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "\n",
    "import signal\n",
    "import logging\n",
    "import pandas as pd\n",
    "import torch\n",
    "import unicodedata\n",
    "import difflib\n",
    "from underthesea import ner\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GenerationConfig\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.notebook import tqdm\n",
    "from huggingface_hub import snapshot_download\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604e4d21",
   "metadata": {},
   "source": [
    "# Suppress Transformers Warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6917915",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger(\"transformers.generation.utils\").setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bf91c3",
   "metadata": {},
   "source": [
    "# LOAD .env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb792ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dotenv_path: /home/guest/Projects/DSC2025/BAN/preprocess/../envs/.env\n"
     ]
    }
   ],
   "source": [
    "dotenv_path = os.path.join(os.getcwd(), \"..\", \"envs\", \".env\")\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path)\n",
    "else:\n",
    "    load_dotenv()\n",
    "    \n",
    "print(f\"dotenv_path: {dotenv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25697001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!\n"
     ]
    }
   ],
   "source": [
    "HF_TOKEN = (\n",
    "    os.getenv(\"HUGGING_FACE_TOKEN\") or\n",
    "    os.getenv(\"HUGGINGFACE_TOKEN\") or\n",
    "    os.getenv(\"HF_TOKEN\") or\n",
    "    os.getenv(\"HUNGGING_FACE_TOKEN\") or\n",
    "    None\n",
    ")\n",
    "\n",
    "if HF_TOKEN:\n",
    "    print(\"OK!\")\n",
    "else:\n",
    "    print(\"Add HF key to .env\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f7ee783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public_test_correction_folder_path: ./vihallu_public_test_correction\n",
      "debug_correction_villua_folder_path: ./debug_vihallu_public_test_correction\n"
     ]
    }
   ],
   "source": [
    "public_test_correction_folder = \"./vihallu_public_test_correction\"\n",
    "os.makedirs(public_test_correction_folder, exist_ok=True)\n",
    "print(f\"public_test_correction_folder_path: {public_test_correction_folder}\")\n",
    "\n",
    "debug_correction_villua_folder = \"./debug_vihallu_public_test_correction\"\n",
    "os.makedirs(debug_correction_villua_folder, exist_ok=True)\n",
    "print(f\"debug_correction_villua_folder_path: {debug_correction_villua_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c58ba2",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90ba0c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"VietnamAIHub/Vietnamese_llama2_7B_8K_SFT_General_domain\"\n",
    "INPUT_CSV = \"./vihallu-public-test.csv\"\n",
    "OUTPUT_CSV =  os.path.join(public_test_correction_folder, \"fixed-vihallu-public-test_final.csv\")\n",
    "COLUMNS_TO_FIX = ['context', 'prompt', 'response']\n",
    "\n",
    "# <<< THAY ĐỔI: Thêm BATCH_SIZE để xử lý hàng loạt >>>\n",
    "BATCH_SIZE = 4 # Bạn có thể điều chỉnh tùy theo VRAM\n",
    "MAX_INPUT_LENGTH = 1024\n",
    "WORD_COUNT_TOLERANCE = 3\n",
    "ACCEPT_SIMILARITY_THRESHOLD = 0.88\n",
    "LENGTH_CHANGE_ALLOWED_RATIO = 0.15 \n",
    "STRICT_BASE_SIMILARITY      = 0.96\n",
    "LENIENT_BASE_SIMILARITY     = 0.93      \n",
    "\n",
    "\n",
    "DEBUG_LOG_CSV =  os.path.join(debug_correction_villua_folder, \"debug_corrections_final.csv\")\n",
    "SAMPLE_DEBUG_N = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca64caa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0\n",
      "+ Original: Nguồn thhu chinh của thuê truc tiếp la nhủg ai?\n",
      "-> Correction: Nguồn thu chính của thuế trực tiếp là những ai?\n",
      "\n",
      "index: 1\n",
      "+ Original: Ý nhĩa cũa viẹc tổ chưc cuôc thi Intervision là gì z?\n",
      "-> Correction: Ý nghĩa của việc tổ chức cuộc thi Intervision là gì?\n",
      "\n",
      "index: 2\n",
      "+ Original: thủ đô ha nội là trung tâm văn hóa lớn nhất nc ta\n",
      "-> Correction: Thủ đô Hà Nội là trung tâm văn hóa lớn nhất nước ta.\n",
      "\n",
      "index: 3\n",
      "+ Original: ...không đứng đầu thế giới về dự trữ ngoại tệ?\n",
      "-> Correction: ...không đứng đầu thế giới về dự trữ ngoại tệ?\n",
      "\n",
      "index: 4\n",
      "+ Original: Các nhà khoa học muốn tìm đến S.J thì phải đến đâu?\n",
      "-> Correction: Các nhà khoa học muốn tìm đến S.J thì phải đến đâu?\n",
      "\n",
      "index: 5\n",
      "+ Original: Mô hình encoder-decoder sử dụng trong bài này là T5.\n",
      "-> Correction: Mô hình encoder-decoder sử dụng trong bài này là T5.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FEW_SHOTS = [\n",
    "    (\"Nguồn thhu chinh của thuê truc tiếp la nhủg ai?\", \"Nguồn thu chính của thuế trực tiếp là những ai?\"),\n",
    "    (\"Ý nhĩa cũa viẹc tổ chưc cuôc thi Intervision là gì z?\", \"Ý nghĩa của việc tổ chức cuộc thi Intervision là gì?\"),\n",
    "    (\"thủ đô ha nội là trung tâm văn hóa lớn nhất nc ta\", \"Thủ đô Hà Nội là trung tâm văn hóa lớn nhất nước ta.\"),\n",
    "    (\"...không đứng đầu thế giới về dự trữ ngoại tệ?\", \"...không đứng đầu thế giới về dự trữ ngoại tệ?\"),\n",
    "    (\"Các nhà khoa học muốn tìm đến S.J thì phải đến đâu?\", \"Các nhà khoa học muốn tìm đến S.J thì phải đến đâu?\"),\n",
    "    (\"Mô hình encoder-decoder sử dụng trong bài này là T5.\", \"Mô hình encoder-decoder sử dụng trong bài này là T5.\"),\n",
    "]\n",
    "\n",
    "for idx, few_shot in enumerate(FEW_SHOTS):\n",
    "    print(f\"index: {idx}\\n+ Original: {few_shot[0]}\\n-> Correction: {few_shot[1]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294f3a57",
   "metadata": {},
   "source": [
    "# Device info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f36dbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "\n",
      "=== CONFIG: ===\n",
      "Model: VietnamAIHub/Vietnamese_llama2_7B_8K_SFT_General_domain\n",
      "INPUT CSV: ./vihallu-public-test.csv\n",
      "OUTPUT CSV: ./vihallu_public_test_correction/fixed-vihallu-public-test_final.csv\n",
      "DEBUG_LOG_CSV: ./debug_vihallu_public_test_correction/debug_corrections_final.csv\n",
      "\n",
      "=== Columns to fix: ['context', 'prompt', 'response'] ===\n",
      "MAX_INPUT_LENGTH: 1024\n",
      "WORD_COUNT_TOLERANCE: 3\n",
      "SMAMPLE_DEBUG_N:20\n",
      "\n",
      "=== DEBUG: Thresholds (Final Optimized) ===\n",
      "STRICT (prompt, response): BASE_SIMILARITY >= 0.96\n",
      "LENGTH_CHANGE_ALLOWED_RATIO: 0.15\n",
      "LENIENT (context): BASE_SIMILARITY >= 0.93\n",
      "GENERAL: ACCEPT_SIMILARITY >= 0.88\n",
      "====================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"\\n=== CONFIG: ===\")\n",
    "print(f\"Model: {MODEL}\")\n",
    "print(f\"INPUT CSV: {INPUT_CSV}\")\n",
    "print(f\"OUTPUT CSV: {OUTPUT_CSV}\")\n",
    "print(f\"DEBUG_LOG_CSV: {DEBUG_LOG_CSV}\")\n",
    "\n",
    "print(f\"\\n=== Columns to fix: {COLUMNS_TO_FIX} ===\")\n",
    "print(f\"MAX_INPUT_LENGTH: {MAX_INPUT_LENGTH}\")\n",
    "print(f\"WORD_COUNT_TOLERANCE: {WORD_COUNT_TOLERANCE}\")\n",
    "print(f\"SMAMPLE_DEBUG_N:{SAMPLE_DEBUG_N}\")\n",
    "\n",
    "print(\"\\n=== DEBUG: Thresholds (Final Optimized) ===\")\n",
    "print(f\"STRICT (prompt, response): BASE_SIMILARITY >= {STRICT_BASE_SIMILARITY}\")\n",
    "print(f\"LENGTH_CHANGE_ALLOWED_RATIO: {LENGTH_CHANGE_ALLOWED_RATIO}\")\n",
    "print(f\"LENIENT (context): BASE_SIMILARITY >= {LENIENT_BASE_SIMILARITY}\")\n",
    "print(f\"GENERAL: ACCEPT_SIMILARITY >= {ACCEPT_SIMILARITY_THRESHOLD}\")\n",
    "print(\"====================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b319cc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(INPUT_CSV): \n",
    "    raise FileNotFoundError(f\"Không tìm thấy file '{INPUT_CSV}'.\")\n",
    "\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "df_global = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1331a0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing output file at ./vihallu_public_test_correction/fixed-vihallu-public-test_final.csv. Resuming process.\n",
      "Loaded 32 completed rows. Skipping them.\n",
      "Resuming with 968 remaining rows out of 1000.\n"
     ]
    }
   ],
   "source": [
    "processed_indices = set()\n",
    "if os.path.exists(OUTPUT_CSV):\n",
    "    print(f\"Found existing output file at {OUTPUT_CSV}. Resuming process.\")\n",
    "    try:\n",
    "        df_processed = pd.read_csv(OUTPUT_CSV)\n",
    "        # File output của chúng ta có cột 'index' do hàm save tạo ra\n",
    "        if 'index' in df_processed.columns:\n",
    "            processed_indices = set(df_processed['index'])\n",
    "            print(f\"Loaded {len(processed_indices)} completed rows. Skipping them.\")\n",
    "            \n",
    "            original_row_count = len(df)\n",
    "            df = df[~df.index.isin(processed_indices)]\n",
    "            print(f\"Resuming with {len(df)} remaining rows out of {original_row_count}.\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"Output file is empty. Starting from the beginning.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read processed rows from {OUTPUT_CSV}. Starting from scratch. Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed8523aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bộ chứa cho các row đã được xử lý (chỉ những row này sẽ được lưu vào OUTPUT_CSV)\n",
    "processed_rows = []\n",
    "processed_indices = set()\n",
    "# STOP flag (sẽ được set True bởi signal handler)\n",
    "stop_requested = False\n",
    "\n",
    "\n",
    "# hàm lưu hiện trạng (chỉ lưu các cột index, context, prompt, response, predict_label)\n",
    "def save_processed_rows_and_exit():\n",
    "    if not processed_rows:\n",
    "        print(\"No new processed rows to save.\")\n",
    "        return\n",
    "\n",
    "    # 1. Lấy dữ liệu từ bộ chứa\n",
    "    output_df = pd.DataFrame(processed_rows)\n",
    "\n",
    "    # 2. Đảm bảo đủ các cột cần thiết (Logic quan trọng từ code của bạn)\n",
    "    required_cols = [\"index\", \"id\", \"context\", \"prompt\", \"response\", \"predict_label\"]\n",
    "    for c in required_cols:\n",
    "        if c not in output_df.columns:\n",
    "            output_df[c] = \"\"\n",
    "    \n",
    "    # Sắp xếp lại các cột theo đúng thứ tự\n",
    "    output_df = output_df[required_cols]\n",
    "\n",
    "    # 3. Kiểm tra xem file đã tồn tại chưa để quyết định ghi header (Logic của tôi)\n",
    "    file_exists = os.path.exists(OUTPUT_CSV) and os.path.getsize(OUTPUT_CSV) > 0\n",
    "\n",
    "    # 4. Ghi file ở chế độ append (ghi tiếp)\n",
    "    output_df.to_csv(OUTPUT_CSV, mode='a', header=not file_exists, index=False, encoding=\"utf-8-sig\")\n",
    "    \n",
    "    print(f\"Successfully saved/appended {len(output_df)} new rows to {OUTPUT_CSV}\")\n",
    "    processed_rows.clear() # Xóa list sau khi lưu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bcb2cb",
   "metadata": {},
   "source": [
    "## Ctrl+C handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17b0701c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected IPython environment — relying on KeyboardInterrupt for interruption handling.\n"
     ]
    }
   ],
   "source": [
    "def signal_handler(sig, frame):\n",
    "    global stop_requested\n",
    "    print(\"\\n\\n[Ctrl+C] Request received — will stop after current row and save processed rows...\")\n",
    "    stop_requested = True\n",
    "\n",
    "\n",
    "if 'ipykernel' not in sys.modules and 'IPython' not in sys.modules:\n",
    "    signal.signal(signal.SIGINT, signal_handler)\n",
    "else:\n",
    "    print(\"Detected IPython environment — relying on KeyboardInterrupt for interruption handling.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861d913c",
   "metadata": {},
   "source": [
    "## tokenizer/model setup -> set up 8-bit if not fallback to origin 16-bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ca3cd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_kwargs = {\n",
    "    \"use_fast\": False, \n",
    "    \"token\": HF_TOKEN, \n",
    "    \"padding_side\": \"left\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b350b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer_local_or_remote(model_id):\n",
    "    try:\n",
    "        tok = AutoTokenizer.from_pretrained(model_id, local_files_only=True, **{k: v for k, v in tokenizer_kwargs.items() if k != \"token\"})\n",
    "    except Exception:\n",
    "        tok = AutoTokenizer.from_pretrained(model_id, **tokenizer_kwargs)\n",
    "    if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
    "    return tok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "512f3a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_local_or_snapshot(model_id):\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub\")\n",
    "    print(f\"Cache huggingface path: {cache_dir}\")\n",
    "    loading_strategies = [\n",
    "        # {\"name\": \"8bit\", \"kwargs\": {\"quantization_config\": BitsAndBytesConfig(load_in_8bit=True), \"device_map\": \"auto\", \"token\": HF_TOKEN, \"cache_dir\": cache_dir}},\n",
    "        {\"name\": \"4bit\", \"kwargs\": {\"quantization_config\": BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16), \"device_map\": \"auto\", \"token\": HF_TOKEN, \"cache_dir\": cache_dir}},\n",
    "        {\"name\": \"bfloat16\", \"kwargs\": {\"torch_dtype\": torch.bfloat16, \"device_map\": \"auto\", \"token\": HF_TOKEN, \"cache_dir\": cache_dir}},\n",
    "    ]\n",
    "    \n",
    "    last_e = None\n",
    "    for strat in loading_strategies:    \n",
    "        try:\n",
    "            print(f\"Trying to load with config: {strat['name']}\")\n",
    "            m = AutoModelForCausalLM.from_pretrained(model_id, **strat[\"kwargs\"])\n",
    "            print(f\"Successfully loaded with config: {strat['name']}\")\n",
    "            model_path = (\n",
    "                snapshot_download(model_id, token=HF_TOKEN, cache_dir=cache_dir)\n",
    "                if HF_TOKEN\n",
    "                else cache_dir\n",
    "            )\n",
    "            return m, model_path\n",
    "        except Exception as e:\n",
    "            print(f\"Failed with config: {strat['name']} - {type(e).__name__}: {e}\")\n",
    "            last_e = e\n",
    "    raise RuntimeError(\"Cannot load model\") from last_e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a934c1e3",
   "metadata": {},
   "source": [
    "## Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b06baa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaTokenizer(name_or_path='VietnamAIHub/Vietnamese_llama2_7B_8K_SFT_General_domain', vocab_size=32000, model_max_length=8192, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t32000: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = load_tokenizer_local_or_remote(MODEL)\n",
    "tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e0cc05",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f3124e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Cache huggingface path: /home/guest/.cache/huggingface/hub\n",
      "Trying to load with config: 4bit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de0ff8d8fa9a4c209a91964c599cec88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded with config: 4bit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4541b10227d74057b8dd6e24569abab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading model...\")\n",
    "model, model_source = load_model_local_or_snapshot(MODEL)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6c8f1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "Model loaded from: /home/guest/.cache/huggingface/hub/models--VietnamAIHub--Vietnamese_llama2_7B_8K_SFT_General_domain/snapshots/12466d4619deed5fb5972760c082d7382151376c\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    model_device = next(model.parameters()).device\n",
    "except: \n",
    "    model_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "print(f\"Model device: {model_device}\")\n",
    "print(f\"Model loaded from: {model_source}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79df2a41",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6bcaea",
   "metadata": {},
   "source": [
    "## Remove diaccritics and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85f60a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_diacritics_and_punct(s: str):\n",
    "    s = unicodedata.normalize('NFD', s)\n",
    "    s = ''.join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    s = unicodedata.normalize('NFC', s)\n",
    "    s = re.sub(r'[^\\w\\s]', '', s)\n",
    "    return s.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a198a1",
   "metadata": {},
   "source": [
    "## Nomalize spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b3876cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_spaces(s: str): \n",
    "    return re.sub(r'\\s+', ' ', s).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880ff441",
   "metadata": {},
   "source": [
    "# Calculate String Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "913ca251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_similarity(a, b): \n",
    "    return difflib.SequenceMatcher(None, a, b).ratio()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ec1b89",
   "metadata": {},
   "source": [
    "## Extract number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0df84343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_numbers(text: str):\n",
    "    \"\"\"Trích xuất tất cả các chuỗi số, bao gồm cả số có dấu phẩy và dấu chấm.\"\"\"\n",
    "    return set(re.findall(r'\\d+[,.]?\\d*', text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b7f225",
   "metadata": {},
   "source": [
    "## Extract proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2bfb835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_proper_nouns(text: str):\n",
    "    \"\"\"\n",
    "    Trích xuất các thực thể tên riêng (danh từ riêng) sử dụng underthesea NER.\n",
    "    \"\"\"\n",
    "    # ner trả về list các tuple: (text, tag, start_pos, end_pos)\n",
    "    # Chúng ta chỉ lấy phần text (phần tử đầu tiên)\n",
    "    if not text or not isinstance(text, str):\n",
    "        return set()\n",
    "    try:\n",
    "        # ner() trả về list các tuple: (entity_text, entity_type)\n",
    "        entities = ner(text)\n",
    "        # Chúng ta chỉ lấy phần text của các thực thể\n",
    "        proper_nouns = {entity[0] for entity in entities}\n",
    "        return proper_nouns\n",
    "    except Exception:\n",
    "        # Trả về set rỗng nếu underthesea gặp lỗi\n",
    "        return set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3671183",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecda240",
   "metadata": {},
   "source": [
    "## Prompt & Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e63b2fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_correction_prompt_llama2(original_text):\n",
    "    fs_examples = \"\\n\".join(\n",
    "        [f\"Văn bản gốc: \\\"{o}\\\"\\nVăn bản đã sửa: \\\"{c}\\\"\" for o, c in FEW_SHOTS]\n",
    "    )\n",
    "    system_prompt = (\n",
    "        \"Bạn là một robot hiệu đính MÁY MÓC và CẨN TRỌNG.\\n\"\n",
    "        \"**QUY TẮC SỐ 1 (QUAN TRỌNG NHẤT):** KHÔNG THAY ĐỔI Ý NGHĨA. \"\n",
    "        \"Giữ nguyên từ đúng ngữ nghĩa (ví dụ: 'ngoại tệ'), giữ nguyên từ viết tắt không rõ (ví dụ: 'S.J').\\n\"\n",
    "        \"**QUY TẮC SỐ 2:** CHỈ SỬA lỗi chính tả, lỗi gõ phím, lỗi dấu thanh, lỗi viết hoa.\\n\"\n",
    "        \"**QUY TẮC SỐ 3:** Nếu văn bản đã đúng, lặp lại y hệt.\\n\"\n",
    "        \"**QUY TẮC SỐ 4:** TUYỆT ĐỐI không thay đổi, thêm, hoặc xóa bất kỳ con số hay tên riêng nào (ví dụ: '1995', 'Hà Nội', 'S.J').\"\n",
    "    )\n",
    "    user_prompt = (\n",
    "        \"Các ví dụ:\\n\"\n",
    "        f\"{fs_examples}\\n\\n\"\n",
    "        \"Sửa văn bản sau. Chỉ trả về văn bản đã sửa.\\n\"\n",
    "        f\"Văn bản gốc: \\\"{original_text}\\\"\"\n",
    "    )\n",
    "    return f\"<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{user_prompt} [/INST]\\nVăn bản đã sửa: \\\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28996f7",
   "metadata": {},
   "source": [
    "## Generate from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a948217b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_model(prompt, original_text_for_token_limit):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_INPUT_LENGTH)\n",
    "    try: inputs = {k: v.to(model_device) for k, v in inputs.items()}\n",
    "    except Exception: pass\n",
    "    \n",
    "    input_token_length = len(tokenizer.encode(original_text_for_token_limit, add_special_tokens=False))\n",
    "    \n",
    "    dynamic_min_new_tokens = int(input_token_length * 0.8)\n",
    "    dynamic_max_new_tokens = int(input_token_length * 1.5) + 50\n",
    "    \n",
    "    gen_cfg = GenerationConfig(\n",
    "        min_new_tokens=dynamic_min_new_tokens,\n",
    "        max_new_tokens=dynamic_max_new_tokens,\n",
    "        do_sample=False, \n",
    "        num_beams=1, \n",
    "        repetition_penalty=1.1, \n",
    "        early_stopping=False,\n",
    "        pad_token_id=tokenizer.eos_token_id, \n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        out = model.generate(**inputs, generation_config=gen_cfg)\n",
    "    full_decoded = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    \n",
    "    if \"[/INST]\" in full_decoded:\n",
    "        response_part = full_decoded.split(\"[/INST]\")[-1].strip()\n",
    "        if response_part.lower().startswith(\"văn bản đã sửa:\"):\n",
    "            response_part = response_part[len(\"Văn bản đã sửa:\"):].strip()\n",
    "        corrected = response_part.split('\\n')[0].strip()\n",
    "        corrected = re.sub(r'^[\"\\']|[\"\\']$', '', corrected)\n",
    "        \n",
    "        if corrected.endswith('\"'): \n",
    "            corrected = corrected[:-1]\n",
    "        return corrected\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca3a110",
   "metadata": {},
   "source": [
    "# Correction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "395d8a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "correction_cache = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81229984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_correction(original_text, corrected_text, col_name, row_id, debug_print=False):\n",
    "    \"\"\"\n",
    "    Hàm này KHÔNG gọi LLM. Nó chỉ nhận vào văn bản gốc và văn bản đã sửa,\n",
    "    sau đó áp dụng các quy tắc guardrails để quyết định có chấp nhận thay đổi không.\n",
    "    \"\"\"\n",
    "    accepted = False\n",
    "    final_text = original_text\n",
    "    reason = \"not_changed\"\n",
    "    base_sim = 1.0\n",
    "\n",
    "    if corrected_text and corrected_text != original_text:\n",
    "        current_base_threshold = STRICT_BASE_SIMILARITY if col_name in ['prompt', 'response'] else LENIENT_BASE_SIMILARITY\n",
    "        orig_words = normalize_spaces(original_text).split()\n",
    "        corr_words = normalize_spaces(corrected_text).split()\n",
    "        base_sim = string_similarity(remove_diacritics_and_punct(original_text), remove_diacritics_and_punct(corrected_text))\n",
    "\n",
    "        passes_initial_checks = False\n",
    "        if abs(len(orig_words) - len(corr_words)) > WORD_COUNT_TOLERANCE:\n",
    "            reason = f\"word_count_mismatch: original_words:{len(orig_words)} vs correction_word: {len(corr_words)}\"\n",
    "        elif base_sim < current_base_threshold:\n",
    "            reason = f\"base_similarity_too_low_{base_sim:.2f}_(threshold:{current_base_threshold})\"\n",
    "        elif len(original_text) > 0 and abs(len(corrected_text) - len(original_text)) / len(original_text) > LENGTH_CHANGE_ALLOWED_RATIO:\n",
    "            reason = \"length_changed_too_much\"\n",
    "        elif string_similarity(original_text, corrected_text) < ACCEPT_SIMILARITY_THRESHOLD:\n",
    "            reason = f\"low_similarity_{string_similarity(original_text, corrected_text):.2f}\"\n",
    "        elif original_text.endswith('?') and not corrected_text.endswith('?'):\n",
    "            reason = \"question_mark_removed\"\n",
    "        else:\n",
    "            passes_initial_checks = True\n",
    "\n",
    "        if passes_initial_checks:\n",
    "            original_numbers = extract_numbers(original_text)\n",
    "            corrected_numbers = extract_numbers(corrected_text)\n",
    "            original_nouns = extract_proper_nouns(original_text)\n",
    "            corrected_nouns = extract_proper_nouns(corrected_text)\n",
    "            \n",
    "            if original_numbers != corrected_numbers:\n",
    "                reason = f\"numbers_altered: {original_numbers.symmetric_difference(corrected_numbers)}\"\n",
    "                passes_initial_checks = False\n",
    "            elif original_nouns != corrected_nouns:\n",
    "                reason = f\"proper_nouns_altered: {original_nouns.symmetric_difference(corrected_nouns)}\"\n",
    "                passes_initial_checks = False\n",
    "\n",
    "        if passes_initial_checks:\n",
    "            accepted = True\n",
    "            reason = \"accepted_change\"\n",
    "            final_text = corrected_text\n",
    "            \n",
    "    if debug_print:\n",
    "        print(\"\\\\n--- DEBUG SAMPLE ---\")\n",
    "        print(f\"### ID: {row_id} | COLUMN: {col_name}\")\n",
    "        print(f\"+   ORIGINAL: \\\\n{original_text}\")\n",
    "        print(f\"\\\\n+ CORRECTED (model returned): \\\\n{corrected_text}\")\n",
    "        if corrected_text and corrected_text != original_text:\n",
    "            print(\"\\\\n--- METRICS & THRESHOLDS ---\")\n",
    "            print(f\"  - Base Similarity: {base_sim:.4f}\")\n",
    "            print(f\"  - Direct Similarity: {string_similarity(original_text, corrected_text):.4f}\")\n",
    "            print(\"\\\\n--- FACT CHECKING ---\")\n",
    "            print(f\"  - Original Numbers (len: {len(extract_numbers(original_text))}): \\\\n{extract_numbers(original_text)}\")\n",
    "            print(f\"  - Corrected Numbers (len: {len(extract_numbers(corrected_text))}): \\\\n{extract_numbers(corrected_text)}\")\n",
    "            print(f\"  - Original Nouns (len: {len(extract_proper_nouns(original_text))}): \\\\n{extract_proper_nouns(original_text)}\")\n",
    "            print(f\"  - Corrected Nouns (len: {len(extract_proper_nouns(corrected_text))}): \\\\n{extract_proper_nouns(corrected_text)}\")\n",
    "        else:\n",
    "            print(\"\\\\n--- METRICS & THRESHOLDS ---\")\n",
    "            print(\"No change proposed by model.\")\n",
    "        print(\"\\\\n--- DECISION ---\")\n",
    "        print(f\"RESULT: {'ACCEPTED' if accepted else 'REJECTED'}\")\n",
    "        if not accepted:\n",
    "            print(f\"REASON: {reason}\")\n",
    "        print(f\"FINAL TEXT: \\\\n{final_text}\")\n",
    "        print(\"-\" * 10)\n",
    "        \n",
    "    try:\n",
    "        df_row = pd.DataFrame([{\"id\": row_id, \"column\": col_name, \"original\": original_text, \"corrected_model\": corrected_text, \"final_text\": final_text, \"accepted\": accepted, \"reason\": reason, \"similarity\": string_similarity(original_text, corrected_text) if corrected_text else 1.0, \"base_similarity\": base_sim}])\n",
    "        log_header = not os.path.exists(DEBUG_LOG_CSV)\n",
    "        df_row.to_csv(DEBUG_LOG_CSV, index=False, mode='a', header=log_header, encoding=\"utf-8-sig\")\n",
    "    except Exception as e:\n",
    "        if debug_print: print(f\"Warning: cannot write debug csv: {e}\")\n",
    "        \n",
    "    return final_text, accepted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479b5985",
   "metadata": {},
   "source": [
    "# Main Loop for correcting Vietnamese's spelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cf2c02",
   "metadata": {},
   "source": [
    "## --- Giai đoạn 1: Thu thập & Generate ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8af922b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1: Collecting tasks and generating corrections...\n"
     ]
    }
   ],
   "source": [
    "tasks_to_process = []\n",
    "print(\"Phase 1: Collecting tasks and generating corrections...\")\n",
    "\n",
    "# Đảm bảo 'index' là một cột để dùng cho việc resume\n",
    "if 'index' not in df.columns:\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    # Bỏ qua các hàng đã xử lý trong các lần chạy trước\n",
    "    if row['index'] in processed_indices:\n",
    "        continue\n",
    "    \n",
    "    # Thu thập các tác vụ cần xử lý\n",
    "    for col in COLUMNS_TO_FIX:\n",
    "        original_text = str(row.get(col, '')).strip()\n",
    "        if original_text:\n",
    "            tasks_to_process.append({\n",
    "                \"df_index\": row['index'],\n",
    "                \"id\": row.get('id', row['index']),\n",
    "                \"column\": col,\n",
    "                \"original_text\": original_text\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8da32aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'df_index': 32,\n",
       "  'id': 'ae7ea16b-f316-4c1d-b0d0-7f2a1465cdae',\n",
       "  'column': 'context',\n",
       "  'original_text': 'Birmingham là một phố chợ cỡ trung bình vào thời kỳ trung cổ, sau đó trở nên nổi bật ở tầm quốc tế trong thế kỷ 18 khi là trọng tâm trong Khai sáng Midlands rồi cách mạng công nghiệp. Trong cách mạng công nghiệp, Birmingham đi tiên phong trong các tiến bộ toàn cầu về phát triển khoa học, kỹ thuật, và kinh tế, sản sinh hàng loạt sáng kiến giúp đặt một phần nền tảng cho xã hội công nghiệp hiện đại. Đến năm 1791, Birmingham được ca ngợi là \"thị trấn sản xuất đầu tiên trên thế giới\". Thành phố có hồ sơ kinh tế đặc trưng, với hàng nghìn xưởng nhỏ đa dạng về các ngành nghề chuyên biệt và có kỹ năng cao, khuyến khích mức độ sáng tạo và sáng kiến cao khác thường, tạo ra cơ sở kinh tế đa dạng và linh hoạt cho giai đoạn thịnh vượng công nghiệp kéo dài cho đến cuối thế kỷ 20. Động cơ hơi nước công nghiệp được phát minh tại Birmingham, đây có lẽ là sáng kiến quan trọng nhất trong lịch sử Anh Quốc. Công nghiệp hoá dẫn đến mức độ cao về tính lưu động xã hội, nuôi dưỡng một nền văn hoá cấp tiến chính trị có cơ sở đại chúng, khiến thành phố có được ảnh hưởng chính trị lớn nhất trong số các địa phương ngoài Luân Đôn, và có vai trò then chốt trong phát triển dân chủ tại Anh Quốc. Trong Chiến tranh thế giới thứ hai, Birmingham bị Không quân Đức oanh tạc ác liệt. Thiệt hại về cơ sở hạ tầng do chiến tranh, cộng thêm chính sách phá đổ và xây mới của những nhà lập kế hoạch đã dẫn đến tái phát triển quy mô lớn trong các thập niên sau.'},\n",
       " {'df_index': 32,\n",
       "  'id': 'ae7ea16b-f316-4c1d-b0d0-7f2a1465cdae',\n",
       "  'column': 'prompt',\n",
       "  'original_text': 'Làm thế nào mà phát minh của máy tính cá nhân, được cho là ra đời tại Birmingham trong thế kỷ 18, lại trở thành yếu tố then chốt đưa thành phố này từ một phố chợ cỡ trung bình trở thành trung tâm có ảnh hưởng lớn đến chính trị và phát triển dân chủ tại Anh Quốc?'},\n",
       " {'df_index': 32,\n",
       "  'id': 'ae7ea16b-f316-4c1d-b0d0-7f2a1465cdae',\n",
       "  'column': 'response',\n",
       "  'original_text': 'Phát minh của máy tính cá nhân trong thế kỷ 18 tại Birmingham là yếu tố then chốt giúp thành phố này nổi bật quốc tế, mặc dù thực tế máy tính cá nhân chưa ra đời cho đến thế kỷ 20.'},\n",
       " {'df_index': 33,\n",
       "  'id': '6753d640-2495-4e56-b7ba-7a0cba741d42',\n",
       "  'column': 'context',\n",
       "  'original_text': 'Tiếng Anh đã phát triển trong quãng thời gian hơn 1.400 năm. Dạng cổ nhất của tiếng Anh - một tập hợp các phương ngữ Anglo-Frisia được mang đến đảo Anh bởi người Anglo-Saxon vào thế kỷ thứ V - được gọi là tiếng Anh cổ. Thời tiếng Anh trung đại bắt đầu vào cuối thế kỷ XI khi người Norman xâm lược Anh; đây là thời kỳ tiếng Anh được ảnh hưởng bởi tiếng Pháp/Norman. Thời tiếng Anh cận đại bắt đầu vào cuối thế kỷ XV với sự xuất hiện của máy in ép ở Luân Đôn và Kinh Thánh Vua James, và sự khởi đầu của Great Vowel Shift. Nhờ ảnh hưởng toàn cầu của Đế quốc Anh, tiếng Anh hiện đại lan rộng ra toàn thế giới trong thời gian từ thế kỷ XVII đến giữa thế kỷ XX. Qua tất cả các loại truyền thông in ấn và điện tử, cũng như sự nổi lên của Hoa Kỳ như một siêu cường, tiếng Anh trở thành ngôn ngữ dẫn đầu trong giao tiếp quốc tế, là lingua franca ở nhiều khu vực và ở nhiều phạm vi chuyên biệt như khoa học, hàng hải và luật pháp.'},\n",
       " {'df_index': 33,\n",
       "  'id': '6753d640-2495-4e56-b7ba-7a0cba741d42',\n",
       "  'column': 'prompt',\n",
       "  'original_text': 'Trong giai đoạn thế kỷ XVII-XX, liệu sự phát triển của tiếng Anh toàn cầu có phải chủ yếu do sự phát minh của máy in ép ở Luân Đôn vào thế kỷ XV?'},\n",
       " {'df_index': 33,\n",
       "  'id': '6753d640-2495-4e56-b7ba-7a0cba741d42',\n",
       "  'column': 'response',\n",
       "  'original_text': 'Sự phát triển của tiếng Anh toàn cầu từ thế kỷ XVII đến XX không chỉ nhờ ảnh hưởng toàn cầu của Đế quốc Anh mà còn do sự phát triển của nền công nghiệp hàng hải, khám phá các lục địa mới và việc sử dụng tiếng Anh trong các hiệp ước thương mại quốc tế.'},\n",
       " {'df_index': 34,\n",
       "  'id': 'fba48cb8-665c-4d8b-9cd3-affa84bd973a',\n",
       "  'column': 'context',\n",
       "  'original_text': 'Các quốc gia đã xuất hiện ở Trung Quốc vào cuối thiên niên kỷ thứ ba đầu thiên niên kỷ thứ hai TCN. Các cuộc chiến tranh lớn nổ ra giữa các quốc gia ở Trung Đông. Hiệp ước Kadesh, một trong những hiệp ước hòa bình đầu tiên, được ký kết giữa người Hittites và Ai Cập cổ đại khoảng 1275 TCN. Vào thế kỷ thứ VI TCN, Hoàng đế Cyrus II (Cyrus Đại Đế) trỗi dậy kiến lập Đế quốc Ba Tư cường thịnh, chinh phạt được các nước Media, Lydia và Babylon. Ai Cập cũng rơi vào tay của con trai ông là Hoàng đế Cambyses II. Ngoài ra, lịch sử thế giới cổ đại cũng có những quốc gia hùng mạnh khác như đế quốc Maurya (thế kỷ thứ IV TCN), Trung Quốc (thế kỷ thứ III TCN), và Đế quốc La Mã (thế kỷ thứ I TCN).'},\n",
       " {'df_index': 34,\n",
       "  'id': 'fba48cb8-665c-4d8b-9cd3-affa84bd973a',\n",
       "  'column': 'prompt',\n",
       "  'original_text': 'Ai đã ký kết Hiệp ước Kadesh, một trong những hiệp ước hòa bình đầu tiên, giữa người Hittites và Đế quốc Ba Tư vào khoảng năm 1275 TCN?'},\n",
       " {'df_index': 34,\n",
       "  'id': 'fba48cb8-665c-4d8b-9cd3-affa84bd973a',\n",
       "  'column': 'response',\n",
       "  'original_text': 'Hiệp ước Kadesh được ký kết giữa người Hittites và Đế quốc Maurya vào khoảng năm 1275 TCN, nhằm chấm dứt xung đột kéo dài giữa hai bên.'},\n",
       " {'df_index': 35,\n",
       "  'id': 'dd3de1a6-21f0-43fa-a50b-55ba853e1f2d',\n",
       "  'column': 'context',\n",
       "  'original_text': 'Nhiều nơi và nhiều thực thể đã được đặt tên Washington để vinh danh ông. Tên Washington trở thành tên của thủ đô quốc gia, Washington, D.C., một trong hai thủ đô quốc gia trên thế giới đã được đặt tên của một vị tổng thống Mỹ (tên tổng thống Mỹ khác được đặt cho thủ đô của Liberia là Monrovia). Tiểu bang Washington là tiểu bang duy nhất được đặt tên của một vị tổng thống Mỹ. Đại học George Washington và Đại học Washington tại St. Louis được đặt tên ông cũng như Đại học Washington và Lee đã từng đổi thành Học viện Washington khi Washington quyên tặng rất nhiều tiền cho trường này vào năm 1796. Đại học Washington tại Chestertown, Maryland (do hiến chương tiểu bang Maryland thành lập năm 1782) được Washington ủng hộ suốt cuộc đời mình bằng việc trao tặng 50 đồng tiền vàng Anh và phục vụ trong ban hội đồng đặt trách quan khách và điều hành của trường cho đến năm 1789 (khi Washington được bầu làm tổng thống). Vô số thành phố và thị trấn Mỹ có một đường phố mang tên Washington.'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks_to_process[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d46e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2904 text fields to process in this run.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c266da0abb0c4ae1a82efe1734d07fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch Generating:   0%|          | 0/726 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Found {len(tasks_to_process)} text fields to process in this run.\")\n",
    "generated_corrections = {} \n",
    "\n",
    "for i in tqdm(range(0, len(tasks_to_process), BATCH_SIZE), desc=\"Batch Generating\", leave=True, dynamic_ncols=True):\n",
    "    if stop_requested: break\n",
    "    batch_tasks = tasks_to_process[i:i + BATCH_SIZE]\n",
    "    original_texts_batch = [task['original_text'] for task in batch_tasks]\n",
    "    prompts_batch = [make_correction_prompt_llama2(text) for text in original_texts_batch]\n",
    "\n",
    "    inputs = tokenizer(prompts_batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_INPUT_LENGTH).to(model_device)\n",
    "    max_len_in_batch = max(len(tokenizer.encode(t)) for t in original_texts_batch) if original_texts_batch else 0\n",
    "    gen_cfg = GenerationConfig(max_new_tokens=int(max_len_in_batch * 1.5) + 50, do_sample=False, num_beams=1, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        out = model.generate(**inputs, generation_config=gen_cfg)\n",
    "    decoded_results = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "\n",
    "    for j, task in enumerate(batch_tasks):\n",
    "        full_decoded = decoded_results[j]\n",
    "        corrected_text = \"\"\n",
    "        if \"[/INST]\" in full_decoded:\n",
    "            response_part = full_decoded.split(\"[/INST]\")[-1].strip()\n",
    "            if response_part.lower().startswith(\"văn bản đã sửa:\"): response_part = response_part[len(\"Văn bản đã sửa:\"):].strip()\n",
    "            corrected_text = re.sub(r'^[\"\\']|[\"\\']$', '', response_part.split('\\\\n')[0].strip())\n",
    "        generated_corrections[(task['df_index'], task['column'])] = corrected_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c7b0c0",
   "metadata": {},
   "source": [
    "## --- Giai đoạn 2: Xác thực & Lưu ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e64fc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\\\nPhase 2: Validating results and updating DataFrame...\")\n",
    "corrected_ids = set()\n",
    "if os.path.exists(DEBUG_LOG_CSV) and not processed_indices:\n",
    "    os.remove(DEBUG_LOG_CSV)\n",
    "\n",
    "try:\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Validating & Saving\", leave=True, dynamic_ncols=True):\n",
    "        if stop_requested: break\n",
    "        \n",
    "        current_index = row['index']\n",
    "        if current_index in processed_indices: continue\n",
    "\n",
    "        row_copy = row.copy()\n",
    "        row_was_changed = False\n",
    "        \n",
    "        for col in COLUMNS_TO_FIX:\n",
    "            original_text = str(row.get(col, '')).strip()\n",
    "            if not original_text: continue\n",
    "            \n",
    "            corrected_text_from_model = generated_corrections.get((current_index, col), original_text)\n",
    "            debug_flag = (current_index < SAMPLE_DEBUG_N)\n",
    "            final_text, accepted = validate_correction(original_text, corrected_text_from_model, col, row.get('id', current_index), debug_print=debug_flag)\n",
    "            \n",
    "            if accepted:\n",
    "                row_copy[col] = final_text\n",
    "                row_was_changed = True\n",
    "                corrected_ids.add(row.get('id', current_index))\n",
    "\n",
    "        processed_rows.append(row_copy.to_dict())\n",
    "        \n",
    "        # Lưu tiến trình định kỳ\n",
    "        if len(processed_rows) % 20 == 0:\n",
    "            print(f\"\\\\n--- Saving progress at original row index {current_index} ---\")\n",
    "            save_processed_rows_and_exit()\n",
    "            print(\"--- Performing periodic memory cleanup ---\")\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\\\nUser interruption detected. Saving processed rows...\")\n",
    "except Exception as e:\n",
    "    print(f\"\\\\nAn error occurred: {e}\")\n",
    "    print(f\"\\\\nAn unexpected error occurred: {type(e).__name__}: {e}\")\n",
    "finally:\n",
    "    print(\"\\\\nProcessing finished or stopped. Saving any remaining rows...\")\n",
    "    save_processed_rows_and_exit()\n",
    "\n",
    "    print(\"\\\\n--- Summary ---\")\n",
    "    initial_total = len(df_global)\n",
    "    final_processed_count = 0\n",
    "    if os.path.exists(OUTPUT_CSV):\n",
    "        try:\n",
    "            final_processed_count = len(pd.read_csv(OUTPUT_CSV))\n",
    "        except Exception: pass\n",
    "    print(f\"Total processed rows in file: {final_processed_count} / {initial_total}.\")\n",
    "    print(f\"{len(corrected_ids)} unique IDs were changed in this run.\")\n",
    "\n",
    "    try: del df, df_global, corrected_ids\n",
    "    except NameError: pass\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
