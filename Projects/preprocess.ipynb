{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7313fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U ipywidgets widgetsnbextension huggingface_hub tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da099db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Tắt thanh tiến trình của huggingface_hub để tránh lỗi ipywidgets\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f96c655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from underthesea import sent_tokenize\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ec68013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thêm progress_apply vào pandas để có thanh tiến trình\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "# %pip install underthesea\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560b9f9c",
   "metadata": {},
   "source": [
    "# --- 1. Tải model retriever ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beb69bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Tải các model cần thiết ---\n",
    "# Model để chấm điểm câu (giống model cũ của bạn)\n",
    "scorer_model = SentenceTransformer(\n",
    "    \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    ")\n",
    "\n",
    "# Tokenizer của model chính bạn sẽ huấn luyện (phải cùng loại)\n",
    "# Ví dụ: dùng tokenizer của model trong file config của bạn\n",
    "main_tokenizer = AutoTokenizer.from_pretrained(\"joeddav/xlm-roberta-large-xnli\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e164372",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 200  # Giới hạn của model chính\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce79ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_and_truncate(context: str, scorer, tokenizer, max_tokens: int) -> str:\n",
    "    \"\"\"\n",
    "    Tóm tắt trích rút context để vừa với giới hạn token mà vẫn giữ thứ tự.\n",
    "    \"\"\"\n",
    "    # Bước 1: Tách câu\n",
    "    sentences = sent_tokenize(context)\n",
    "\n",
    "    # Nếu context đã đủ ngắn thì trả về luôn\n",
    "    if len(tokenizer.encode(context)) <= max_tokens:\n",
    "        return context\n",
    "\n",
    "    # Bước 2: Vector hóa\n",
    "    # Vector trung tâm đại diện cho toàn bộ văn bản\n",
    "    context_embedding = scorer.encode(context, convert_to_tensor=True)\n",
    "    # Vector cho từng câu\n",
    "    sentence_embeddings = scorer.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "    # Bước 3: Chấm điểm\n",
    "    cosine_scores = util.cos_sim(context_embedding, sentence_embeddings)[0]\n",
    "\n",
    "    # Bước 4: Tạo danh sách (điểm số, chỉ số vị trí, câu)\n",
    "    scored_sentences = []\n",
    "    for i, score in enumerate(cosine_scores):\n",
    "        scored_sentences.append((score.item(), i, sentences[i]))\n",
    "\n",
    "    # Sắp xếp theo điểm số từ cao xuống thấp\n",
    "    scored_sentences.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # Bước 5: Xây dựng lại context\n",
    "    final_sentences = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    # Lấy các câu quan trọng nhất theo điểm số\n",
    "    for score, index, sentence in scored_sentences:\n",
    "        final_sentences.append((index, sentence))\n",
    "\n",
    "    # Sắp xếp lại các câu đã chọn theo thứ tự gốc\n",
    "    final_sentences.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Ghép câu lại cho đến khi đạt giới hạn token\n",
    "    summarized_context = \"\"\n",
    "    for index, sentence in final_sentences:\n",
    "        temp_context = (\n",
    "            summarized_context + \" \" + sentence if summarized_context else sentence\n",
    "        )\n",
    "        if len(tokenizer.encode(temp_context)) > max_tokens:\n",
    "            break\n",
    "        summarized_context = temp_context\n",
    "\n",
    "    return summarized_context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1487ce57",
   "metadata": {},
   "source": [
    "# --- 2. Đọc dữ liệu gốc ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8e3dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Áp dụng vào DataFrame ---\n",
    "df = pd.read_csv(\"data/vihallu-train.csv\")\n",
    "df[\"context\"] = df[\"context\"].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19804591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bắt đầu tóm tắt và rút gọn context...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1acef22719b94d4aa9f2449648c79838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Xử lý thành công.\n",
      "✅ Đã lưu dữ liệu đã xử lý vào: data/vihallu-train-summarized.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Bắt đầu tóm tắt và rút gọn context...\")\n",
    "# Áp dụng hàm mới\n",
    "df[\"summarized_context\"] = df[\"context\"].progress_apply(\n",
    "    lambda x: summarize_and_truncate(x, scorer_model, main_tokenizer, MAX_TOKENS)\n",
    ")\n",
    "print(\"✅ Xử lý thành công.\")\n",
    "\n",
    "# Lưu file mới\n",
    "output_path = \"data/vihallu-train-summarized.csv\"\n",
    "# Bạn có thể giữ lại cột context gốc và cột đã xử lý để so sánh\n",
    "df[[\"prompt\", \"response\", \"label\", \"context\", \"summarized_context\"]].to_csv(\n",
    "    output_path, index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "print(f\"✅ Đã lưu dữ liệu đã xử lý vào: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "210b2783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context gốc:\n",
      "Vào những năm 1870, hai nhà điêu khắc Augustus Saint-Gaudens và Daniel Chester French sinh sống và làm việc gần Quảng trường. Đến những năm 1920, Công viên Quảng trường Washington được công nhận cấp quốc gia là một trung tâm của phong trào nổi loạn về nghệ thuật và đạo đức. Do đó, khuôn viên NYU tại Quảng trường Washington trở nên đa dạng và hối hả nhờ có năng lượng của cuộc sống đô thị, điều này đã dẫn đến những sự thay đổi về mặt học thuật ở NYU. Những cư dân nổi tiếng thời kỳ này bao gồm Eugene O'Neill, John Sloan, và Maurice Prendergast. Vào những năm 1930, những người theo chủ nghĩa biểu hiện trừu tượng Jackson Pollock và Willem de Kooning, cùng với những người ủng hộ thuyết duy thực Edward Hopper và Thomas Hart Benton đều có xưởng vẽ xung quanh Quảng trường Washington. Những năm 1960 khu này trở thành một trong những trung tâm của thế hệ âm nhạc beat và folk (dân gian), khi Allen Ginsberg và Bob Dylan sinh sống tại đó. Điều này đã dẫn tới căng thẳng giữa họ và NYU, vốn hồi đó đang trải qua một chiến dịch mở rộng cơ sở vật chất lớn. Năm 1975, trường mở một Bộ sưu tập Nghệ thuật Xám tại số 100 đường Washington Square East, nhằm lưu trữ bộ sưu tập nghệ thuật của NYU và tổ chức các buổi triển lãm có chất lượng tương đương các bảo tàng.\n",
      "\n",
      "Context đã tóm tắt và rút gọn:\n",
      "Vào những năm 1870, hai nhà điêu khắc Augustus Saint-Gaudens và Daniel Chester French sinh sống và làm việc gần Quảng trường. Đến những năm 1920, Công viên Quảng trường Washington được công nhận cấp quốc gia là một trung tâm của phong trào nổi loạn về nghệ thuật và đạo đức. Do đó, khuôn viên NYU tại Quảng trường Washington trở nên đa dạng và hối hả nhờ có năng lượng của cuộc sống đô thị, điều này đã dẫn đến những sự thay đổi về mặt học thuật ở NYU. Những cư dân nổi tiếng thời kỳ này bao gồm Eugene O'Neill, John Sloan, và Maurice Prendergast. Vào những năm 1930, những người theo chủ nghĩa biểu hiện trừu tượng Jackson Pollock và Willem de Kooning, cùng với những người ủng hộ thuyết duy thực Edward Hopper và Thomas Hart Benton đều có xưởng vẽ xung quanh Quảng trường Washington.\n",
      "\n",
      "Điểm cosine similarity giữa context gốc và đã tóm tắt: 1.0000\n"
     ]
    }
   ],
   "source": [
    "context = df[\"context\"].iloc[0]\n",
    "summarized_context = df[\"summarized_context\"].iloc[0]\n",
    "\n",
    "print(\"Context gốc:\")\n",
    "print(context)\n",
    "print(\"\\nContext đã tóm tắt và rút gọn:\")\n",
    "print(summarized_context)\n",
    "\n",
    "cosine_score = util.cos_sim(\n",
    "    scorer_model.encode(context, convert_to_tensor=True),\n",
    "    scorer_model.encode(summarized_context, convert_to_tensor=True),\n",
    ")[0][0]\n",
    "print(f\"\\nĐiểm cosine similarity giữa context gốc và đã tóm tắt: {cosine_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afcabf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs221",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
