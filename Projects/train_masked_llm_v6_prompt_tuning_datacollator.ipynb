{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c761c320",
   "metadata": {},
   "source": [
    "# CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda34064",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install peft accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f24b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [1] - ÄÃƒ Cáº¬P NHáº¬T CHO PROMPT TUNING\n",
    "import os\n",
    "from peft import TaskType  # <<< THÃŠM IMPORT NÃ€Y\n",
    "\n",
    "MODEL_NAMES = [\n",
    "    \"joeddav/xlm-roberta-large-xnli\",\n",
    "    \"microsoft/infoxlm-large\",\n",
    "    \"uitnlp/CafeBERT\",\n",
    "    \"FacebookAI/xlm-roberta-large\",\n",
    "    \"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\",\n",
    "    \"MoritzLaurer/ernie-m-large-mnli-xnli\",\n",
    "    \"microsoft/deberta-xlarge-mnli\",\n",
    "]\n",
    "\n",
    "\n",
    "class Config:\n",
    "    ROOT_DIR = os.getcwd()\n",
    "    DATA_DIR = os.path.join(ROOT_DIR, \"data\")\n",
    "    TRAIN_FILE = os.path.join(DATA_DIR, \"vihallu-train.csv\")\n",
    "    TEST_FILE = os.path.join(DATA_DIR, \"vihallu-public-test.csv\")\n",
    "    SUBMISSION_DIR = os.path.join(ROOT_DIR, \"submission\")\n",
    "    SUBMISSION_CSV = \"submit.csv\"\n",
    "    SUBMISSION_ZIP = \"submit.zip\"\n",
    "\n",
    "    MODEL_NAME = MODEL_NAMES[3]\n",
    "    # --- THAY Äá»”I TÃŠN THÆ¯ Má»¤C OUTPUT CHO PEFT ---\n",
    "    MODEL_OUTPUT_DIR = os.path.join(\n",
    "        ROOT_DIR,\n",
    "        \"models\",\n",
    "        f\"{MODEL_NAME.split('/')[-1]}-prompt-tuned\",  # Sá»­a tÃªn thÆ° má»¥c\n",
    "    )\n",
    "\n",
    "    MAX_LENGTH = 512\n",
    "    RANDOM_STATE = 42\n",
    "    EPOCHS = 10  # CÃ³ thá»ƒ cáº§n nhiá»u epoch hÆ¡n vÃ¬ chá»‰ train prompt\n",
    "    BATCH_SIZE = 4\n",
    "    GRADIENT_ACCUMULATION_STEPS = 4\n",
    "\n",
    "    # --- Sá»¬ Dá»¤NG Cáº¤U HÃŒNH á»”N Äá»ŠNH CHO PROMPT ---\n",
    "    SCHEDULER_TYPE = \"cosine\"  # Cosine Ä‘Æ¡n giáº£n thÆ°á»ng tá»‘t cho PEFT\n",
    "    LEARNING_RATE = 1e-3  # <<-- LR cao hÆ¡n nhiá»u cho prompt embeddings\n",
    "    WEIGHT_DECAY = 0.01  # Giá»¯ nguyÃªn hoáº·c giáº£m nháº¹\n",
    "    NUM_CYCLES = 3\n",
    "\n",
    "    # --- CÃC THAM Sá» PEFT (PROMPT TUNING) ---\n",
    "    PEFT_TYPE = \"PROMPT_TUNING\"\n",
    "    PEFT_TASK_TYPE = TaskType.SEQ_CLS  # <<< CHO SEQUENCE CLASSIFICATION\n",
    "    NUM_VIRTUAL_TOKENS = 20  # Sá»‘ lÆ°á»£ng token áº£o Ä‘á»ƒ há»c (thÆ°á»ng tá»« 10-100)\n",
    "\n",
    "    # --- CÃC THAM Sá» ÃT QUAN TRá»ŒNG HÆ N KHI DÃ™NG PEFT ---\n",
    "    # CLASSIFIER_DROPOUT = 0.1 # Ãt áº£nh hÆ°á»Ÿng vÃ¬ classifier gá»‘c bá»‹ Ä‘Ã³ng bÄƒng\n",
    "    LABEL_SMOOTHING = 0.05\n",
    "    EPSILON = 1e-8\n",
    "    PATIENCE_LIMIT = 4  # Giá»¯ patience an toÃ n\n",
    "    TOTAL_STEP_SCALE = 0.1\n",
    "    VALIDATION_SPLIT_SIZE = 0.2\n",
    "\n",
    "    LABEL_MAP = {\"no\": 0, \"extrinsic\": 1, \"intrinsic\": 2}\n",
    "    ID2LABEL = {v: k for k, v in LABEL_MAP.items()}\n",
    "    CLASS_WEIGHTS = [1.0393466963622866, 1.0114145354717525, 0.9531590413943355]\n",
    "\n",
    "\n",
    "cfg = Config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01017e0",
   "metadata": {},
   "source": [
    "# LOGGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1e41e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# ThÆ° má»¥c gá»‘c Ä‘á»ƒ lÆ°u táº¥t cáº£ cÃ¡c file log\n",
    "LOG_BASE_DIR = \"logs\"\n",
    "\n",
    "# DÃ¹ng má»™t dictionary Ä‘á»ƒ lÆ°u cÃ¡c logger Ä‘Ã£ táº¡o, trÃ¡nh viá»‡c táº¡o láº¡i vÃ  gÃ¢y ra log trÃ¹ng láº·p\n",
    "_loggers = {}\n",
    "\n",
    "\n",
    "def setup_logger(model_name: str, log_level=logging.INFO):\n",
    "    \"\"\"\n",
    "    Thiáº¿t láº­p vÃ  tráº£ vá» má»™t logger Ä‘á»ƒ ghi log vÃ o cáº£ console vÃ  file.\n",
    "\n",
    "    - Má»—i model sáº½ cÃ³ má»™t thÆ° má»¥c log riÃªng dá»±a trÃªn `model_name`.\n",
    "    - Má»—i láº§n cháº¡y sáº½ táº¡o má»™t file log má»›i cÃ³ tÃªn lÃ  timestamp (vÃ­ dá»¥: 2023-10-27_15-30-00.log).\n",
    "    - Äáº£m báº£o khÃ´ng cÃ³ log nÃ o bá»‹ ghi Ä‘Ã¨.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): TÃªn cá»§a model, dÃ¹ng Ä‘á»ƒ táº¡o thÆ° má»¥c con. VÃ­ dá»¥: 'xnli-large-tuned'.\n",
    "        log_level (int): Cáº¥p Ä‘á»™ log, máº·c Ä‘á»‹nh lÃ  logging.INFO.\n",
    "\n",
    "    Returns:\n",
    "        logging.Logger: Instance cá»§a logger Ä‘Ã£ Ä‘Æ°á»£c cáº¥u hÃ¬nh.\n",
    "    \"\"\"\n",
    "    # Náº¿u logger cho model nÃ y Ä‘Ã£ tá»“n táº¡i, tráº£ vá» nÃ³ ngay láº­p tá»©c\n",
    "    if model_name in _loggers:\n",
    "        return _loggers[model_name]\n",
    "\n",
    "    # Xá»­ lÃ½ tÃªn model Ä‘á»ƒ an toÃ n khi táº¡o tÃªn thÆ° má»¥c (thay tháº¿ \"/\")\n",
    "    safe_model_name = model_name.replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
    "    model_log_dir = os.path.join(LOG_BASE_DIR, safe_model_name)\n",
    "    os.makedirs(model_log_dir, exist_ok=True)\n",
    "\n",
    "    # Táº¡o logger\n",
    "    logger = logging.getLogger(safe_model_name)\n",
    "    logger.setLevel(log_level)\n",
    "\n",
    "    # NgÄƒn khÃ´ng cho log lan truyá»n Ä‘áº¿n root logger Ä‘á»ƒ trÃ¡nh in ra console 2 láº§n\n",
    "    logger.propagate = False\n",
    "\n",
    "    # Äá»‹nh dáº¡ng cho log message\n",
    "    formatter = logging.Formatter(\n",
    "        \"%(asctime)s - [%(levelname)s] - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "    )\n",
    "\n",
    "    # Táº¡o File Handler Ä‘á»ƒ ghi log ra file\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    log_file_path = os.path.join(model_log_dir, f\"{timestamp}.log\")\n",
    "\n",
    "    file_handler = logging.FileHandler(log_file_path, encoding=\"utf-8\")\n",
    "    file_handler.setLevel(log_level)\n",
    "    file_handler.setFormatter(formatter)\n",
    "\n",
    "    # Táº¡o Console (Stream) Handler Ä‘á»ƒ in log ra mÃ n hÃ¬nh\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(log_level)\n",
    "    console_handler.setFormatter(formatter)\n",
    "\n",
    "    # ThÃªm cÃ¡c handler vÃ o logger\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "    # LÆ°u logger vÃ o cache\n",
    "    _loggers[model_name] = logger\n",
    "\n",
    "    logger.info(\n",
    "        f\"Logger cho '{safe_model_name}' Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o. File log: {log_file_path}\"\n",
    "    )\n",
    "\n",
    "    return logger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148d0474",
   "metadata": {},
   "source": [
    "## Setup logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eac9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 13:42:24 - [INFO] - Logger cho 'FacebookAI_xlm-roberta-large-training' Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o. File log: logs/FacebookAI_xlm-roberta-large-training/2025-10-17_13-42-24.log\n",
      "2025-10-17 13:42:24 - [INFO] - Logger initialized for FacebookAI/xlm-roberta-large\n",
      "2025-10-17 13:42:24 - [INFO] - ============================================================\n",
      "2025-10-17 13:42:24 - [INFO] - ğŸš€ STARTING TRAINING SESSION\n",
      "2025-10-17 13:42:24 - [INFO] - ============================================================\n",
      "2025-10-17 13:42:24 - [INFO] - ROOT_DIR: /home/guest/Projects/CS221\n",
      "2025-10-17 13:42:24 - [INFO] - DATA_DIR: /home/guest/Projects/CS221/data\n",
      "2025-10-17 13:42:24 - [INFO] - TRAIN_FILE: /home/guest/Projects/CS221/data/vihallu-train.csv\n",
      "2025-10-17 13:42:24 - [INFO] - TEST_FILE: /home/guest/Projects/CS221/data/vihallu-public-test.csv\n",
      "2025-10-17 13:42:24 - [INFO] - SUBMISSION_DIR: /home/guest/Projects/CS221/submission\n",
      "2025-10-17 13:42:24 - [INFO] - SUBMISSION_CSV: submit.csv\n",
      "2025-10-17 13:42:24 - [INFO] - SUBMISSION_ZIP: submit.zip\n",
      "2025-10-17 13:42:24 - [INFO] - MODEL_NAME: FacebookAI/xlm-roberta-large\n",
      "2025-10-17 13:42:24 - [INFO] - MODEL_OUTPUT_DIR: /home/guest/Projects/CS221/models/xlm-roberta-large-tuned\n",
      "2025-10-17 13:42:24 - [INFO] - MAX_LENGTH: 512\n",
      "2025-10-17 13:42:24 - [INFO] - RANDOM_STATE: 42\n",
      "2025-10-17 13:42:24 - [INFO] - EPOCHS: 10\n",
      "2025-10-17 13:42:24 - [INFO] - BATCH_SIZE: 4\n",
      "2025-10-17 13:42:24 - [INFO] - GRADIENT_ACCUMULATION_STEPS: 4\n",
      "2025-10-17 13:42:24 - [INFO] - SCHEDULER_TYPE: cosine\n",
      "2025-10-17 13:42:24 - [INFO] - LEARNING_RATE: 8e-06\n",
      "2025-10-17 13:42:24 - [INFO] - WEIGHT_DECAY: 0.03\n",
      "2025-10-17 13:42:24 - [INFO] - NUM_CYCLES: 3\n",
      "2025-10-17 13:42:24 - [INFO] - CLASSIFIER_DROPOUT: 0.1\n",
      "2025-10-17 13:42:24 - [INFO] - LABEL_SMOOTHING: 0.05\n",
      "2025-10-17 13:42:24 - [INFO] - TOTAL_STEP_SCALE: 0.1\n",
      "2025-10-17 13:42:24 - [INFO] - EPSILON: 1e-08\n",
      "2025-10-17 13:42:24 - [INFO] - PATIENCE_LIMIT: 3\n",
      "2025-10-17 13:42:24 - [INFO] - VALIDATION_SPLIT_SIZE: 0.2\n",
      "2025-10-17 13:42:24 - [INFO] - LABEL_MAP: {'no': 0, 'extrinsic': 1, 'intrinsic': 2}\n",
      "2025-10-17 13:42:24 - [INFO] - ID2LABEL: {0: 'no', 1: 'extrinsic', 2: 'intrinsic'}\n",
      "2025-10-17 13:42:24 - [INFO] - CLASS_WEIGHTS: [1.0393466963622866, 1.0114145354717525, 0.9531590413943355]\n",
      "2025-10-17 13:42:24 - [INFO] - ============================================================\n",
      "2025-10-17 13:42:24 - [INFO] - Logger initialized for FacebookAI/xlm-roberta-large\n",
      "2025-10-17 13:42:24 - [INFO] - ============================================================\n",
      "2025-10-17 13:42:24 - [INFO] - ğŸš€ STARTING TRAINING SESSION\n",
      "2025-10-17 13:42:24 - [INFO] - ============================================================\n",
      "2025-10-17 13:42:24 - [INFO] - ROOT_DIR: /home/guest/Projects/CS221\n",
      "2025-10-17 13:42:24 - [INFO] - DATA_DIR: /home/guest/Projects/CS221/data\n",
      "2025-10-17 13:42:24 - [INFO] - TRAIN_FILE: /home/guest/Projects/CS221/data/vihallu-train.csv\n",
      "2025-10-17 13:42:24 - [INFO] - TEST_FILE: /home/guest/Projects/CS221/data/vihallu-public-test.csv\n",
      "2025-10-17 13:42:24 - [INFO] - SUBMISSION_DIR: /home/guest/Projects/CS221/submission\n",
      "2025-10-17 13:42:24 - [INFO] - SUBMISSION_CSV: submit.csv\n",
      "2025-10-17 13:42:24 - [INFO] - SUBMISSION_ZIP: submit.zip\n",
      "2025-10-17 13:42:24 - [INFO] - MODEL_NAME: FacebookAI/xlm-roberta-large\n",
      "2025-10-17 13:42:24 - [INFO] - MODEL_OUTPUT_DIR: /home/guest/Projects/CS221/models/xlm-roberta-large-tuned\n",
      "2025-10-17 13:42:24 - [INFO] - MAX_LENGTH: 512\n",
      "2025-10-17 13:42:24 - [INFO] - RANDOM_STATE: 42\n",
      "2025-10-17 13:42:24 - [INFO] - EPOCHS: 10\n",
      "2025-10-17 13:42:24 - [INFO] - BATCH_SIZE: 4\n",
      "2025-10-17 13:42:24 - [INFO] - GRADIENT_ACCUMULATION_STEPS: 4\n",
      "2025-10-17 13:42:24 - [INFO] - SCHEDULER_TYPE: cosine\n",
      "2025-10-17 13:42:24 - [INFO] - LEARNING_RATE: 8e-06\n",
      "2025-10-17 13:42:24 - [INFO] - WEIGHT_DECAY: 0.03\n",
      "2025-10-17 13:42:24 - [INFO] - NUM_CYCLES: 3\n",
      "2025-10-17 13:42:24 - [INFO] - CLASSIFIER_DROPOUT: 0.1\n",
      "2025-10-17 13:42:24 - [INFO] - LABEL_SMOOTHING: 0.05\n",
      "2025-10-17 13:42:24 - [INFO] - TOTAL_STEP_SCALE: 0.1\n",
      "2025-10-17 13:42:24 - [INFO] - EPSILON: 1e-08\n",
      "2025-10-17 13:42:24 - [INFO] - PATIENCE_LIMIT: 3\n",
      "2025-10-17 13:42:24 - [INFO] - VALIDATION_SPLIT_SIZE: 0.2\n",
      "2025-10-17 13:42:24 - [INFO] - LABEL_MAP: {'no': 0, 'extrinsic': 1, 'intrinsic': 2}\n",
      "2025-10-17 13:42:24 - [INFO] - ID2LABEL: {0: 'no', 1: 'extrinsic', 2: 'intrinsic'}\n",
      "2025-10-17 13:42:24 - [INFO] - CLASS_WEIGHTS: [1.0393466963622866, 1.0114145354717525, 0.9531590413943355]\n",
      "2025-10-17 13:42:24 - [INFO] - ============================================================\n"
     ]
    }
   ],
   "source": [
    "logger = setup_logger(f\"{cfg.MODEL_NAME}-training\")\n",
    "logger.info(f\"Logger initialized for {cfg.MODEL_NAME}\")\n",
    "\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(\"ğŸš€ STARTING TRAINING SESSION\")\n",
    "logger.info(\"=\" * 60)\n",
    "for key, value in Config.__dict__.items():\n",
    "    if not key.startswith(\"__\") and not callable(value):\n",
    "        logger.info(f\"{key}: {value}\")\n",
    "logger.info(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c3b202",
   "metadata": {},
   "source": [
    "# Hallucination Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce606de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [6] - ÄÃƒ Cáº¬P NHáº¬T\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class HallucinationDataset(Dataset):\n",
    "    def __init__(self, premises, hypotheses, labels, tokenizer, max_len):\n",
    "        self.premises = premises\n",
    "        self.hypotheses = hypotheses\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        premise = self.premises[idx]\n",
    "        hypothesis = self.hypotheses[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize báº±ng cÃ¡ch truyá»n 2 chuá»—i riÃªng biá»‡t\n",
    "        # Tokenizer sáº½ tá»± Ä‘á»™ng cáº¯t bá»›t `premise` náº¿u cáº§n\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            premise,\n",
    "            hypothesis,  # <-- text_pair\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"],\n",
    "            \"attention_mask\": encoding[\"attention_mask\"],\n",
    "            \"labels\": label,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbc1878",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcfa808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def prepare_data(config, logger=None):\n",
    "    df = pd.read_csv(config.TRAIN_FILE)\n",
    "    print(f\"âœ… Äá»c thÃ nh cÃ´ng {len(df)} máº«u tá»« file Ä‘Ã£ xá»­ lÃ½: {config.TRAIN_FILE}\")\n",
    "\n",
    "    # Táº¡o 2 cá»™t premise vÃ  hypothesis tá»« ngá»¯ cáº£nh (context)\n",
    "    df[\"premise\"] = (\n",
    "        \"CÃ¢u há»i: \"\n",
    "        + df[\"prompt\"].astype(str)\n",
    "        + \" Ngá»¯ cáº£nh: \"\n",
    "        + df[\"context\"].astype(str)\n",
    "    )\n",
    "    df[\"hypothesis\"] = df[\"response\"].astype(str)\n",
    "\n",
    "    df[\"label_id\"] = df[\"label\"].map(config.LABEL_MAP)\n",
    "    df.dropna(subset=[\"label_id\"], inplace=True)\n",
    "    df[\"label_id\"] = df[\"label_id\"].astype(int)\n",
    "\n",
    "    train_df, val_df = train_test_split(\n",
    "        df,\n",
    "        test_size=config.VALIDATION_SPLIT_SIZE,\n",
    "        random_state=config.RANDOM_STATE,\n",
    "        stratify=df[\"label_id\"],\n",
    "    )\n",
    "\n",
    "    if logger:\n",
    "        logger.info(\n",
    "            f\"Chia dá»¯ liá»‡u: {len(train_df)} máº«u train, {len(val_df)} máº«u validation.\"\n",
    "        )\n",
    "\n",
    "    # --- PHáº¦N NÃ‚NG Cáº¤P: LÆ¯U FILE RA THÆ¯ Má»¤C DATA ---\n",
    "    # Táº¡o thÆ° má»¥c 'processed' trong 'data' náº¿u chÆ°a cÃ³\n",
    "    processed_data_dir = os.path.join(config.DATA_DIR, \"processed\")\n",
    "    os.makedirs(processed_data_dir, exist_ok=True)\n",
    "\n",
    "    # Äá»‹nh nghÄ©a Ä‘Æ°á»ng dáº«n file\n",
    "    train_output_path = os.path.join(processed_data_dir, \"train_split.csv\")\n",
    "    val_output_path = os.path.join(processed_data_dir, \"validation_split.csv\")\n",
    "\n",
    "    # LÆ°u cÃ¡c DataFrame\n",
    "    train_df.to_csv(train_output_path, index=False, encoding=\"utf-8-sig\")\n",
    "    val_df.to_csv(val_output_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"âœ… ÄÃ£ lÆ°u táº­p train vÃ o: {train_output_path}\")\n",
    "    print(f\"âœ… ÄÃ£ lÆ°u táº­p validation vÃ o: {val_output_path}\")\n",
    "    # --- Káº¾T THÃšC PHáº¦N NÃ‚NG Cáº¤P ---\n",
    "\n",
    "    return train_df, val_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f432b434",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39c273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [8] - ÄÃƒ Cáº¬P NHáº¬T HOÃ€N TOÃ€N CHO PEFT\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn as nn\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    PromptTuningConfig,\n",
    "    TaskType,\n",
    "    PeftModel,\n",
    ")  # <<< THÃŠM IMPORT PEFT\n",
    "\n",
    "\n",
    "def get_model_and_tokenizer(config, logger):\n",
    "    \"\"\"Táº£i pre-trained model, tokenizer vÃ  Ã¡p dá»¥ng PEFT.\"\"\"\n",
    "    logger.info(f\"Äang táº£i model gá»‘c: {config.MODEL_NAME}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\n",
    "\n",
    "    # Táº£i model gá»‘c cho sequence classification\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        config.MODEL_NAME,\n",
    "        num_labels=len(config.LABEL_MAP),\n",
    "        # ThÃªm cÃ¡c map label náº¿u cáº§n (thÆ°á»ng khÃ´ng cáº§n cho PEFT vÃ¬ head gá»‘c bá»‹ Ä‘Ã³ng bÄƒng)\n",
    "        # label2id=config.LABEL_MAP,\n",
    "        # id2label=config.ID2LABEL,\n",
    "    )\n",
    "\n",
    "    # --- Táº O VÃ€ ÃP Dá»¤NG PEFT CONFIG ---\n",
    "    if config.PEFT_TYPE == \"PROMPT_TUNING\":\n",
    "        logger.info(\n",
    "            f\"Ãp dá»¥ng {config.PEFT_TYPE} vá»›i {config.NUM_VIRTUAL_TOKENS} virtual tokens.\"\n",
    "        )\n",
    "        peft_config = PromptTuningConfig(\n",
    "            task_type=config.PEFT_TASK_TYPE,\n",
    "            num_virtual_tokens=config.NUM_VIRTUAL_TOKENS,\n",
    "            # CÃ¡c tham sá»‘ khÃ¡c cÃ³ thá»ƒ thÃªm:\n",
    "            # tokenizer_name_or_path=config.MODEL_NAME,\n",
    "            # prompt_tuning_init=\"TEXT\", # Khá»Ÿi táº¡o prompt báº±ng text\n",
    "            # prompt_tuning_init_text=\"Classify the hallucination type:\", # Text dÃ¹ng Ä‘á»ƒ khá»Ÿi táº¡o\n",
    "        )\n",
    "    # ThÃªm cÃ¡c loáº¡i PEFT khÃ¡c náº¿u muá»‘n (LoRA, etc.)\n",
    "    # elif config.PEFT_TYPE == \"LORA\":\n",
    "    #    ...\n",
    "    else:\n",
    "        logger.warning(\n",
    "            f\"PEFT_TYPE '{config.PEFT_TYPE}' khÃ´ng Ä‘Æ°á»£c há»— trá»£ hoáº·c khÃ´ng Ä‘Æ°á»£c chá»n. Sá»­ dá»¥ng model gá»‘c.\"\n",
    "        )\n",
    "        return model, tokenizer  # Tráº£ vá» model gá»‘c náº¿u khÃ´ng dÃ¹ng PEFT\n",
    "\n",
    "    # Láº¥y mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c PEFT bao bá»c\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    logger.info(\"ÄÃ£ Ã¡p dá»¥ng PEFT adapter thÃ nh cÃ´ng.\")\n",
    "    model.print_trainable_parameters()  # In ra sá»‘ lÆ°á»£ng tham sá»‘ cáº§n huáº¥n luyá»‡n\n",
    "    # ------------------------------------\n",
    "\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21114019",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b10adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim import AdamW\n",
    "from huggingface_hub import login\n",
    "from transformers import get_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.contrib.logging import logging_redirect_tqdm\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e6336",
   "metadata": {},
   "source": [
    "## train one epoch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1154314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model,\n",
    "    data_loader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    device,\n",
    "    epoch=None,\n",
    "    total_epochs=None,\n",
    "    gradient_accumulation_steps=1,\n",
    "):\n",
    "    \"\"\"Huáº¥n luyá»‡n mÃ´ hÃ¬nh trong má»™t epoch báº±ng gradient accumulation.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    desc = f\"Train\" if epoch is None else f\"Epoch {epoch}/{total_epochs}\"\n",
    "    progress_bar = tqdm(\n",
    "        data_loader, desc=desc, leave=False, dynamic_ncols=True, mininterval=0.5\n",
    "    )\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    steps_in_epoch = len(data_loader)\n",
    "    with logging_redirect_tqdm():  # make logger calls safe\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits  # shape (batch_size, num_labels)\n",
    "\n",
    "            loss = loss_fn(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            scaled_loss = loss / gradient_accumulation_steps\n",
    "            scaled_loss.backward()\n",
    "\n",
    "            if (step + 1) % gradient_accumulation_steps == 0 or (\n",
    "                step + 1\n",
    "            ) == steps_in_epoch:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    return total_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3398cf",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194e5a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, loss_fn, device):\n",
    "    \"\"\"ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn táº­p dá»¯ liá»‡u.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_val_loss = 0\n",
    "\n",
    "    progress_bar = tqdm(data_loader, desc=\"Evaluating\", leave=False, dynamic_ncols=True)\n",
    "\n",
    "    with torch.no_grad(), logging_redirect_tqdm():\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # <<< TÃNH LOSS TRÃŠN Táº¬P VALIDATION\n",
    "            loss = loss_fn(logits, labels)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(data_loader)  # <<< TÃNH LOSS TRUNG BÃŒNH\n",
    "    return all_labels, all_preds, avg_val_loss  # <<< TRáº¢ Vá»€ THÃŠM LOSS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08cfc13",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a07ea24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dotenv_path: /home/guest/Projects/CS221/envs/.env\n"
     ]
    }
   ],
   "source": [
    "# Táº£i biáº¿n mÃ´i trÆ°á»ng tá»« file envs/.env.\n",
    "dotenv_path = os.path.join(os.getcwd(), \"envs\", \".env\")\n",
    "load_dotenv(dotenv_path)\n",
    "print(f\"dotenv_path: {dotenv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48a9b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: TÃ¬m tháº¥y HUGGING_FACE_TOKEN. Äang Ä‘Äƒng nháº­p...\n",
      "INFO: ÄÄƒng nháº­p Hugging Face thÃ nh cÃ´ng.\n"
     ]
    }
   ],
   "source": [
    "# láº¥y HF token Ä‘á»ƒ login\n",
    "hf_token = os.getenv(\"HUGGING_FACE_TOKEN\")\n",
    "\n",
    "if hf_token:\n",
    "    print(\"INFO: TÃ¬m tháº¥y HUGGING_FACE_TOKEN. Äang Ä‘Äƒng nháº­p...\")\n",
    "    login(token=hf_token)\n",
    "    print(\"INFO: ÄÄƒng nháº­p Hugging Face thÃ nh cÃ´ng.\")\n",
    "else:\n",
    "    print(\n",
    "        \"WARNING: KhÃ´ng tÃ¬m tháº¥y HUGGING_FACE_TOKEN trong file .env. Má»™t sá»‘ model cÃ³ thá»ƒ yÃªu cáº§u Ä‘Äƒng nháº­p.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950b94e0",
   "metadata": {},
   "source": [
    "## 1. Chuáº©n bá»‹ dá»¯ liá»‡u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c739cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 13:42:27 - [INFO] - Báº¯t Ä‘áº§u pipeline huáº¥n luyá»‡n.\n",
      "2025-10-17 13:42:27 - [INFO] - BÆ°á»›c 1: Chuáº©n bá»‹ dá»¯ liá»‡u...\n",
      "2025-10-17 13:42:27 - [INFO] - Chia dá»¯ liá»‡u: 5600 máº«u train, 1400 máº«u validation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Äá»c thÃ nh cÃ´ng 7000 máº«u tá»« file Ä‘Ã£ xá»­ lÃ½: /home/guest/Projects/CS221/data/vihallu-train.csv\n",
      "âœ… ÄÃ£ lÆ°u táº­p train vÃ o: /home/guest/Projects/CS221/data/processed/train_split.csv\n",
      "âœ… ÄÃ£ lÆ°u táº­p validation vÃ o: /home/guest/Projects/CS221/data/processed/validation_split.csv\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Báº¯t Ä‘áº§u pipeline huáº¥n luyá»‡n.\")\n",
    "\n",
    "# 1. Chuáº©n bá»‹ dá»¯ liá»‡u\n",
    "logger.info(\"BÆ°á»›c 1: Chuáº©n bá»‹ dá»¯ liá»‡u...\")\n",
    "train_df, val_df = prepare_data(cfg, logger=logger)\n",
    "if train_df is None:\n",
    "    logger.error(\"Dá»¯ liá»‡u khÃ´ng thá»ƒ Ä‘Æ°á»£c chuáº©n bá»‹. Dá»«ng chÆ°Æ¡ng trÃ¬nh.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b8a1b7",
   "metadata": {},
   "source": [
    "## 2. Táº£i model vÃ  tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f4f00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 13:42:27 - [INFO] - BÆ°á»›c 2: Táº£i model 'FacebookAI/xlm-roberta-large' vÃ  tokenizer...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Äang táº£i model: FacebookAI/xlm-roberta-large\n",
      "Model config: XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# --- Sá»¬A Láº I Lá»œI Gá»ŒI HÃ€M TRONG NOTEBOOK ---\n",
    "# Cell [13] cáº§n sá»­a láº¡i Ä‘á»ƒ truyá»n logger vÃ o\n",
    "logger.info(f\"BÆ°á»›c 2: Táº£i model '{cfg.MODEL_NAME}' vÃ  tokenizer...\")\n",
    "model, tokenizer = get_model_and_tokenizer(cfg, logger)  # Truyá»n logger vÃ o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0416e4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torchinfo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9d1648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 13:42:29 - [INFO] - PhÃ¢n tÃ­ch kiáº¿n trÃºc mÃ´ hÃ¬nh báº±ng torchinfo...\n",
      "2025-10-17 13:42:32 - [INFO] - Kiáº¿n trÃºc chi tiáº¿t cá»§a mÃ´ hÃ¬nh:\n",
      "=====================================================================================================================================================================\n",
      "Layer (type:depth-idx)                                            Input Shape               Output Shape              Param #                   Mult-Adds\n",
      "=====================================================================================================================================================================\n",
      "XLMRobertaForSequenceClassification                               --                        [4, 3]                    --                        --\n",
      "â”œâ”€XLMRobertaModel: 1-1                                            [4, 512]                  [4, 512, 1024]            --                        --\n",
      "â”‚    â””â”€XLMRobertaEmbeddings: 2-1                                  --                        [4, 512, 1024]            --                        --\n",
      "â”‚    â”‚    â””â”€Embedding: 3-1                                        [4, 512]                  [4, 512, 1024]            256,002,048               1,024,008,192\n",
      "â”‚    â”‚    â””â”€Embedding: 3-2                                        [4, 512]                  [4, 512, 1024]            1,024                     4,096\n",
      "â”‚    â”‚    â””â”€Embedding: 3-3                                        [4, 512]                  [4, 512, 1024]            526,336                   2,105,344\n",
      "â”‚    â”‚    â””â”€LayerNorm: 3-4                                        [4, 512, 1024]            [4, 512, 1024]            2,048                     8,192\n",
      "â”‚    â”‚    â””â”€Dropout: 3-5                                          [4, 512, 1024]            [4, 512, 1024]            --                        --\n",
      "â”‚    â””â”€XLMRobertaEncoder: 2-2                                     [4, 512, 1024]            [4, 512, 1024]            --                        --\n",
      "â”‚    â”‚    â””â”€ModuleList: 3-6                                       --                        --                        302,309,376               --\n",
      "â”œâ”€XLMRobertaClassificationHead: 1-2                               [4, 512, 1024]            [4, 3]                    --                        --\n",
      "â”‚    â””â”€Dropout: 2-3                                               [4, 1024]                 [4, 1024]                 --                        --\n",
      "â”‚    â””â”€Linear: 2-4                                                [4, 1024]                 [4, 1024]                 1,049,600                 4,198,400\n",
      "â”‚    â””â”€Dropout: 2-5                                               [4, 1024]                 [4, 1024]                 --                        --\n",
      "â”‚    â””â”€Linear: 2-6                                                [4, 1024]                 [4, 3]                    3,075                     12,300\n",
      "=====================================================================================================================================================================\n",
      "Total params: 559,893,507\n",
      "Trainable params: 559,893,507\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 2.24\n",
      "=====================================================================================================================================================================\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 4496.33\n",
      "Params size (MB): 2239.57\n",
      "Estimated Total Size (MB): 6735.92\n",
      "=====================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "import torch  # Äáº£m báº£o Ä‘Ã£ import torch\n",
    "\n",
    "logger.info(\"PhÃ¢n tÃ­ch kiáº¿n trÃºc mÃ´ hÃ¬nh báº±ng torchinfo...\")\n",
    "\n",
    "# --- DÃ¹ng torchinfo Ä‘á»ƒ hiá»ƒn thá»‹ ---\n",
    "# Táº¡o má»™t input giáº£ vá»›i batch_size vÃ  max_length nhÆ° trong config\n",
    "input_ids_example = torch.randint(\n",
    "    0, tokenizer.vocab_size, (cfg.BATCH_SIZE, cfg.MAX_LENGTH)\n",
    ")\n",
    "\n",
    "# 1. Gá»i summary vá»›i verbose=0 Ä‘á»ƒ khÃ´ng in ra console vÃ  lÆ°u káº¿t quáº£ vÃ o biáº¿n\n",
    "#    ThÃªm cÃ¡c cá»™t báº¡n muá»‘n xem, vÃ­ dá»¥: 'output_size', 'num_params'\n",
    "model_summary = summary(\n",
    "    model,\n",
    "    input_data={\"input_ids\": input_ids_example},\n",
    "    verbose=0,  # <-- Quan trá»ng: NgÄƒn khÃ´ng cho tá»± Ä‘á»™ng in\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"mult_adds\"],\n",
    ")\n",
    "\n",
    "# 2. Chuyá»ƒn Ä‘á»‘i tÆ°á»£ng summary thÃ nh string vÃ  Ä‘Æ°a vÃ o logger\n",
    "logger.info(f\"Kiáº¿n trÃºc chi tiáº¿t cá»§a mÃ´ hÃ¬nh:\\n{str(model_summary)}\")\n",
    "\n",
    "\n",
    "# # (TÃ¹y chá»n) Báº¡n váº«n cÃ³ thá»ƒ in ra mÃ n hÃ¬nh náº¿u muá»‘n xem ngay trong notebook\n",
    "# print(\"In summary ra mÃ n hÃ¬nh notebook:\")\n",
    "# print(model_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ffd869",
   "metadata": {},
   "source": [
    "## 3. Táº¡o Dataset vÃ  DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9803da5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 13:42:32 - [INFO] - BÆ°á»›c 3: Táº¡o Dataset vÃ  DataLoader...\n",
      "2025-10-17 13:42:32 - [INFO] - âœ… Táº¡o DataLoader thÃ nh cÃ´ng vá»›i DataCollatorWithPadding chuáº©n!\n",
      "2025-10-17 13:42:32 - [INFO] - âœ… Táº¡o DataLoader thÃ nh cÃ´ng vá»›i DataCollatorWithPadding chuáº©n!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding  # <-- 1. Import DataCollator\n",
    "\n",
    "# --- Táº O DATASET VÃ€ DATALOADER ---\n",
    "logger.info(\"BÆ°á»›c 3: Táº¡o Dataset vÃ  DataLoader...\")\n",
    "\n",
    "# Táº¡o Dataset (vá»›i class HallucinationDataset Ä‘Ã£ Ä‘Æ°á»£c chá»‰nh sá»­a á»Ÿ trÃªn)\n",
    "train_dataset = HallucinationDataset(\n",
    "    premises=train_df[\"premise\"].to_list(),\n",
    "    hypotheses=train_df[\"hypothesis\"].to_list(),\n",
    "    labels=train_df[\"label_id\"].to_list(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=cfg.MAX_LENGTH,\n",
    ")\n",
    "val_dataset = HallucinationDataset(\n",
    "    premises=val_df[\"premise\"].to_list(),\n",
    "    hypotheses=val_df[\"hypothesis\"].to_list(),\n",
    "    labels=val_df[\"label_id\"].to_list(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=cfg.MAX_LENGTH,\n",
    ")\n",
    "\n",
    "# 3. Táº¡o má»™t instance cá»§a DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 4. Táº¡o DataLoader vÃ  truyá»n data_collator vÃ o\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=cfg.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,  # <-- DÃ¹ng data_collator á»Ÿ Ä‘Ã¢y\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=cfg.BATCH_SIZE,\n",
    "    collate_fn=data_collator,  # <-- DÃ¹ng data_collator á»Ÿ Ä‘Ã¢y\n",
    ")\n",
    "\n",
    "logger.info(\"âœ… Táº¡o DataLoader thÃ nh cÃ´ng vá»›i DataCollatorWithPadding chuáº©n!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ac99c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 13:42:32 - [INFO] - Gradient accumulation steps: 4 | Effective batch size: 16\n"
     ]
    }
   ],
   "source": [
    "gradient_accumulation_steps = max(1, cfg.GRADIENT_ACCUMULATION_STEPS)\n",
    "effective_batch_size = cfg.BATCH_SIZE * gradient_accumulation_steps\n",
    "logger.info(\n",
    "    \"Gradient accumulation steps: %s | Effective batch size: %s\",\n",
    "    gradient_accumulation_steps,\n",
    "    effective_batch_size,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae8a4a1",
   "metadata": {},
   "source": [
    "### Check dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8564a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Kiá»ƒm tra 1 batch dá»¯ liá»‡u Ä‘áº§u vÃ o ---\n",
      "KÃ­ch thÆ°á»›c input_ids: torch.Size([4, 367])\n",
      "KÃ­ch thÆ°á»›c attention_mask: torch.Size([4, 367])\n",
      "NhÃ£n trong batch: tensor([2, 2, 0, 0])\n",
      "\n",
      "Má»™t máº«u Ä‘Ã£ Ä‘Æ°á»£c token hÃ³a vÃ  giáº£i mÃ£ láº¡i:\n",
      "<s> CÃ¢u há»i: Bá»©c xáº¡ nhiá»‡t tá»« máº·t Ä‘áº¥t ngÆ°á»£c trá»Ÿ láº¡i khÃ´ng khÃ­ áº£nh hÆ°á»Ÿng nhÆ° tháº¿ nÃ o Ä‘áº¿n vá»›i táº§ng Ä‘á»‘i lÆ°u? Ngá»¯ cáº£nh: NguyÃªn nhÃ¢n cÃ¡c biáº¿n Ä‘á»•i nhiá»‡t Ä‘á»™ trong táº§ng Ä‘á»‘i lÆ°u lÃ  do nhiá»‡t Ä‘á»™ Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh bá»Ÿi bá»©c xáº¡ nhiá»‡t tá»« máº·t Ä‘áº¥t ngÆ°á»£c trá»Ÿ láº¡i khÃ´ng khÃ­. Máº·c dÃ¹ tia náº¯ng Máº·t Trá»i tiáº¿p xÃºc vá»›i pháº§n khÃ´ng khÃ­ á»Ÿ trÃªn cao trÆ°á»›c, nhÆ°ng khÃ´ng khÃ­ khÃ¡ trong suá»‘t nghÄ©a lÃ  nÃ³ háº¥p thá»¥ ráº¥t Ã­t nÄƒng lÆ°á»£ng cá»§a tia náº¯ng. Äa pháº§n nÄƒng lÆ°á»£ng Máº·t Trá»i rÆ¡i xuá»‘ng máº·t Ä‘áº¥t, táº¡i Ä‘Ã¢y, nÃ³ bá»‹ háº¥p thá»¥ máº¡nh bá»Ÿi máº·t Ä‘áº¥t, vÃ  lÃ m máº·t Ä‘áº¥t nÃ³ng lÃªn (nÃ³ng hÆ¡n khÃ´ng khÃ­ trÃªn cao). Máº·t Ä‘áº¥t nÃ³ng truyá»n nhiá»‡t trá»±c tiáº¿p cho lá»›p khÃ´ng khÃ­ gáº§n máº·t Ä‘áº¥t; khÃ´ng khÃ­ gáº§n máº·t Ä‘áº¥t nÃ³ng lÃªn vÃ  ná»Ÿ ra, nháº¹ hÆ¡n pháº§n khÃ´ng khÃ­ láº¡nh á»Ÿ trÃªn vÃ  bay lÃªn cao nhá» lá»±c Ä‘áº©y ÃcsimÃ©t. Khi khÃ´ng khÃ­ nÃ³ng bay lÃªn cao, nÃ³ giÃ£n ná»Ÿ Ä‘oáº¡n nhiá»‡t nghÄ©a lÃ  thá»ƒ tÃ­ch tÄƒng vÃ  nhiá»‡t Ä‘á»™ giáº£m (giá»‘ng nhÆ° cÃ¡ch hoáº¡t Ä‘á»™ng cá»§a má»™t sá»‘ tá»§ láº¡nh, mÃ¡y Ä‘iá»u hÃ²a). CÃ ng lÃªn cao, khÃ´ng khÃ­ cÃ ng nguá»™i dáº§n. Khi ra xa khá»i bá» máº·t TrÃ¡i Äáº¥t thÃ¬ nhiá»‡t Ä‘á»‘i lÆ°u cÃ³ cÃ¡c hiá»‡u á»©ng nhá» hÆ¡n vÃ  khÃ´ng khÃ­ láº¡nh hÆ¡n. á» cÃ¡c cao Ä‘á»™ lá»›n hÆ¡n thÃ¬ khÃ´ng khÃ­ loÃ£ng hÆ¡n vÃ  giá»¯ nhiá»‡t kÃ©m hÆ¡n, khiáº¿n cho nhiá»‡t bá»‹ táº£n Ä‘i háº¿t. Cá»© má»—i khi Ä‘á»™ cao tÄƒng lÃªn 1.000 mÃ©t thÃ¬ nhiá»‡t Ä‘á»™ láº¡i giáº£m trung bÃ¬nh khoáº£ng 6,5 Â°C.</s></s> Bá»©c xáº¡ nhiá»‡t tá»« máº·t Ä‘áº¥t khÃ´ng áº£nh hÆ°á»Ÿng nhiá»u Ä‘áº¿n táº§ng Ä‘á»‘i lÆ°u, vÃ¬ nhiá»‡t Ä‘á»™ trong táº§ng Ä‘á»‘i lÆ°u chá»§ yáº¿u Ä‘Æ°á»£c Ä‘iá»u chá»‰nh bá»Ÿi tia náº¯ng Máº·t Trá»i trá»±c tiáº¿p chiáº¿u vÃ o khÃ´ng khÃ­ trÃªn cao, lÃ m nÃ³ng khÃ´ng khÃ­ trÆ°á»›c khi nÃ³ tiáº¿p xÃºc vá»›i máº·t Ä‘áº¥t.</s>\n",
      "------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Kiá»ƒm tra 1 batch dá»¯ liá»‡u Ä‘áº§u vÃ o ---\")\n",
    "sample_batch = next(iter(train_loader))\n",
    "\n",
    "print(\"KÃ­ch thÆ°á»›c input_ids:\", sample_batch[\"input_ids\"].shape)\n",
    "print(\"KÃ­ch thÆ°á»›c attention_mask:\", sample_batch[\"attention_mask\"].shape)\n",
    "print(\"NhÃ£n trong batch:\", sample_batch[\"labels\"])\n",
    "\n",
    "# Giáº£i mÃ£ má»™t máº«u Ä‘á»ƒ xem nÃ³ trÃ´ng nhÆ° tháº¿ nÃ o\n",
    "decoded_text = tokenizer.decode(sample_batch[\"input_ids\"][0], skip_special_tokens=False)\n",
    "print(\"\\nMá»™t máº«u Ä‘Ã£ Ä‘Æ°á»£c token hÃ³a vÃ  giáº£i mÃ£ láº¡i:\")\n",
    "print(decoded_text)\n",
    "print(\"------------------------------------------\\n\")\n",
    "# --- Káº¾T THÃšC BÆ¯á»šC KIá»‚M TRA ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa66f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Kiá»ƒm tra chi tiáº¿t 5 máº«u Ä‘áº§u tiÃªn Ä‘á»ƒ so sÃ¡nh trÆ°á»›c vÃ  sau khi xá»­ lÃ½ ---\n",
      "\n",
      "=============== MáºªU 0 ===============\n",
      "Sá»‘ token gá»‘c (Æ°á»›c tÃ­nh): 280\n",
      "Sá»‘ token sau khi xá»­ lÃ½ (giá»›i háº¡n bá»Ÿi max_len=512): 280\n",
      "âœ…  OK: Äá»™ dÃ i máº«u náº±m trong giá»›i háº¡n, khÃ´ng bá»‹ cáº¯t.\n",
      "\n",
      "--- VÄƒn báº£n Gá»C  ---\n",
      "CÃ¢u há»i: Chiáº¿n sá»± nÄƒm 1950 Ä‘Ã£ dÃ¡nh báº¡i quÃ¢n Hoa Ká»³ o chiÃªn tráº¡n nÃ o? Ngá»¯ cáº£nh: Tá»« ngÃ y 25 thÃ¡ng 10 Ä‘áº¿n ngÃ y 5 thÃ¡ng 11 (1950) lÃ  chiáº¿n dá»‹ch Ä‘áº§u tiÃªn cá»§a Trung Quá»‘c. QuÃ¢n Trung Quá»‘c dÃ¹ng 2 sÆ° Ä‘oÃ n cá»§a quÃ¢n Ä‘oÃ n 42 tá»• chá»©c phÃ²ng ngá»± á»Ÿ khu vá»±c HoÃ ng Tháº£o LÄ©nh, PhÃ³ Chiáº¿n LÄ©nh thuá»™c máº·t tráº­n miá»n Ä‘Ã´ng, láº¡i dÃ¹ng 3 quÃ¢n Ä‘oÃ n vÃ  má»™t sÆ° Ä‘oÃ n cá»§a quÃ¢n Ä‘oÃ n 42 (sau tÄƒng thÃªm 2 quÃ¢n Ä‘oÃ n) pháº£n kÃ­ch á»Ÿ máº·t tráº­n miá»n TÃ¢y. Chiáº¿n dá»‹ch nÃ y Ä‘Ã£ Ä‘Ã¡nh lui quÃ¢n Má»¹ Ä‘áº¿n phÃ­a nam sÃ´ng Thanh XuyÃªn. NgÃ y 7 thÃ¡ng 11, cÃ¡c quÃ¢n Ä‘oÃ n 20, 26, 27 thuá»™c Binh Ä‘oÃ n 9 quÃ¢n ChÃ­ nguyá»‡n dÆ°á»›i quyá»n chá»‰ huy cá»§a TÆ° lá»‡nh kiÃªm ChÃ­nh uá»· Tá»‘ng Thá»i LuÃ¢n tiáº¿n vÃ o Triá»u TiÃªn. Tá»›i lÃºc nÃ y, binh lá»±c tÃ¡c chiáº¿n cá»§a Trung Quá»‘c lÃªn tá»›i 9 quÃ¢n Ä‘oÃ n gá»“m 30 sÆ° Ä‘oÃ n, tá»•ng cá»™ng hÆ¡n 380.000 quÃ¢n, chiáº¿m Æ°u tháº¿ hÆ¡n so vá»›i quÃ¢n LiÃªn Há»£p Quá»‘c gá»“m 5 quÃ¢n Ä‘oÃ n, 13 sÆ° Ä‘oÃ n, 3 lá»¯ Ä‘oÃ n, tá»•ng cá»™ng 220.000 quÃ¢n. [SEP] QuÃ¢n Trung Quá»‘c Ä‘Ã£ Ä‘Ã¡nh báº¡i quÃ¢n Hoa Ká»³ táº¡i phÃ­a báº¯c sÃ´ng Thanh XuyÃªn, nÆ¡i mÃ  há» chá»‰ sá»­ dá»¥ng má»™t sÆ° Ä‘oÃ n duy nháº¥t Ä‘á»ƒ pháº£n cÃ´ng, máº·c dÃ¹ lá»±c lÆ°á»£ng tá»•ng cá»™ng chá»‰ cÃ³ 180.000 quÃ¢n.\n",
      "\n",
      "--- VÄƒn báº£n SAU KHI DECODE tá»« input_ids ---\n",
      "<s> CÃ¢u há»i: Chiáº¿n sá»± nÄƒm 1950 Ä‘Ã£ dÃ¡nh báº¡i quÃ¢n Hoa Ká»³ o chiÃªn tráº¡n nÃ o? Ngá»¯ cáº£nh: Tá»« ngÃ y 25 thÃ¡ng 10 Ä‘áº¿n ngÃ y 5 thÃ¡ng 11 (1950) lÃ  chiáº¿n dá»‹ch Ä‘áº§u tiÃªn cá»§a Trung Quá»‘c. QuÃ¢n Trung Quá»‘c dÃ¹ng 2 sÆ° Ä‘oÃ n cá»§a quÃ¢n Ä‘oÃ n 42 tá»• chá»©c phÃ²ng ngá»± á»Ÿ khu vá»±c HoÃ ng Tháº£o LÄ©nh, PhÃ³ Chiáº¿n LÄ©nh thuá»™c máº·t tráº­n miá»n Ä‘Ã´ng, láº¡i dÃ¹ng 3 quÃ¢n Ä‘oÃ n vÃ  má»™t sÆ° Ä‘oÃ n cá»§a quÃ¢n Ä‘oÃ n 42 (sau tÄƒng thÃªm 2 quÃ¢n Ä‘oÃ n) pháº£n kÃ­ch á»Ÿ máº·t tráº­n miá»n TÃ¢y. Chiáº¿n dá»‹ch nÃ y Ä‘Ã£ Ä‘Ã¡nh lui quÃ¢n Má»¹ Ä‘áº¿n phÃ­a nam sÃ´ng Thanh XuyÃªn. NgÃ y 7 thÃ¡ng 11, cÃ¡c quÃ¢n Ä‘oÃ n 20, 26, 27 thuá»™c Binh Ä‘oÃ n 9 quÃ¢n ChÃ­ nguyá»‡n dÆ°á»›i quyá»n chá»‰ huy cá»§a TÆ° lá»‡nh kiÃªm ChÃ­nh uá»· Tá»‘ng Thá»i LuÃ¢n tiáº¿n vÃ o Triá»u TiÃªn. Tá»›i lÃºc nÃ y, binh lá»±c tÃ¡c chiáº¿n cá»§a Trung Quá»‘c lÃªn tá»›i 9 quÃ¢n Ä‘oÃ n gá»“m 30 sÆ° Ä‘oÃ n, tá»•ng cá»™ng hÆ¡n 380.000 quÃ¢n, chiáº¿m Æ°u tháº¿ hÆ¡n so vá»›i quÃ¢n LiÃªn Há»£p Quá»‘c gá»“m 5 quÃ¢n Ä‘oÃ n, 13 sÆ° Ä‘oÃ n, 3 lá»¯ Ä‘oÃ n, tá»•ng cá»™ng 220.000 quÃ¢n.</s></s> QuÃ¢n Trung Quá»‘c Ä‘Ã£ Ä‘Ã¡nh báº¡i quÃ¢n Hoa Ká»³ táº¡i phÃ­a báº¯c sÃ´ng Thanh XuyÃªn, nÆ¡i mÃ  há» chá»‰ sá»­ dá»¥ng má»™t sÆ° Ä‘oÃ n duy nháº¥t Ä‘á»ƒ pháº£n cÃ´ng, máº·c dÃ¹ lá»±c lÆ°á»£ng tá»•ng cá»™ng chá»‰ cÃ³ 180.000 quÃ¢n.</s>\n",
      "\n",
      "=============== MáºªU 1 ===============\n",
      "Sá»‘ token gá»‘c (Æ°á»›c tÃ­nh): 264\n",
      "Sá»‘ token sau khi xá»­ lÃ½ (giá»›i háº¡n bá»Ÿi max_len=512): 264\n",
      "âœ…  OK: Äá»™ dÃ i máº«u náº±m trong giá»›i háº¡n, khÃ´ng bá»‹ cáº¯t.\n",
      "\n",
      "--- VÄƒn báº£n Gá»C  ---\n",
      "CÃ¢u há»i: TÃ­nh cÃ¡ch cá»§a Edward cÃ³ pháº£i lÃ  hiá»n lÃ nh vÃ  nhÃ¢n háº­u, Ä‘áº·c biá»‡t lÃ  khi Ã´ng thÆ°á»ng Ä‘Æ°á»£c mÃ´ táº£ nhÆ° má»™t ngÆ°á»i dá»… gáº§n vÃ  khoan dung vá»›i nhá»¯ng ngÆ°á»i xung quanh khÃ´ng? Ngá»¯ cáº£nh: Edward ná»•i tiáº¿ng lÃ  má»™t ngÆ°á»i kháº¯c nghiá»‡t, vÃ  ráº¥t Ä‘Ã¡ng sá»£; má»™t cÃ¢u chuyá»‡n ká»ƒ vá» sá»± kiá»‡n TrÆ°á»Ÿng Tu viá»‡n St Paul's, muá»‘n Ä‘á»‘i Ä‘áº§u vá»›i Edward khi Ã´ng tÄƒng thuáº¿ lÃªn cao nÄƒm 1295, bá»‹ Ä‘áº©y ngÃ£ tá»« trÃªn cao vÃ  cháº¿t khi nhÃ  vua cÃ³ máº·t á»Ÿ Ä‘Ã³. Khi Edward xá»© Caernarfon Ä‘Ã²i Ã´ng phong cho sá»§ng nam cá»§a háº¯n Gaveston má»™t lÃ£nh Ä‘á»‹a bÃ¡ tÆ°á»›c, nhÃ  vua ná»•i cÆ¡n thá»‹nh ná»™ vÃ  rá»©t tá»«ng náº¯m tÃ³c cá»§a con trai Ã´ng. Má»™t sá»‘ ngÆ°á»i Ä‘Æ°Æ¡ng thá»i coi Edward lÃ  ngÆ°á»i Ä‘Ã¡ng sá»£, Ä‘áº·c biá»‡t lÃ  trong nhá»¯ng ngÃ y Ä‘áº§u cá»§a Ã´ng. BÃ i hÃ¡t Lewes nÄƒm 1264 mÃ´ táº£ Ã´ng giá»‘ng nhÆ° má»™t loÃ i bÃ¡o, loÃ i Ä‘á»™ng váº­t Ä‘Ã¡ng sá»£, máº¡nh máº½ vÃ  khÃ´ng thá»ƒ lÆ°á»ng trÆ°á»›c Ä‘Æ°á»£c. [SEP] Edward Ä‘Æ°á»£c biáº¿t Ä‘áº¿n lÃ  má»™t ngÆ°á»i hiá»n lÃ nh vÃ  nhÃ¢n háº­u, thÆ°á»ng xuyÃªn Ä‘Æ°á»£c mÃ´ táº£ nhÆ° má»™t ngÆ°á»i dá»… gáº§n vÃ  khoan dung. Ã”ng luÃ´n Ä‘á»‘i xá»­ nháº¹ nhÃ ng vá»›i nhá»¯ng ngÆ°á»i báº¥t Ä‘á»“ng quan Ä‘iá»ƒm, pháº£n Ã¡nh sá»± bao dung cá»§a mÃ¬nh.\n",
      "\n",
      "--- VÄƒn báº£n SAU KHI DECODE tá»« input_ids ---\n",
      "<s> CÃ¢u há»i: TÃ­nh cÃ¡ch cá»§a Edward cÃ³ pháº£i lÃ  hiá»n lÃ nh vÃ  nhÃ¢n háº­u, Ä‘áº·c biá»‡t lÃ  khi Ã´ng thÆ°á»ng Ä‘Æ°á»£c mÃ´ táº£ nhÆ° má»™t ngÆ°á»i dá»… gáº§n vÃ  khoan dung vá»›i nhá»¯ng ngÆ°á»i xung quanh khÃ´ng? Ngá»¯ cáº£nh: Edward ná»•i tiáº¿ng lÃ  má»™t ngÆ°á»i kháº¯c nghiá»‡t, vÃ  ráº¥t Ä‘Ã¡ng sá»£; má»™t cÃ¢u chuyá»‡n ká»ƒ vá» sá»± kiá»‡n TrÆ°á»Ÿng Tu viá»‡n St Paul's, muá»‘n Ä‘á»‘i Ä‘áº§u vá»›i Edward khi Ã´ng tÄƒng thuáº¿ lÃªn cao nÄƒm 1295, bá»‹ Ä‘áº©y ngÃ£ tá»« trÃªn cao vÃ  cháº¿t khi nhÃ  vua cÃ³ máº·t á»Ÿ Ä‘Ã³. Khi Edward xá»© Caernarfon Ä‘Ã²i Ã´ng phong cho sá»§ng nam cá»§a háº¯n Gaveston má»™t lÃ£nh Ä‘á»‹a bÃ¡ tÆ°á»›c, nhÃ  vua ná»•i cÆ¡n thá»‹nh ná»™ vÃ  rá»©t tá»«ng náº¯m tÃ³c cá»§a con trai Ã´ng. Má»™t sá»‘ ngÆ°á»i Ä‘Æ°Æ¡ng thá»i coi Edward lÃ  ngÆ°á»i Ä‘Ã¡ng sá»£, Ä‘áº·c biá»‡t lÃ  trong nhá»¯ng ngÃ y Ä‘áº§u cá»§a Ã´ng. BÃ i hÃ¡t Lewes nÄƒm 1264 mÃ´ táº£ Ã´ng giá»‘ng nhÆ° má»™t loÃ i bÃ¡o, loÃ i Ä‘á»™ng váº­t Ä‘Ã¡ng sá»£, máº¡nh máº½ vÃ  khÃ´ng thá»ƒ lÆ°á»ng trÆ°á»›c Ä‘Æ°á»£c.</s></s> Edward Ä‘Æ°á»£c biáº¿t Ä‘áº¿n lÃ  má»™t ngÆ°á»i hiá»n lÃ nh vÃ  nhÃ¢n háº­u, thÆ°á»ng xuyÃªn Ä‘Æ°á»£c mÃ´ táº£ nhÆ° má»™t ngÆ°á»i dá»… gáº§n vÃ  khoan dung. Ã”ng luÃ´n Ä‘á»‘i xá»­ nháº¹ nhÃ ng vá»›i nhá»¯ng ngÆ°á»i báº¥t Ä‘á»“ng quan Ä‘iá»ƒm, pháº£n Ã¡nh sá»± bao dung cá»§a mÃ¬nh.</s>\n",
      "\n",
      "=============== MáºªU 2 ===============\n",
      "Sá»‘ token gá»‘c (Æ°á»›c tÃ­nh): 310\n",
      "Sá»‘ token sau khi xá»­ lÃ½ (giá»›i háº¡n bá»Ÿi max_len=512): 310\n",
      "âœ…  OK: Äá»™ dÃ i máº«u náº±m trong giá»›i háº¡n, khÃ´ng bá»‹ cáº¯t.\n",
      "\n",
      "--- VÄƒn báº£n Gá»C  ---\n",
      "CÃ¢u há»i: Nhung ham mo co duoc thiet ke va cham khac nhu the nao z? Ngá»¯ cáº£nh: Roma cÃ³ má»™t lÆ°á»£ng lá»›n háº§m má»™ cá»• hoáº·c nhá»¯ng nÆ¡i chÃ´n cáº¥t ngáº§m trong thÃ nh phá»‘ hay gáº§n thÃ nh phá»‘, vá»›i sá»‘ lÆ°á»£ng Ã­t nháº¥t 40, má»™t sá»‘ vá»«a Ä‘Æ°á»£c phÃ¡t hiá»‡n chá»‰ trong vÃ i tháº­p ká»· gáº§n Ä‘Ã¢y. Máº·c dÃ¹ ná»•i tiáº¿ng nháº¥t lÃ  nhá»¯ng nÆ¡i chÃ´n cáº¥t KitÃ´ há»¯u nhÆ°ng váº«n cÃ³ má»™ ngoáº¡i giÃ¡o vÃ  ngÆ°á»i Do ThÃ¡i, hoáº·c Ä‘Æ°á»£c chÃ´n cáº¥t trong háº§m má»™ riÃªng biá»‡t hoáº·c náº±m xen káº½ chung vá»›i nhau trong má»™t khu Ä‘áº¥t. Nhá»¯ng háº§m má»™ quy mÃ´ lá»›n Ä‘áº§u tiÃªn Ä‘Æ°á»£c khai quáº­t tá»« tháº¿ ká»· thá»© hai trá»Ÿ Ä‘i. Ban Ä‘áº§u chÃºng Ä‘Æ°á»£c cháº¡m kháº¯c tá»« má»™t loáº¡i má»m tá»« tro nÃºi lá»­a lÃ  Ä‘Ã¡ tÃºp vÃ  Ä‘áº·t táº¡i nhá»¯ng vá»‹ trÃ­ ngoÃ i ranh giá»›i cá»§a thÃ nh phá»‘ vÃ¬ luáº­t La MÃ£ cáº¥m chÃ´n cáº¥t trong thÃ nh phá»‘. Hiá»‡n nay GiÃ¡o hoÃ ng náº¯m quyá»n vÃ  trÃ¡ch nhiá»‡m báº£o trÃ¬ cÃ¡c háº§m má»™. GiÃ¡o hoÃ ng Ä‘Ã£ trao quyá»n cho DÃ²ng SalÃªdiÃªng Don Bosco trong viá»‡c giÃ¡m sÃ¡t háº§m má»™ cá»§a ThÃ¡nh Callixtus á»Ÿ ngoáº¡i Ã´ Roma. [SEP] Nhá»¯ng háº§m má»™ cá»• thÆ°á»ng Ä‘Æ°á»£c thiáº¿t káº¿ vá»›i cÃ¡c bÃ­ch há»a vÃ  tranh kháº£m tinh xáº£o, mÃ´ táº£ cÃ¡c cáº£nh tÃ´n giÃ¡o vÃ  biá»ƒu tÆ°á»£ng vÄƒn hÃ³a. CÃ¡c nghá»‡ nhÃ¢n thá»i Ä‘Ã³ sá»­ dá»¥ng ká»¹ thuáº­t cháº¡m kháº¯c tá»‰ má»‰ Ä‘á»ƒ táº¡o ra nhá»¯ng hÃ nh lang uá»‘n lÆ°á»£n vÃ  phÃ²ng chÃ´n cáº¥t phá»©c\n",
      "\n",
      "--- VÄƒn báº£n SAU KHI DECODE tá»« input_ids ---\n",
      "<s> CÃ¢u há»i: Nhung ham mo co duoc thiet ke va cham khac nhu the nao z? Ngá»¯ cáº£nh: Roma cÃ³ má»™t lÆ°á»£ng lá»›n háº§m má»™ cá»• hoáº·c nhá»¯ng nÆ¡i chÃ´n cáº¥t ngáº§m trong thÃ nh phá»‘ hay gáº§n thÃ nh phá»‘, vá»›i sá»‘ lÆ°á»£ng Ã­t nháº¥t 40, má»™t sá»‘ vá»«a Ä‘Æ°á»£c phÃ¡t hiá»‡n chá»‰ trong vÃ i tháº­p ká»· gáº§n Ä‘Ã¢y. Máº·c dÃ¹ ná»•i tiáº¿ng nháº¥t lÃ  nhá»¯ng nÆ¡i chÃ´n cáº¥t KitÃ´ há»¯u nhÆ°ng váº«n cÃ³ má»™ ngoáº¡i giÃ¡o vÃ  ngÆ°á»i Do ThÃ¡i, hoáº·c Ä‘Æ°á»£c chÃ´n cáº¥t trong háº§m má»™ riÃªng biá»‡t hoáº·c náº±m xen káº½ chung vá»›i nhau trong má»™t khu Ä‘áº¥t. Nhá»¯ng háº§m má»™ quy mÃ´ lá»›n Ä‘áº§u tiÃªn Ä‘Æ°á»£c khai quáº­t tá»« tháº¿ ká»· thá»© hai trá»Ÿ Ä‘i. Ban Ä‘áº§u chÃºng Ä‘Æ°á»£c cháº¡m kháº¯c tá»« má»™t loáº¡i má»m tá»« tro nÃºi lá»­a lÃ  Ä‘Ã¡ tÃºp vÃ  Ä‘áº·t táº¡i nhá»¯ng vá»‹ trÃ­ ngoÃ i ranh giá»›i cá»§a thÃ nh phá»‘ vÃ¬ luáº­t La MÃ£ cáº¥m chÃ´n cáº¥t trong thÃ nh phá»‘. Hiá»‡n nay GiÃ¡o hoÃ ng náº¯m quyá»n vÃ  trÃ¡ch nhiá»‡m báº£o trÃ¬ cÃ¡c háº§m má»™. GiÃ¡o hoÃ ng Ä‘Ã£ trao quyá»n cho DÃ²ng SalÃªdiÃªng Don Bosco trong viá»‡c giÃ¡m sÃ¡t háº§m má»™ cá»§a ThÃ¡nh Callixtus á»Ÿ ngoáº¡i Ã´ Roma.</s></s> Nhá»¯ng háº§m má»™ cá»• thÆ°á»ng Ä‘Æ°á»£c thiáº¿t káº¿ vá»›i cÃ¡c bÃ­ch há»a vÃ  tranh kháº£m tinh xáº£o, mÃ´ táº£ cÃ¡c cáº£nh tÃ´n giÃ¡o vÃ  biá»ƒu tÆ°á»£ng vÄƒn hÃ³a. CÃ¡c nghá»‡ nhÃ¢n thá»i Ä‘Ã³ sá»­ dá»¥ng ká»¹ thuáº­t cháº¡m kháº¯c tá»‰ má»‰ Ä‘á»ƒ táº¡o ra nhá»¯ng hÃ nh lang uá»‘n lÆ°á»£n vÃ  phÃ²ng chÃ´n cáº¥t phá»©c</s>\n",
      "\n",
      "=============== MáºªU 3 ===============\n",
      "Sá»‘ token gá»‘c (Æ°á»›c tÃ­nh): 303\n",
      "Sá»‘ token sau khi xá»­ lÃ½ (giá»›i háº¡n bá»Ÿi max_len=512): 303\n",
      "âœ…  OK: Äá»™ dÃ i máº«u náº±m trong giá»›i háº¡n, khÃ´ng bá»‹ cáº¯t.\n",
      "\n",
      "--- VÄƒn báº£n Gá»C  ---\n",
      "CÃ¢u há»i: Thá»i gian ngÆ°á»i dÃ¢n HÃ n Quá»‘c pháº£i lao Ä‘á»™ng trong khoáº£ng thá»i gian 1960-1970 lÃ  bao lÃ¢u? Ngá»¯ cáº£nh: Káº¿ hoáº¡ch phÃ¡t triá»ƒn kinh táº¿ cá»§a HÃ n Quá»‘c trong tháº­p niÃªn 1960-1970 dá»±a vÃ o xuáº¥t kháº©u nhá» giÃ¡ thÃ nh tháº¥p. Chi phÃ­ sáº£n xuáº¥t Ä‘Æ°á»£c cá»‘ tÃ¬nh háº¡ tháº¥p Ä‘áº¿n má»©c tá»‘i thiá»ƒu báº±ng cÃ¡ch toÃ n dÃ¢n pháº£i cam chá»‹u gian khá»•, tiÃªu dÃ¹ng háº¿t sá»©c tiáº¿t kiá»‡m, chÃ­nh sÃ¡ch toÃ n quá»‘c tháº¯t lÆ°ng buá»™c bá»¥ng Ä‘Æ°á»£c Ã¡p dá»¥ng. CÃ¡c sáº£n pháº©m xa xá»‰ nhÆ° má»¹ pháº©m, oto cao cáº¥p, quáº§n Ã¡o thá»i trang, tivi mÃ u... bá»‹ háº¡n cháº¿ nháº­p kháº©u á»Ÿ má»©c tá»‘i Ä‘a. NgÆ°á»i dÃ¢n lÃ m viá»‡c náº·ng nhá»c vÃ  triá»n miÃªn, nhÆ°ng sá»‘ng kham khá»•. HÃ ng tuáº§n má»—i ngÆ°á»i dÃ¢n Ä‘á»u pháº£i nhá»‹n Äƒn má»™t bá»¯a, khÃ´ng hÃºt thuá»‘c ngoáº¡i nháº­p, khÃ´ng uá»‘ng cÃ  phÃª. Thá»i gian lao Ä‘á»™ng kÃ©o dÃ i Ä‘áº¿n 12-14 tiáº¿ng má»—i ngÃ y. Äiá»u kiá»‡n lao Ä‘á»™ng kÃ©m, lÆ°Æ¡ng ráº¥t tháº¥p. CÃ¡c nguá»“n tÃ i chÃ­nh cÃ³ Ä‘Æ°á»£c nhá» chÃ­nh sÃ¡ch tiáº¿t kiá»‡m Ä‘áº¿n má»©c kham khá»• láº¡i Ä‘Æ°á»£c tÃ¡i Ä‘áº§u tÆ° vÃ o sáº£n xuáº¥t. Phong trÃ o Saemaeul (cÃ²n gá»i lÃ  Phong trÃ o cá»™ng Ä‘á»“ng cÆ° dÃ¢n má»›i) cá»§a ChÃ­nh phá»§ táº­p trung vÃ o phÃ¡t triá»ƒn nÃ´ng thÃ´n HÃ n Quá»‘c báº±ng viá»‡c Ä‘á»™ng viÃªn ngÆ°á»i dÃ¢n lao Ä‘á»™ng cÃ´ng Ã­ch, cáº£i táº¡o cÆ¡ sá»Ÿ háº¡ táº§ng mÃ  khÃ´ng cáº§n Ä‘Æ°á»£c tráº£ lÆ°Æ¡ng. [SEP] NgÆ°á»i dÃ¢n HÃ n Quá»‘c trong khoáº£ng thá»i gian 1960-1970 pháº£i lao Ä‘á»™ng kÃ©o dÃ i tá»« 12-14 tiáº¿ng má»—i ngÃ y.\n",
      "\n",
      "--- VÄƒn báº£n SAU KHI DECODE tá»« input_ids ---\n",
      "<s> CÃ¢u há»i: Thá»i gian ngÆ°á»i dÃ¢n HÃ n Quá»‘c pháº£i lao Ä‘á»™ng trong khoáº£ng thá»i gian 1960-1970 lÃ  bao lÃ¢u? Ngá»¯ cáº£nh: Káº¿ hoáº¡ch phÃ¡t triá»ƒn kinh táº¿ cá»§a HÃ n Quá»‘c trong tháº­p niÃªn 1960-1970 dá»±a vÃ o xuáº¥t kháº©u nhá» giÃ¡ thÃ nh tháº¥p. Chi phÃ­ sáº£n xuáº¥t Ä‘Æ°á»£c cá»‘ tÃ¬nh háº¡ tháº¥p Ä‘áº¿n má»©c tá»‘i thiá»ƒu báº±ng cÃ¡ch toÃ n dÃ¢n pháº£i cam chá»‹u gian khá»•, tiÃªu dÃ¹ng háº¿t sá»©c tiáº¿t kiá»‡m, chÃ­nh sÃ¡ch toÃ n quá»‘c tháº¯t lÆ°ng buá»™c bá»¥ng Ä‘Æ°á»£c Ã¡p dá»¥ng. CÃ¡c sáº£n pháº©m xa xá»‰ nhÆ° má»¹ pháº©m, oto cao cáº¥p, quáº§n Ã¡o thá»i trang, tivi mÃ u... bá»‹ háº¡n cháº¿ nháº­p kháº©u á»Ÿ má»©c tá»‘i Ä‘a. NgÆ°á»i dÃ¢n lÃ m viá»‡c náº·ng nhá»c vÃ  triá»n miÃªn, nhÆ°ng sá»‘ng kham khá»•. HÃ ng tuáº§n má»—i ngÆ°á»i dÃ¢n Ä‘á»u pháº£i nhá»‹n Äƒn má»™t bá»¯a, khÃ´ng hÃºt thuá»‘c ngoáº¡i nháº­p, khÃ´ng uá»‘ng cÃ  phÃª. Thá»i gian lao Ä‘á»™ng kÃ©o dÃ i Ä‘áº¿n 12-14 tiáº¿ng má»—i ngÃ y. Äiá»u kiá»‡n lao Ä‘á»™ng kÃ©m, lÆ°Æ¡ng ráº¥t tháº¥p. CÃ¡c nguá»“n tÃ i chÃ­nh cÃ³ Ä‘Æ°á»£c nhá» chÃ­nh sÃ¡ch tiáº¿t kiá»‡m Ä‘áº¿n má»©c kham khá»• láº¡i Ä‘Æ°á»£c tÃ¡i Ä‘áº§u tÆ° vÃ o sáº£n xuáº¥t. Phong trÃ o Saemaeul (cÃ²n gá»i lÃ  Phong trÃ o cá»™ng Ä‘á»“ng cÆ° dÃ¢n má»›i) cá»§a ChÃ­nh phá»§ táº­p trung vÃ o phÃ¡t triá»ƒn nÃ´ng thÃ´n HÃ n Quá»‘c báº±ng viá»‡c Ä‘á»™ng viÃªn ngÆ°á»i dÃ¢n lao Ä‘á»™ng cÃ´ng Ã­ch, cáº£i táº¡o cÆ¡ sá»Ÿ háº¡ táº§ng mÃ  khÃ´ng cáº§n Ä‘Æ°á»£c tráº£ lÆ°Æ¡ng.</s></s> NgÆ°á»i dÃ¢n HÃ n Quá»‘c trong khoáº£ng thá»i gian 1960-1970 pháº£i lao Ä‘á»™ng kÃ©o dÃ i tá»« 12-14 tiáº¿ng má»—i ngÃ y.</s>\n",
      "\n",
      "=============== MáºªU 4 ===============\n",
      "Sá»‘ token gá»‘c (Æ°á»›c tÃ­nh): 345\n",
      "Sá»‘ token sau khi xá»­ lÃ½ (giá»›i háº¡n bá»Ÿi max_len=512): 345\n",
      "âœ…  OK: Äá»™ dÃ i máº«u náº±m trong giá»›i háº¡n, khÃ´ng bá»‹ cáº¯t.\n",
      "\n",
      "--- VÄƒn báº£n Gá»C  ---\n",
      "CÃ¢u há»i: Sau khi Nga tá»« chá»‘i bÃ¡n vÅ© khÃ­ vÃ  trong bá»‘i cáº£nh Trung Quá»‘c tá»« lÃ¢u Ä‘Ã£ cÃ³ lá»‹ch sá»­ há»£p tÃ¡c quÃ¢n sá»± cháº·t cháº½ vá»›i cÃ¡c nÆ°á»›c NATO, Trung Quá»‘c Ä‘Ã£ chuyá»ƒn hÆ°á»›ng sang mua vÅ© khÃ­ tá»« nÆ°á»›c nÃ o? Ngá»¯ cáº£nh: Khoa há»c vÃ  ká»¹ thuáº­t trong CÃ´ng nghiá»‡p quá»‘c phÃ²ng cá»§a Cá»™ng hÃ²a nhÃ¢n dÃ¢n Trung Hoa háº§u háº¿t Ä‘Æ°á»£c Ä‘áº·t ná»n mÃ³ng khi LiÃªn XÃ´ Ä‘áº§u tÆ° máº¡nh máº½ vÃ o Trung Quá»‘c vÃ o nhá»¯ng nÄƒm 1950. VÃ  pháº§n lá»›n cÃ¡c vÅ© khÃ­ quan trá»ng cá»§a LiÃªn XÃ´ Ä‘Ã£ Ä‘Æ°á»£c cáº¥p giáº¥y phÃ©p Ä‘á»ƒ sáº£n xuáº¥t táº¡i Trung Quá»‘c. CÅ©ng nhÆ° LiÃªn XÃ´ Ä‘Ã£ giÃºp Ä‘á»¡ phÃ¡t triá»ƒn cÃ´ng nghá»‡ háº¡t nhÃ¢n vÃ  vÅ© khÃ­ nguyÃªn tá»­ táº¡i Trung Quá»‘c. CHND Trung Hoa cÅ©ng Ä‘Ã£ cÃ³ Ä‘Æ°á»£c má»™t sá»‘ cÃ´ng nghá»‡ cá»§a Hoa Ká»³ khi má»‘i quan há»‡ giá»¯a hai nÆ°á»›c trá»Ÿ nÃªn ná»“ng áº¥m vÃ o nhá»¯ng nÄƒm 1970. CÅ©ng nhÆ° Trung Quá»‘c báº¯t Ä‘áº§u sao chÃ©p nhá»¯ng vÅ© khÃ­ mÃ  mÃ¬nh mua Ä‘Æ°á»£c tá»« phÆ°Æ¡ng TÃ¢y nhÆ°ng khÃ´ng nhiá»u do cÃ¡c nÆ°á»›c phÆ°Æ¡ng TÃ¢y tháº­n trá»ng hÆ¡n trong viá»‡c mua bÃ¡n vÅ© khÃ­ vá»›i Trung Quá»‘c cÅ©ng nhÆ° bá»‹ cáº¥m váº­n vÅ© khÃ­ vÃ o nÄƒm 1989. Äáº¿n nhá»¯ng nÄƒm 1990 thÃ¬ Trung Quá»‘c báº¯t Ä‘áº§u sao chÃ©p quy mÃ´ lá»›n cÃ¡c vÅ© khÃ­ hiá»‡n Ä‘áº¡i mua Ä‘Æ°á»£c tá»« Nga. CÃ²n khi Nga tá»« chá»‘i bÃ¡n cÃ¡c loáº¡i vÅ© khÃ­ cá»§a mÃ¬nh thÃ¬ Trung Quá»‘c chuyá»ƒn sang mua cá»§a Ukraina vá»‘n cÅ©ng sá»Ÿ há»¯u nhiá»u loáº¡i vÅ© khÃ­ hiá»‡n Ä‘áº¡i tá»« thá»i LiÃªn XÃ´. Hiá»‡n táº¡i thÃ¬ Trung Quá»‘c Ä‘ang tÃ­ch cá»±c sao chÃ©p cÃ¡c loáº¡i vÅ© khÃ­ cá»§a phÆ°Æ¡ng TÃ¢y mua Ä‘Æ°á»£c tá»« Israel. [SEP] Trung Quá»‘c Ä‘Ã£ chuyá»ƒn sang há»£p tÃ¡c mua vÅ© khÃ­ tá»« Brazil, quá»‘c gia khÃ´ng cÃ³ lá»‹ch sá»­ há»£p tÃ¡c quÃ¢n sá»± cháº·t cháº½ vá»›i Trung Quá»‘c vÃ  khÃ´ng náº±m trong liÃªn minh NATO.\n",
      "\n",
      "--- VÄƒn báº£n SAU KHI DECODE tá»« input_ids ---\n",
      "<s> CÃ¢u há»i: Sau khi Nga tá»« chá»‘i bÃ¡n vÅ© khÃ­ vÃ  trong bá»‘i cáº£nh Trung Quá»‘c tá»« lÃ¢u Ä‘Ã£ cÃ³ lá»‹ch sá»­ há»£p tÃ¡c quÃ¢n sá»± cháº·t cháº½ vá»›i cÃ¡c nÆ°á»›c NATO, Trung Quá»‘c Ä‘Ã£ chuyá»ƒn hÆ°á»›ng sang mua vÅ© khÃ­ tá»« nÆ°á»›c nÃ o? Ngá»¯ cáº£nh: Khoa há»c vÃ  ká»¹ thuáº­t trong CÃ´ng nghiá»‡p quá»‘c phÃ²ng cá»§a Cá»™ng hÃ²a nhÃ¢n dÃ¢n Trung Hoa háº§u háº¿t Ä‘Æ°á»£c Ä‘áº·t ná»n mÃ³ng khi LiÃªn XÃ´ Ä‘áº§u tÆ° máº¡nh máº½ vÃ o Trung Quá»‘c vÃ o nhá»¯ng nÄƒm 1950. VÃ  pháº§n lá»›n cÃ¡c vÅ© khÃ­ quan trá»ng cá»§a LiÃªn XÃ´ Ä‘Ã£ Ä‘Æ°á»£c cáº¥p giáº¥y phÃ©p Ä‘á»ƒ sáº£n xuáº¥t táº¡i Trung Quá»‘c. CÅ©ng nhÆ° LiÃªn XÃ´ Ä‘Ã£ giÃºp Ä‘á»¡ phÃ¡t triá»ƒn cÃ´ng nghá»‡ háº¡t nhÃ¢n vÃ  vÅ© khÃ­ nguyÃªn tá»­ táº¡i Trung Quá»‘c. CHND Trung Hoa cÅ©ng Ä‘Ã£ cÃ³ Ä‘Æ°á»£c má»™t sá»‘ cÃ´ng nghá»‡ cá»§a Hoa Ká»³ khi má»‘i quan há»‡ giá»¯a hai nÆ°á»›c trá»Ÿ nÃªn ná»“ng áº¥m vÃ o nhá»¯ng nÄƒm 1970. CÅ©ng nhÆ° Trung Quá»‘c báº¯t Ä‘áº§u sao chÃ©p nhá»¯ng vÅ© khÃ­ mÃ  mÃ¬nh mua Ä‘Æ°á»£c tá»« phÆ°Æ¡ng TÃ¢y nhÆ°ng khÃ´ng nhiá»u do cÃ¡c nÆ°á»›c phÆ°Æ¡ng TÃ¢y tháº­n trá»ng hÆ¡n trong viá»‡c mua bÃ¡n vÅ© khÃ­ vá»›i Trung Quá»‘c cÅ©ng nhÆ° bá»‹ cáº¥m váº­n vÅ© khÃ­ vÃ o nÄƒm 1989. Äáº¿n nhá»¯ng nÄƒm 1990 thÃ¬ Trung Quá»‘c báº¯t Ä‘áº§u sao chÃ©p quy mÃ´ lá»›n cÃ¡c vÅ© khÃ­ hiá»‡n Ä‘áº¡i mua Ä‘Æ°á»£c tá»« Nga. CÃ²n khi Nga tá»« chá»‘i bÃ¡n cÃ¡c loáº¡i vÅ© khÃ­ cá»§a mÃ¬nh thÃ¬ Trung Quá»‘c chuyá»ƒn sang mua cá»§a Ukraina vá»‘n cÅ©ng sá»Ÿ há»¯u nhiá»u loáº¡i vÅ© khÃ­ hiá»‡n Ä‘áº¡i tá»« thá»i LiÃªn XÃ´. Hiá»‡n táº¡i thÃ¬ Trung Quá»‘c Ä‘ang tÃ­ch cá»±c sao chÃ©p cÃ¡c loáº¡i vÅ© khÃ­ cá»§a phÆ°Æ¡ng TÃ¢y mua Ä‘Æ°á»£c tá»« Israel.</s></s> Trung Quá»‘c Ä‘Ã£ chuyá»ƒn sang há»£p tÃ¡c mua vÅ© khÃ­ tá»« Brazil, quá»‘c gia khÃ´ng cÃ³ lá»‹ch sá»­ há»£p tÃ¡c quÃ¢n sá»± cháº·t cháº½ vá»›i Trung Quá»‘c vÃ  khÃ´ng náº±m trong liÃªn minh NATO.</s>\n",
      "\n",
      "===========================================\n",
      "Kiá»ƒm tra hoÃ n táº¥t. HÃ£y so sÃ¡nh vÄƒn báº£n trÃªn Ä‘á»ƒ xem cÃ³ sá»± khÃ¡c biá»‡t á»Ÿ cuá»‘i chuá»—i khÃ´ng.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Kiá»ƒm tra chi tiáº¿t 5 máº«u Ä‘áº§u tiÃªn Ä‘á»ƒ so sÃ¡nh trÆ°á»›c vÃ  sau khi xá»­ lÃ½ ---\")\n",
    "\n",
    "# Láº¥y 5 máº«u Ä‘áº§u tiÃªn tá»« DataFrame gá»‘c Ä‘á»ƒ so sÃ¡nh\n",
    "num_samples_to_check = 5\n",
    "for i in range(num_samples_to_check):\n",
    "    print(f\"\\n=============== MáºªU {i} ===============\")\n",
    "\n",
    "    # 1. Láº¥y dá»¯ liá»‡u gá»‘c tá»« DataFrame\n",
    "    original_premise = train_df[\"premise\"].iloc[i]\n",
    "    original_hypothesis = train_df[\"hypothesis\"].iloc[i]\n",
    "    # Ná»‘i 2 chuá»—i láº¡i giá»‘ng cÃ¡ch tokenizer sáº½ tháº¥y chÃºng\n",
    "    original_combined_text = original_premise + \" [SEP] \" + original_hypothesis\n",
    "\n",
    "    # 2. Láº¥y dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ tá»« Dataset\n",
    "    processed_sample = train_dataset[i]\n",
    "    processed_input_ids = processed_sample[\"input_ids\"]\n",
    "\n",
    "    # 3. Giáº£i mÃ£ (decode) cÃ¡c input_ids Ä‘Ã£ xá»­ lÃ½ trá»Ÿ láº¡i thÃ nh vÄƒn báº£n\n",
    "    decoded_text = tokenizer.decode(processed_input_ids, skip_special_tokens=False)\n",
    "\n",
    "    # 4. So sÃ¡nh vÃ  in káº¿t quáº£\n",
    "    original_token_count = len(tokenizer.encode(original_premise, original_hypothesis))\n",
    "    processed_token_count = len(processed_input_ids)\n",
    "\n",
    "    print(f\"Sá»‘ token gá»‘c (Æ°á»›c tÃ­nh): {original_token_count}\")\n",
    "    print(\n",
    "        f\"Sá»‘ token sau khi xá»­ lÃ½ (giá»›i háº¡n bá»Ÿi max_len={cfg.MAX_LENGTH}): {processed_token_count}\"\n",
    "    )\n",
    "\n",
    "    if original_token_count > cfg.MAX_LENGTH:\n",
    "        print(\"âš ï¸  Cáº¢NH BÃO: Máº«u nÃ y Ä‘Ã£ bá»‹ cáº¯t bá»›t (truncated)!\")\n",
    "    else:\n",
    "        print(\"âœ…  OK: Äá»™ dÃ i máº«u náº±m trong giá»›i háº¡n, khÃ´ng bá»‹ cáº¯t.\")\n",
    "\n",
    "    print(\"\\n--- VÄƒn báº£n Gá»C  ---\")\n",
    "    print(original_combined_text)\n",
    "\n",
    "    print(\"\\n--- VÄƒn báº£n SAU KHI DECODE tá»« input_ids ---\")\n",
    "    print(decoded_text)\n",
    "\n",
    "print(\"\\n===========================================\")\n",
    "print(\n",
    "    \"Kiá»ƒm tra hoÃ n táº¥t. HÃ£y so sÃ¡nh vÄƒn báº£n trÃªn Ä‘á»ƒ xem cÃ³ sá»± khÃ¡c biá»‡t á»Ÿ cuá»‘i chuá»—i khÃ´ng.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2739f76b",
   "metadata": {},
   "source": [
    "## 4. Thiáº¿t láº­p Huáº¥n luyá»‡n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62005cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 13:42:32 - [INFO] - BÆ°á»›c 4: Thiáº¿t láº­p mÃ´i trÆ°á»ng huáº¥n luyá»‡n vÃ  kiáº¿n trÃºc model...\n",
      "2025-10-17 13:42:32 - [INFO] - Sá»­ dá»¥ng thiáº¿t bá»‹: cuda\n",
      "2025-10-17 13:42:32 - [INFO] - âœ… TÃ¬m tháº¥y 1 GPU(s).\n",
      "2025-10-17 13:42:32 - [INFO] - âœ… Äang sá»­ dá»¥ng GPU: NVIDIA GeForce RTX 5070 Ti\n",
      "2025-10-17 13:42:32 - [INFO] - Kiáº¿n trÃºc cá»§a mÃ´ hÃ¬nh:\n",
      "XLMRobertaForSequenceClassification(\n",
      "  (roberta): XLMRobertaModel(\n",
      "    (embeddings): XLMRobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): XLMRobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-23): 24 x XLMRobertaLayer(\n",
      "          (attention): XLMRobertaAttention(\n",
      "            (self): XLMRobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): XLMRobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): XLMRobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): XLMRobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): XLMRobertaClassificationHead(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): XLMRobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch  # Äáº£m báº£o Ä‘Ã£ import torch\n",
    "\n",
    "logger.info(\"BÆ°á»›c 4: Thiáº¿t láº­p mÃ´i trÆ°á»ng huáº¥n luyá»‡n vÃ  kiáº¿n trÃºc model...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Log thÃ´ng tin thiáº¿t bá»‹ (GPU/CPU) ---\n",
    "logger.info(f\"Sá»­ dá»¥ng thiáº¿t bá»‹: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    logger.info(f\"âœ… TÃ¬m tháº¥y {gpu_count} GPU(s).\")\n",
    "    logger.info(f\"âœ… Äang sá»­ dá»¥ng GPU: {gpu_name}\")\n",
    "else:\n",
    "    logger.warning(\"âš ï¸ KhÃ´ng tÃ¬m tháº¥y GPU, sá»­ dá»¥ng CPU. QuÃ¡ trÃ¬nh training sáº½ ráº¥t cháº­m.\")\n",
    "\n",
    "# --- Báº®T Äáº¦U PHáº¦N THÃŠM Má»šI ---\n",
    "# Chuyá»ƒn toÃ n bá»™ kiáº¿n trÃºc model thÃ nh dáº¡ng string Ä‘á»ƒ Ä‘Æ°a vÃ o logger\n",
    "model_architecture_string = str(model)\n",
    "\n",
    "# Ghi log kiáº¿n trÃºc model\n",
    "logger.info(f\"Kiáº¿n trÃºc cá»§a mÃ´ hÃ¬nh:\\n{model_architecture_string}\")\n",
    "# --- Káº¾T THÃšC PHáº¦N THÃŠM Má»šI ---\n",
    "\n",
    "# Di chuyá»ƒn model Ä‘áº¿n device Ä‘Ã£ chá»n\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8449d146",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=cfg.LEARNING_RATE,\n",
    "    weight_decay=cfg.WEIGHT_DECAY,\n",
    "    eps=cfg.EPSILON,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a39f57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 13:42:32 - [INFO] - Scheduler will run for 3500 total steps (350 per epoch)\n",
      "2025-10-17 13:42:32 - [INFO] - Sá»­ dá»¥ng scheduler chung: cosine\n",
      "2025-10-17 13:42:32 - [INFO] - Warmup steps: 350\n",
      "2025-10-17 13:42:32 - [INFO] - Sá»­ dá»¥ng scheduler chung: cosine\n",
      "2025-10-17 13:42:32 - [INFO] - Warmup steps: 350\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    get_scheduler,\n",
    "    get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "num_update_steps_per_epoch = math.ceil(len(train_loader) / gradient_accumulation_steps)\n",
    "num_training_steps = num_update_steps_per_epoch * cfg.EPOCHS\n",
    "logger.info(\n",
    "    \"Scheduler will run for %s total steps (%s per epoch)\",\n",
    "    num_training_steps,\n",
    "    num_update_steps_per_epoch,\n",
    ")\n",
    "\n",
    "if cfg.TOTAL_STEP_SCALE <= 0:\n",
    "    warmup_steps = 0\n",
    "elif cfg.TOTAL_STEP_SCALE <= 1:\n",
    "    warmup_steps = max(1, int(cfg.TOTAL_STEP_SCALE * num_training_steps))\n",
    "else:\n",
    "    warmup_steps = min(int(cfg.TOTAL_STEP_SCALE), num_training_steps)\n",
    "\n",
    "# <<< THÃŠM KHá»I Lá»†NH IF Äá»‚ Xá»¬ LÃ TRÆ¯á»œNG Há»¢P Äáº¶C BIá»†T\n",
    "if cfg.SCHEDULER_TYPE == \"cosine_with_restarts\":\n",
    "    logger.info(\n",
    "        f\"Sá»­ dá»¥ng scheduler chuyÃªn dá»¥ng: cosine_with_hard_restarts_schedule_with_warmup vá»›i {cfg.NUM_CYCLES} chu ká»³.\"\n",
    "    )\n",
    "    scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=num_training_steps,\n",
    "        num_cycles=cfg.NUM_CYCLES,  # <-- Tham sá»‘ chuyÃªn biá»‡t hoáº¡t Ä‘á»™ng á»Ÿ Ä‘Ã¢y!\n",
    "    )\n",
    "else:\n",
    "    # Giá»¯ láº¡i hÃ m get_scheduler chung cho táº¥t cáº£ cÃ¡c loáº¡i scheduler khÃ¡c\n",
    "    logger.info(f\"Sá»­ dá»¥ng scheduler chung: {cfg.SCHEDULER_TYPE}\")\n",
    "    scheduler = get_scheduler(\n",
    "        cfg.SCHEDULER_TYPE,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "logger.info(\"Warmup steps: %s\", warmup_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73d99dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 13:42:32 - [INFO] - Sá»­ dá»¥ng Class Weights & Label smoothing cho hÃ m loss.\n"
     ]
    }
   ],
   "source": [
    "# Chuyá»ƒn class weights tá»« config thÃ nh tensor vÃ  Ä‘Æ°a lÃªn device\n",
    "if cfg.CLASS_WEIGHTS:\n",
    "    logger.info(\"Sá»­ dá»¥ng Class Weights & Label smoothing cho hÃ m loss.\")\n",
    "    class_weights_tensor = torch.tensor(cfg.CLASS_WEIGHTS, dtype=torch.float).to(device)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(\n",
    "        weight=class_weights_tensor,\n",
    "        label_smoothing=cfg.LABEL_SMOOTHING,\n",
    "    ).to(device)\n",
    "else:\n",
    "    logger.info(\"Sá»­ dá»¥ng CrossEntropyLoss thÃ´ng thÆ°á»ng (khÃ´ng cÃ³ trá»ng sá»‘).\")\n",
    "    loss_fn = torch.nn.CrossEntropyLoss().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e5a7d7",
   "metadata": {},
   "source": [
    "## 5. VÃ²ng láº·p Huáº¥n luyá»‡n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275d4c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 13:42:32 - [INFO] - --- Epoch 1/10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e5703ad10714280998fd86adcf03a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/1400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 13:46:59 - [INFO] - Loss trung bÃ¬nh trÃªn táº­p train: 1.0761\n",
      "2025-10-17 13:46:59 - [INFO] - Current Learning Rate: 8.00e-06\n",
      "2025-10-17 13:46:59 - [INFO] - Báº¯t Ä‘áº§u Ä‘Ã¡nh giÃ¡ trÃªn táº­p validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d11fd650c1341bf918f13ec699ba39e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 13:47:19 - [INFO] - Validation Loss: 0.9351\n",
      "2025-10-17 13:47:19 - [INFO] - Validation Accuracy: 0.5850\n",
      "2025-10-17 13:47:19 - [INFO] - Validation Macro-F1: 0.5267\n",
      "2025-10-17 13:47:19 - [INFO] - Classification Report trÃªn táº­p validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no     0.6561    0.7862    0.7153       449\n",
      "   extrinsic     0.5313    0.8829    0.6634       461\n",
      "   intrinsic     0.6146    0.1204    0.2014       490\n",
      "\n",
      "    accuracy                         0.5850      1400\n",
      "   macro avg     0.6007    0.5965    0.5267      1400\n",
      "weighted avg     0.6005    0.5850    0.5183      1400\n",
      "\n",
      "2025-10-17 13:47:19 - [INFO] - ğŸ‰ Macro-F1 cáº£i thiá»‡n. Äang lÆ°u model tá»‘t nháº¥t vÃ o '/home/guest/Projects/CS221/models/xlm-roberta-large-tuned'...\n",
      "2025-10-17 13:47:22 - [INFO] - LÆ°u model thÃ nh cÃ´ng.\n",
      "2025-10-17 13:47:22 - [INFO] - --- Epoch 2/10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658255b1304f4ceb843033bfef6dd789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/1400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 13:51:47 - [INFO] - Loss trung bÃ¬nh trÃªn táº­p train: 0.8234\n",
      "2025-10-17 13:51:47 - [INFO] - Current Learning Rate: 7.76e-06\n",
      "2025-10-17 13:51:47 - [INFO] - Báº¯t Ä‘áº§u Ä‘Ã¡nh giÃ¡ trÃªn táº­p validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98781cae7a7b438eb56a14d6919db33c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 13:52:08 - [INFO] - Validation Loss: 0.7438\n",
      "2025-10-17 13:52:08 - [INFO] - Validation Accuracy: 0.7229\n",
      "2025-10-17 13:52:08 - [INFO] - Validation Macro-F1: 0.7144\n",
      "2025-10-17 13:52:08 - [INFO] - Classification Report trÃªn táº­p validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no     0.7457    0.8753    0.8053       449\n",
      "   extrinsic     0.6644    0.8330    0.7392       461\n",
      "   intrinsic     0.7966    0.4796    0.5987       490\n",
      "\n",
      "    accuracy                         0.7229      1400\n",
      "   macro avg     0.7356    0.7293    0.7144      1400\n",
      "weighted avg     0.7367    0.7229    0.7112      1400\n",
      "\n",
      "2025-10-17 13:52:08 - [INFO] - ğŸ‰ Macro-F1 cáº£i thiá»‡n. Äang lÆ°u model tá»‘t nháº¥t vÃ o '/home/guest/Projects/CS221/models/xlm-roberta-large-tuned'...\n",
      "2025-10-17 13:52:11 - [INFO] - LÆ°u model thÃ nh cÃ´ng.\n",
      "2025-10-17 13:52:11 - [INFO] - --- Epoch 3/10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c785d720cd45a38886976601e926f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/1400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 13:56:36 - [INFO] - Loss trung bÃ¬nh trÃªn táº­p train: 0.6770\n",
      "2025-10-17 13:56:36 - [INFO] - Current Learning Rate: 7.06e-06\n",
      "2025-10-17 13:56:36 - [INFO] - Báº¯t Ä‘áº§u Ä‘Ã¡nh giÃ¡ trÃªn táº­p validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b657e86f308448a4944994e5d0e55756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 13:56:57 - [INFO] - Validation Loss: 0.6567\n",
      "2025-10-17 13:56:57 - [INFO] - Validation Accuracy: 0.7800\n",
      "2025-10-17 13:56:57 - [INFO] - Validation Macro-F1: 0.7799\n",
      "2025-10-17 13:56:57 - [INFO] - Classification Report trÃªn táº­p validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no     0.7800    0.8686    0.8219       449\n",
      "   extrinsic     0.7981    0.7462    0.7713       461\n",
      "   intrinsic     0.7633    0.7306    0.7466       490\n",
      "\n",
      "    accuracy                         0.7800      1400\n",
      "   macro avg     0.7805    0.7818    0.7799      1400\n",
      "weighted avg     0.7801    0.7800    0.7789      1400\n",
      "\n",
      "2025-10-17 13:56:57 - [INFO] - ğŸ‰ Macro-F1 cáº£i thiá»‡n. Äang lÆ°u model tá»‘t nháº¥t vÃ o '/home/guest/Projects/CS221/models/xlm-roberta-large-tuned'...\n",
      "2025-10-17 13:56:59 - [INFO] - LÆ°u model thÃ nh cÃ´ng.\n",
      "2025-10-17 13:56:59 - [INFO] - --- Epoch 4/10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb63c2ebb7a41dd85f9ac2e147d56af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10:   0%|          | 0/1400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_macro_f1 = 0.0\n",
    "patience_counter = 0  # bien dem => early stopped khi f1 ko tang them => overfitting\n",
    "\n",
    "for epoch in range(cfg.EPOCHS):\n",
    "    logger.info(f\"--- Epoch {epoch + 1}/{cfg.EPOCHS} ---\")\n",
    "\n",
    "    avg_train_loss = train_one_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        epoch + 1,\n",
    "        cfg.EPOCHS,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    )\n",
    "    logger.info(f\"Loss trung bÃ¬nh trÃªn táº­p train: {avg_train_loss:.4f}\")\n",
    "\n",
    "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    logger.info(\n",
    "        f\"Current Learning Rate: {current_lr:.2e}\"\n",
    "    )  # DÃ¹ng Ä‘á»‹nh dáº¡ng khoa há»c e.g., 8.00e-06\n",
    "\n",
    "    # ÄÃ¡nh giÃ¡ trÃªn táº­p validation\n",
    "    logger.info(\"Báº¯t Ä‘áº§u Ä‘Ã¡nh giÃ¡ trÃªn táº­p validation...\")\n",
    "    val_labels, val_preds, avg_val_loss = evaluate(model, val_loader, loss_fn, device)\n",
    "\n",
    "    accuracy = accuracy_score(val_labels, val_preds)\n",
    "    macro_f1 = f1_score(val_labels, val_preds, average=\"macro\")\n",
    "\n",
    "    logger.info(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    logger.info(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "    logger.info(f\"Validation Macro-F1: {macro_f1:.4f}\")\n",
    "\n",
    "    # # In classification report chi tiáº¿t\n",
    "    target_names = [cfg.ID2LABEL[i] for i in range(len(cfg.LABEL_MAP))]\n",
    "\n",
    "    # In classification report chi tiáº¿t (cÃ³ thá»ƒ giá»¯ láº¡i print hoáº·c log tá»«ng dÃ²ng)\n",
    "    report = classification_report(\n",
    "        val_labels,\n",
    "        val_preds,\n",
    "        target_names=[cfg.ID2LABEL[i] for i in range(len(cfg.LABEL_MAP))],\n",
    "        digits=4,\n",
    "    )\n",
    "    logger.info(f\"Classification Report trÃªn táº­p validation:\\n{report}\")\n",
    "\n",
    "    # LÆ°u láº¡i model tá»‘t nháº¥t dá»±a trÃªn Macro-F1\n",
    "    if macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = macro_f1\n",
    "        patience_counter = 0  # << RESET Bá»˜ Äáº¾M\n",
    "\n",
    "        logger.info(\n",
    "            f\"ğŸ‰ Macro-F1 cáº£i thiá»‡n. Äang lÆ°u model tá»‘t nháº¥t vÃ o '{cfg.MODEL_OUTPUT_DIR}'...\"\n",
    "        )\n",
    "        if not os.path.exists(cfg.MODEL_OUTPUT_DIR):\n",
    "            os.makedirs(cfg.MODEL_OUTPUT_DIR)\n",
    "\n",
    "        model.save_pretrained(cfg.MODEL_OUTPUT_DIR)\n",
    "        tokenizer.save_pretrained(cfg.MODEL_OUTPUT_DIR)\n",
    "        logger.info(\"LÆ°u model thÃ nh cÃ´ng.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        logger.warning(\n",
    "            f\"Macro-F1 khÃ´ng cáº£i thiá»‡n. Patience: {patience_counter}/{cfg.PATIENCE_LIMIT}\"\n",
    "        )\n",
    "        if patience_counter >= cfg.PATIENCE_LIMIT:\n",
    "            logger.info(\"Early stopping! Dá»«ng huáº¥n luyá»‡n.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1843cf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"ğŸ QuÃ¡ trÃ¬nh huáº¥n luyá»‡n hoÃ n táº¥t.\")\n",
    "logger.info(\n",
    "    f\"Model tá»‘t nháº¥t vá»›i Macro-F1 = {best_macro_f1:.4f} Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i '{cfg.MODEL_OUTPUT_DIR}'\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451f5a5e",
   "metadata": {},
   "source": [
    "# PhÃ¢n phá»‘i káº¿t quáº£ Ä‘Ãºng/sai theo tá»«ng lá»›p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc9859",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_label_names = [cfg.ID2LABEL[label_id] for label_id in val_labels]\n",
    "pred_label_names = [cfg.ID2LABEL[pred_id] for pred_id in val_preds]\n",
    "evaluation_df = pd.DataFrame(\n",
    "    {\n",
    "        \"true_label\": val_label_names,\n",
    "        \"predicted_label\": pred_label_names,\n",
    "    }\n",
    ")\n",
    "evaluation_df[\"status\"] = evaluation_df.apply(\n",
    "    lambda row: (\n",
    "        \"correct\" if row[\"true_label\"] == row[\"predicted_label\"] else \"incorrect\"\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "distribution_table = (\n",
    "    evaluation_df.groupby([\"true_label\", \"status\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .rename_axis(None, axis=1)\n",
    "    .reset_index()\n",
    "    .sort_values(\"true_label\")\n",
    ")\n",
    "\n",
    "# 1. ThÃªm cá»™t 'total' báº±ng cÃ¡ch cá»™ng cá»™t 'correct' vÃ  'incorrect'\n",
    "distribution_table[\"total\"] = (\n",
    "    distribution_table[\"correct\"] + distribution_table[\"incorrect\"]\n",
    ")\n",
    "\n",
    "# 2. ThÃªm cá»™t tá»‰ lá»‡ Ä‘Ãºng (correct_rate)\n",
    "distribution_table[\"correct_rate\"] = (\n",
    "    distribution_table[\"correct\"] / distribution_table[\"total\"]\n",
    ")\n",
    "\n",
    "# 3. ThÃªm cá»™t tá»‰ lá»‡ sai (incorrect_rate)\n",
    "distribution_table[\"incorrect_rate\"] = (\n",
    "    distribution_table[\"incorrect\"] / distribution_table[\"total\"]\n",
    ")\n",
    "\n",
    "# (TÃ¹y chá»n) Format cÃ¡c cá»™t tá»‰ lá»‡ thÃ nh dáº¡ng pháº§n trÄƒm cho dá»… Ä‘á»c\n",
    "distribution_table[\"correct_rate\"] = distribution_table[\"correct_rate\"].map(\n",
    "    \"{:.2%}\".format\n",
    ")\n",
    "distribution_table[\"incorrect_rate\"] = distribution_table[\"incorrect_rate\"].map(\n",
    "    \"{:.2%}\".format\n",
    ")\n",
    "\n",
    "# In ra báº£ng káº¿t quáº£\n",
    "logger.info(f\"PhÃ¢n phá»‘i káº¿t quáº£ trÃªn tá»«ng lá»›p:\\n{distribution_table.to_string()}\")\n",
    "\n",
    "# Trong notebook, dÃ¹ng display() sáº½ cho báº£ng Ä‘áº¹p hÆ¡n\n",
    "print(\"Báº£ng phÃ¢n phá»‘i káº¿t quáº£ trÃªn tá»«ng lá»›p:\")\n",
    "display(distribution_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs221",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
