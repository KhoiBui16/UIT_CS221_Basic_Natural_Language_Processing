# predict_masked_llm_temp.py

import torch
from torch.utils.data import Dataset, DataLoader
import pandas as pd
from tqdm.auto import tqdm
import os
import zipfile
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Import c√°c c·∫•u h√¨nh v√† module c·∫ßn thi·∫øt
import config_temp_base as config  # S·ª≠ d·ª•ng config_temp.py
from model_temp_base import get_model_and_tokenizer


class InferenceDataset(Dataset):
    """Dataset cho qu√° tr√¨nh inference, kh√¥ng c·∫ßn nh√£n."""

    def __init__(self, texts, tokenizer, max_len):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]

        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            return_token_type_ids=False,
            padding="max_length",
            truncation=True,
            return_attention_mask=True,
            return_tensors="pt",
        )

        return {
            "input_ids": encoding["input_ids"].flatten(),
            "attention_mask": encoding["attention_mask"].flatten(),
        }


def predict(model, data_loader, device):
    """Ch·∫°y inference tr√™n model v√† tr·∫£ v·ªÅ list c√°c d·ª± ƒëo√°n."""
    model.eval()
    all_preds = []

    with torch.no_grad():
        for batch in tqdm(data_loader, desc="Predicting"):
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)

            outputs = model(input_ids, attention_mask=attention_mask)
            preds = torch.argmax(outputs.logits, dim=-1)

            all_preds.extend(preds.cpu().numpy())

    return all_preds


def main():
    """H√†m ch√≠nh ƒë·ªÉ ch·∫°y pipeline d·ª± ƒëo√°n v√† t·∫°o file submission."""
    # 1. T·∫£i d·ªØ li·ªáu test
    test_path = os.path.join(config.DATA_DIR, config.TEST_FILE)
    try:
        test_df = pd.read_csv(test_path)
        print(f"‚úÖ ƒê·ªçc th√†nh c√¥ng {len(test_df)} m·∫´u t·ª´ file test: {test_path}")
    except FileNotFoundError:
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file test t·∫°i {test_path}")
        return

    # 2. Load model v√† tokenizer ƒë√£ hu·∫•n luy·ªán
    print(f"ƒêang t·∫£i model ƒë√£ hu·∫•n luy·ªán t·ª´: {config.MODEL_OUTPUT_DIR}")

    # Thay v√¨ g·ªçi get_model_and_tokenizer, h√£y load tr·ª±c ti·∫øp t·ª´ th∆∞ m·ª•c ƒë√£ l∆∞u
    tokenizer = AutoTokenizer.from_pretrained(config.MODEL_OUTPUT_DIR)
    model = AutoModelForSequenceClassification.from_pretrained(config.MODEL_OUTPUT_DIR)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    print(f"üöÄ Model ƒë√£ ƒë∆∞·ª£c load v√† chuy·ªÉn sang thi·∫øt b·ªã: {device}")

    # 3. Chu·∫©n b·ªã d·ªØ li·ªáu test

    # # C·∫≠p nh·∫≠t c√°ch t·∫°o input_text  (co the thu cho xnli format nay truoc) => nho dong bo voi file predict

    # premise = "C√¢u h·ªèi: " + test_df['prompt'].astype(str) + " Ng·ªØ c·∫£nh: " + test_df['context'].astype(str)
    # hypothesis = test_df['response'].astype(str)
    # test_df['input_text'] = premise + " </s></s> " + hypothesis

    # test nay2 (0.7812)
    # ƒê·ªìng b·ªô format input gi·ªëng h·ªát file training  (co the thu cho xnli format nay sau) => nho dong bo voi file predict
    test_df["input_text"] = (
        test_df["prompt"]
        + " </s></s> "
        + test_df["response"]
        + " </s></s> "
        + test_df["context"]
    )

    # # Thu prompt moi:
    # test_df['input_text'] = (
    #     "C√¢u h·ªèi: " + test_df['prompt'].astype(str) + " </s></s> " +
    #     "C√¢u tr·∫£ l·ªùi: " + test_df['response'].astype(str) + " </s></s> " +
    #     "Ng·ªØ c·∫£nh: " + test_df['context'].astype(str)
    # )

    # test_df['input_text'] = "Gi·∫£ thuy·∫øt: Khi tr·∫£ l·ªùi c√¢u h·ªèi test_df['prompt'], c√¢u tr·∫£ l·ªùi test_df['response'] ƒë√£ ph·∫£n √°nh ƒë√∫ng v√† kh√¥ng m√¢u thu·∫´n v·ªõi th√¥ng tin trong ng·ªØ c·∫£nh. [SEP] Ng·ªØ c·∫£nh: test_df['context']" # Chua thu nghiem

    test_dataset = InferenceDataset(
        texts=test_df["input_text"].to_list(),
        tokenizer=tokenizer,
        max_len=config.MAX_LENGTH,
    )

    # TƒÉng BATCH_SIZE khi predict ƒë·ªÉ nhanh h∆°n
    test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE * 2)

    # 4. Ch·∫°y d·ª± ƒëo√°n
    predictions = predict(model, test_loader, device)

    # 5. Chuy·ªÉn ƒë·ªïi ID d·ª± ƒëo√°n th√†nh nh√£n d·∫°ng chu·ªói
    predicted_labels = [config.ID2LABEL[pred_id] for pred_id in predictions]

    # 6. T·∫°o file submission.csv
    submission_df = pd.DataFrame(
        {"id": test_df["id"], "predict_label": predicted_labels}
    )

    # T·∫°o th∆∞ m·ª•c submission n·∫øu ch∆∞a c√≥
    if not os.path.exists(config.SUBMISSION_DIR):
        os.makedirs(config.SUBMISSION_DIR)

    csv_path = os.path.join(config.SUBMISSION_DIR, config.SUBMISSION_CSV)
    submission_df.to_csv(csv_path, index=False)
    print(f"‚úÖ ƒê√£ t·∫°o th√†nh c√¥ng file submission: {csv_path}")
    print(submission_df.head(20))

    # 7. N√©n th√†nh file submit.zip
    zip_path = os.path.join(config.SUBMISSION_DIR, config.SUBMISSION_ZIP)
    with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zipf:
        zipf.write(csv_path, arcname=config.SUBMISSION_CSV)
    print(f"‚úÖ ƒê√£ n√©n th√†nh c√¥ng file zip: {zip_path}")

    print("\nüèÅ Qu√° tr√¨nh d·ª± ƒëo√°n v√† t·∫°o file submission ho√†n t·∫•t.")


if __name__ == "__main__":
    main()
