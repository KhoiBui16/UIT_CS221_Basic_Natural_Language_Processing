2025-10-16 22:21:03 - [INFO] - Logger cho 'uitnlp_CafeBERT-training' ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o. File log: logs/uitnlp_CafeBERT-training/2025-10-16_22-21-03.log
2025-10-16 22:21:03 - [INFO] - Logger initialized for uitnlp/CafeBERT
2025-10-16 22:21:03 - [INFO] - ============================================================
2025-10-16 22:21:03 - [INFO] - üöÄ STARTING TRAINING SESSION
2025-10-16 22:21:03 - [INFO] - ============================================================
2025-10-16 22:21:03 - [INFO] - ROOT_DIR: /home/guest/Projects/CS221
2025-10-16 22:21:03 - [INFO] - DATA_DIR: /home/guest/Projects/CS221/data
2025-10-16 22:21:03 - [INFO] - TRAIN_FILE: /home/guest/Projects/CS221/data/vihallu-train.csv
2025-10-16 22:21:03 - [INFO] - TEST_FILE: /home/guest/Projects/CS221/data/vihallu-public-test.csv
2025-10-16 22:21:03 - [INFO] - SUBMISSION_DIR: /home/guest/Projects/CS221/submission
2025-10-16 22:21:03 - [INFO] - SUBMISSION_CSV: submit.csv
2025-10-16 22:21:03 - [INFO] - SUBMISSION_ZIP: submit.zip
2025-10-16 22:21:03 - [INFO] - MODEL_NAME: uitnlp/CafeBERT
2025-10-16 22:21:03 - [INFO] - MODEL_OUTPUT_DIR: /home/guest/Projects/CS221/models/CafeBERT-tuned
2025-10-16 22:21:03 - [INFO] - MAX_LENGTH: 512
2025-10-16 22:21:03 - [INFO] - RANDOM_STATE: 42
2025-10-16 22:21:03 - [INFO] - EPOCHS: 10
2025-10-16 22:21:03 - [INFO] - BATCH_SIZE: 4
2025-10-16 22:21:03 - [INFO] - GRADIENT_ACCUMULATION_STEPS: 4
2025-10-16 22:21:03 - [INFO] - SCHEDULER_TYPE: cosine
2025-10-16 22:21:03 - [INFO] - LEARNING_RATE: 8e-06
2025-10-16 22:21:03 - [INFO] - WEIGHT_DECAY: 0.02
2025-10-16 22:21:03 - [INFO] - CLASSIFIER_DROPOUT: 0.05
2025-10-16 22:21:03 - [INFO] - EPSILON: 1e-08
2025-10-16 22:21:03 - [INFO] - PATIENCE_LIMIT: 2
2025-10-16 22:21:03 - [INFO] - TOTAL_STEP_SCALE: 0.1
2025-10-16 22:21:03 - [INFO] - LABEL_SMOOTHING: 0.05
2025-10-16 22:21:03 - [INFO] - VALIDATION_SPLIT_SIZE: 0.2
2025-10-16 22:21:03 - [INFO] - LABEL_MAP: {'intrinsic': 0, 'extrinsic': 1, 'no': 2}
2025-10-16 22:21:03 - [INFO] - ID2LABEL: {0: 'intrinsic', 1: 'extrinsic', 2: 'no'}
2025-10-16 22:21:03 - [INFO] - CLASS_WEIGHTS: [1.0393466963622866, 1.0114145354717525, 0.9531590413943355]
2025-10-16 22:21:03 - [INFO] - ============================================================
2025-10-16 22:21:05 - [INFO] - B·∫Øt ƒë·∫ßu pipeline hu·∫•n luy·ªán.
2025-10-16 22:21:05 - [INFO] - B∆∞·ªõc 1: Chu·∫©n b·ªã d·ªØ li·ªáu...
2025-10-16 22:21:05 - [INFO] - Chia d·ªØ li·ªáu: 5600 m·∫´u train, 1400 m·∫´u validation.
2025-10-16 22:21:06 - [INFO] - B∆∞·ªõc 2: T·∫£i model 'uitnlp/CafeBERT' v√† tokenizer...
2025-10-16 22:21:08 - [INFO] - Ph√¢n t√≠ch ki·∫øn tr√∫c m√¥ h√¨nh b·∫±ng torchinfo...
2025-10-16 22:21:10 - [INFO] - Ki·∫øn tr√∫c chi ti·∫øt c·ªßa m√¥ h√¨nh:
=====================================================================================================================================================================
Layer (type:depth-idx)                                            Input Shape               Output Shape              Param #                   Mult-Adds
=====================================================================================================================================================================
XLMRobertaForSequenceClassification                               --                        [4, 3]                    --                        --
‚îú‚îÄXLMRobertaModel: 1-1                                            [4, 512]                  [4, 512, 1024]            --                        --
‚îÇ    ‚îî‚îÄXLMRobertaEmbeddings: 2-1                                  --                        [4, 512, 1024]            --                        --
‚îÇ    ‚îÇ    ‚îî‚îÄEmbedding: 3-1                                        [4, 512]                  [4, 512, 1024]            256,002,048               1,024,008,192
‚îÇ    ‚îÇ    ‚îî‚îÄEmbedding: 3-2                                        [4, 512]                  [4, 512, 1024]            1,024                     4,096
‚îÇ    ‚îÇ    ‚îî‚îÄEmbedding: 3-3                                        [4, 512]                  [4, 512, 1024]            526,336                   2,105,344
‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-4                                        [4, 512, 1024]            [4, 512, 1024]            2,048                     8,192
‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-5                                          [4, 512, 1024]            [4, 512, 1024]            --                        --
‚îÇ    ‚îî‚îÄXLMRobertaEncoder: 2-2                                     [4, 512, 1024]            [4, 512, 1024]            --                        --
‚îÇ    ‚îÇ    ‚îî‚îÄModuleList: 3-6                                       --                        --                        302,309,376               --
‚îú‚îÄXLMRobertaClassificationHead: 1-2                               [4, 512, 1024]            [4, 3]                    --                        --
‚îÇ    ‚îî‚îÄDropout: 2-3                                               [4, 1024]                 [4, 1024]                 --                        --
‚îÇ    ‚îî‚îÄLinear: 2-4                                                [4, 1024]                 [4, 1024]                 1,049,600                 4,198,400
‚îÇ    ‚îî‚îÄDropout: 2-5                                               [4, 1024]                 [4, 1024]                 --                        --
‚îÇ    ‚îî‚îÄLinear: 2-6                                                [4, 1024]                 [4, 3]                    3,075                     12,300
=====================================================================================================================================================================
Total params: 559,893,507
Trainable params: 559,893,507
Non-trainable params: 0
Total mult-adds (Units.GIGABYTES): 2.24
=====================================================================================================================================================================
Input size (MB): 0.02
Forward/backward pass size (MB): 4496.33
Params size (MB): 2239.57
Estimated Total Size (MB): 6735.92
=====================================================================================================================================================================
2025-10-16 22:21:11 - [INFO] - B∆∞·ªõc 3: T·∫°o Dataset v√† DataLoader...
2025-10-16 22:21:11 - [INFO] - ‚úÖ T·∫°o DataLoader th√†nh c√¥ng v·ªõi DataCollatorWithPadding chu·∫©n!
2025-10-16 22:21:11 - [INFO] - Gradient accumulation steps: 4 | Effective batch size: 16
2025-10-16 22:21:11 - [INFO] - B∆∞·ªõc 4: Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng hu·∫•n luy·ªán v√† ki·∫øn tr√∫c model...
2025-10-16 22:21:11 - [INFO] - S·ª≠ d·ª•ng thi·∫øt b·ªã: cuda
2025-10-16 22:21:11 - [INFO] - ‚úÖ T√¨m th·∫•y 1 GPU(s).
2025-10-16 22:21:11 - [INFO] - ‚úÖ ƒêang s·ª≠ d·ª•ng GPU: NVIDIA GeForce RTX 5070 Ti
2025-10-16 22:21:11 - [INFO] - Ki·∫øn tr√∫c c·ªßa m√¥ h√¨nh:
XLMRobertaForSequenceClassification(
  (roberta): XLMRobertaModel(
    (embeddings): XLMRobertaEmbeddings(
      (word_embeddings): Embedding(250002, 1024, padding_idx=1)
      (position_embeddings): Embedding(514, 1024, padding_idx=1)
      (token_type_embeddings): Embedding(1, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.05, inplace=False)
    )
    (encoder): XLMRobertaEncoder(
      (layer): ModuleList(
        (0-23): 24 x XLMRobertaLayer(
          (attention): XLMRobertaAttention(
            (self): XLMRobertaSdpaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.05, inplace=False)
            )
            (output): XLMRobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.05, inplace=False)
            )
          )
          (intermediate): XLMRobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): XLMRobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.05, inplace=False)
          )
        )
      )
    )
  )
  (classifier): XLMRobertaClassificationHead(
    (dense): Linear(in_features=1024, out_features=1024, bias=True)
    (dropout): Dropout(p=0.05, inplace=False)
    (out_proj): Linear(in_features=1024, out_features=3, bias=True)
  )
)
2025-10-16 22:21:11 - [INFO] - Scheduler will run for 3500 total steps (350 per epoch)
2025-10-16 22:21:11 - [INFO] - Warmup steps: 350
2025-10-16 22:21:11 - [INFO] - S·ª≠ d·ª•ng Class Weights & Label smoothing cho h√†m loss.
2025-10-16 22:21:11 - [INFO] - --- Epoch 1/10 ---
2025-10-16 22:25:35 - [INFO] - Loss trung b√¨nh tr√™n t·∫≠p train: 1.0695
2025-10-16 22:25:35 - [INFO] - Current Learning Rate: 8.00e-06
2025-10-16 22:25:35 - [INFO] - B·∫Øt ƒë·∫ßu ƒë√°nh gi√° tr√™n t·∫≠p validation...
2025-10-16 22:25:56 - [INFO] - Validation Loss: 0.8680
2025-10-16 22:25:56 - [INFO] - Validation Accuracy: 0.6257
2025-10-16 22:25:56 - [INFO] - Validation Macro-F1: 0.6258
2025-10-16 22:25:56 - [INFO] - Classification Report tr√™n t·∫≠p validation:
              precision    recall  f1-score   support

   intrinsic     0.5362    0.4837    0.5086       490
   extrinsic     0.5990    0.7939    0.6828       461
          no     0.7867    0.6080    0.6859       449

    accuracy                         0.6257      1400
   macro avg     0.6407    0.6285    0.6258      1400
weighted avg     0.6372    0.6257    0.6228      1400

2025-10-16 22:25:56 - [INFO] - üéâ Macro-F1 c·∫£i thi·ªán. ƒêang l∆∞u model t·ªët nh·∫•t v√†o '/home/guest/Projects/CS221/models/CafeBERT-tuned'...
2025-10-16 22:25:58 - [INFO] - L∆∞u model th√†nh c√¥ng.
2025-10-16 22:25:58 - [INFO] - --- Epoch 2/10 ---
2025-10-16 22:30:21 - [INFO] - Loss trung b√¨nh tr√™n t·∫≠p train: 0.7124
2025-10-16 22:30:21 - [INFO] - Current Learning Rate: 7.76e-06
2025-10-16 22:30:21 - [INFO] - B·∫Øt ƒë·∫ßu ƒë√°nh gi√° tr√™n t·∫≠p validation...
2025-10-16 22:30:42 - [INFO] - Validation Loss: 0.7337
2025-10-16 22:30:42 - [INFO] - Validation Accuracy: 0.7307
2025-10-16 22:30:42 - [INFO] - Validation Macro-F1: 0.7236
2025-10-16 22:30:42 - [INFO] - Classification Report tr√™n t·∫≠p validation:
              precision    recall  f1-score   support

   intrinsic     0.8289    0.5143    0.6348       490
   extrinsic     0.7062    0.7874    0.7446       461
          no     0.7010    0.9087    0.7915       449

    accuracy                         0.7307      1400
   macro avg     0.7454    0.7368    0.7236      1400
weighted avg     0.7475    0.7307    0.7212      1400

2025-10-16 22:30:42 - [INFO] - üéâ Macro-F1 c·∫£i thi·ªán. ƒêang l∆∞u model t·ªët nh·∫•t v√†o '/home/guest/Projects/CS221/models/CafeBERT-tuned'...
2025-10-16 22:30:44 - [INFO] - L∆∞u model th√†nh c√¥ng.
2025-10-16 22:30:44 - [INFO] - --- Epoch 3/10 ---
2025-10-16 22:35:07 - [INFO] - Loss trung b√¨nh tr√™n t·∫≠p train: 0.5565
2025-10-16 22:35:07 - [INFO] - Current Learning Rate: 7.06e-06
2025-10-16 22:35:07 - [INFO] - B·∫Øt ƒë·∫ßu ƒë√°nh gi√° tr√™n t·∫≠p validation...
2025-10-16 22:35:28 - [INFO] - Validation Loss: 0.6862
2025-10-16 22:35:28 - [INFO] - Validation Accuracy: 0.7743
2025-10-16 22:35:28 - [INFO] - Validation Macro-F1: 0.7746
2025-10-16 22:35:28 - [INFO] - Classification Report tr√™n t·∫≠p validation:
              precision    recall  f1-score   support

   intrinsic     0.7741    0.7204    0.7463       490
   extrinsic     0.7473    0.7636    0.7554       461
          no     0.8013    0.8441    0.8221       449

    accuracy                         0.7743      1400
   macro avg     0.7742    0.7760    0.7746      1400
weighted avg     0.7740    0.7743    0.7736      1400

2025-10-16 22:35:28 - [INFO] - üéâ Macro-F1 c·∫£i thi·ªán. ƒêang l∆∞u model t·ªët nh·∫•t v√†o '/home/guest/Projects/CS221/models/CafeBERT-tuned'...
2025-10-16 22:35:31 - [INFO] - L∆∞u model th√†nh c√¥ng.
2025-10-16 22:35:31 - [INFO] - --- Epoch 4/10 ---
2025-10-16 22:39:54 - [INFO] - Loss trung b√¨nh tr√™n t·∫≠p train: 0.4240
2025-10-16 22:39:54 - [INFO] - Current Learning Rate: 6.00e-06
2025-10-16 22:39:54 - [INFO] - B·∫Øt ƒë·∫ßu ƒë√°nh gi√° tr√™n t·∫≠p validation...
2025-10-16 22:40:15 - [INFO] - Validation Loss: 0.7775
2025-10-16 22:40:15 - [INFO] - Validation Accuracy: 0.7707
2025-10-16 22:40:15 - [INFO] - Validation Macro-F1: 0.7708
2025-10-16 22:40:15 - [INFO] - Classification Report tr√™n t·∫≠p validation:
              precision    recall  f1-score   support

   intrinsic     0.7672    0.7061    0.7354       490
   extrinsic     0.7489    0.7570    0.7530       461
          no     0.7950    0.8552    0.8240       449

    accuracy                         0.7707      1400
   macro avg     0.7704    0.7728    0.7708      1400
weighted avg     0.7701    0.7707    0.7696      1400

2025-10-16 22:40:15 - [WARNING] - Macro-F1 kh√¥ng c·∫£i thi·ªán. Patience: 1/2
2025-10-16 22:40:15 - [INFO] - --- Epoch 5/10 ---
2025-10-16 22:41:02 - [INFO] - üèÅ Qu√° tr√¨nh hu·∫•n luy·ªán ho√†n t·∫•t.
2025-10-16 22:41:02 - [INFO] - Model t·ªët nh·∫•t v·ªõi Macro-F1 = 0.7746 ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i '/home/guest/Projects/CS221/models/CafeBERT-tuned'
2025-10-16 22:41:06 - [INFO] - Ph√¢n ph·ªëi k·∫øt qu·∫£ tr√™n t·ª´ng l·ªõp:
  true_label  correct  incorrect  total correct_rate incorrect_rate
0  extrinsic      349        112    461       75.70%         24.30%
1  intrinsic      346        144    490       70.61%         29.39%
2         no      384         65    449       85.52%         14.48%
