2025-10-16 19:32:40 - [INFO] - Logger cho 'joeddav_xlm-roberta-large-xnli-training' ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o. File log: logs/joeddav_xlm-roberta-large-xnli-training/2025-10-16_19-32-40.log
2025-10-16 19:32:40 - [INFO] - Logger initialized for joeddav/xlm-roberta-large-xnli
2025-10-16 19:32:40 - [INFO] - ============================================================
2025-10-16 19:32:40 - [INFO] - üöÄ STARTING TRAINING SESSION
2025-10-16 19:32:40 - [INFO] - ============================================================
2025-10-16 19:32:40 - [INFO] - ROOT_DIR: /home/guest/Projects/CS221
2025-10-16 19:32:40 - [INFO] - DATA_DIR: /home/guest/Projects/CS221/data
2025-10-16 19:32:40 - [INFO] - TRAIN_FILE: /home/guest/Projects/CS221/data/vihallu-train.csv
2025-10-16 19:32:40 - [INFO] - TEST_FILE: /home/guest/Projects/CS221/data/vihallu-public-test.csv
2025-10-16 19:32:40 - [INFO] - SUBMISSION_DIR: /home/guest/Projects/CS221/submission
2025-10-16 19:32:40 - [INFO] - SUBMISSION_CSV: submit.csv
2025-10-16 19:32:40 - [INFO] - SUBMISSION_ZIP: submit.zip
2025-10-16 19:32:40 - [INFO] - MODEL_NAME: joeddav/xlm-roberta-large-xnli
2025-10-16 19:32:40 - [INFO] - MODEL_OUTPUT_DIR: /home/guest/Projects/CS221/models/xlm-roberta-large-xnli-tuned
2025-10-16 19:32:40 - [INFO] - MAX_LENGTH: 512
2025-10-16 19:32:40 - [INFO] - RANDOM_STATE: 42
2025-10-16 19:32:40 - [INFO] - EPOCHS: 10
2025-10-16 19:32:40 - [INFO] - BATCH_SIZE: 4
2025-10-16 19:32:40 - [INFO] - GRADIENT_ACCUMULATION_STEPS: 4
2025-10-16 19:32:40 - [INFO] - SCHEDULER_TYPE: cosine
2025-10-16 19:32:40 - [INFO] - LEARNING_RATE: 6e-06
2025-10-16 19:32:40 - [INFO] - WEIGHT_DECAY: 0.02
2025-10-16 19:32:40 - [INFO] - CLASSIFIER_DROPOUT: 0.05
2025-10-16 19:32:40 - [INFO] - EPSILON: 1e-08
2025-10-16 19:32:40 - [INFO] - PATIENCE_LIMIT: 2
2025-10-16 19:32:40 - [INFO] - TOTAL_STEP_SCALE: 0.1
2025-10-16 19:32:40 - [INFO] - LABEL_SMOOTHING: 0.05
2025-10-16 19:32:40 - [INFO] - VALIDATION_SPLIT_SIZE: 0.2
2025-10-16 19:32:40 - [INFO] - LABEL_MAP: {'intrinsic': 0, 'extrinsic': 1, 'no': 2}
2025-10-16 19:32:40 - [INFO] - ID2LABEL: {0: 'intrinsic', 1: 'extrinsic', 2: 'no'}
2025-10-16 19:32:40 - [INFO] - CLASS_WEIGHTS: [1.0393466963622866, 1.0114145354717525, 0.9531590413943355]
2025-10-16 19:32:40 - [INFO] - ============================================================
2025-10-16 19:32:42 - [INFO] - B·∫Øt ƒë·∫ßu pipeline hu·∫•n luy·ªán.
2025-10-16 19:32:42 - [INFO] - B∆∞·ªõc 1: Chu·∫©n b·ªã d·ªØ li·ªáu...
2025-10-16 19:32:42 - [INFO] - Chia d·ªØ li·ªáu: 5600 m·∫´u train, 1400 m·∫´u validation.
2025-10-16 19:32:43 - [INFO] - B∆∞·ªõc 2: T·∫£i model 'joeddav/xlm-roberta-large-xnli' v√† tokenizer...
2025-10-16 19:32:45 - [INFO] - Ph√¢n t√≠ch ki·∫øn tr√∫c m√¥ h√¨nh b·∫±ng torchinfo...
2025-10-16 19:32:47 - [INFO] - Ki·∫øn tr√∫c chi ti·∫øt c·ªßa m√¥ h√¨nh:
=====================================================================================================================================================================
Layer (type:depth-idx)                                            Input Shape               Output Shape              Param #                   Mult-Adds
=====================================================================================================================================================================
XLMRobertaForSequenceClassification                               --                        [4, 3]                    --                        --
‚îú‚îÄXLMRobertaModel: 1-1                                            [4, 512]                  [4, 512, 1024]            --                        --
‚îÇ    ‚îî‚îÄXLMRobertaEmbeddings: 2-1                                  --                        [4, 512, 1024]            --                        --
‚îÇ    ‚îÇ    ‚îî‚îÄEmbedding: 3-1                                        [4, 512]                  [4, 512, 1024]            256,002,048               1,024,008,192
‚îÇ    ‚îÇ    ‚îî‚îÄEmbedding: 3-2                                        [4, 512]                  [4, 512, 1024]            1,024                     4,096
‚îÇ    ‚îÇ    ‚îî‚îÄEmbedding: 3-3                                        [4, 512]                  [4, 512, 1024]            526,336                   2,105,344
‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-4                                        [4, 512, 1024]            [4, 512, 1024]            2,048                     8,192
‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-5                                          [4, 512, 1024]            [4, 512, 1024]            --                        --
‚îÇ    ‚îî‚îÄXLMRobertaEncoder: 2-2                                     [4, 512, 1024]            [4, 512, 1024]            --                        --
‚îÇ    ‚îÇ    ‚îî‚îÄModuleList: 3-6                                       --                        --                        302,309,376               --
‚îú‚îÄXLMRobertaClassificationHead: 1-2                               [4, 512, 1024]            [4, 3]                    --                        --
‚îÇ    ‚îî‚îÄDropout: 2-3                                               [4, 1024]                 [4, 1024]                 --                        --
‚îÇ    ‚îî‚îÄLinear: 2-4                                                [4, 1024]                 [4, 1024]                 1,049,600                 4,198,400
‚îÇ    ‚îî‚îÄDropout: 2-5                                               [4, 1024]                 [4, 1024]                 --                        --
‚îÇ    ‚îî‚îÄLinear: 2-6                                                [4, 1024]                 [4, 3]                    3,075                     12,300
=====================================================================================================================================================================
Total params: 559,893,507
Trainable params: 559,893,507
Non-trainable params: 0
Total mult-adds (Units.GIGABYTES): 2.24
=====================================================================================================================================================================
Input size (MB): 0.02
Forward/backward pass size (MB): 4496.33
Params size (MB): 2239.57
Estimated Total Size (MB): 6735.92
=====================================================================================================================================================================
2025-10-16 19:32:47 - [INFO] - B∆∞·ªõc 3: T·∫°o Dataset v√† DataLoader...
2025-10-16 19:32:47 - [INFO] - ‚úÖ T·∫°o DataLoader th√†nh c√¥ng v·ªõi pipeline t·ªëi ∆∞u!
2025-10-16 19:32:47 - [INFO] - Gradient accumulation steps: 4 | Effective batch size: 16
2025-10-16 19:32:47 - [INFO] - B∆∞·ªõc 4: Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng hu·∫•n luy·ªán v√† ki·∫øn tr√∫c model...
2025-10-16 19:32:47 - [INFO] - S·ª≠ d·ª•ng thi·∫øt b·ªã: cuda
2025-10-16 19:32:47 - [INFO] - ‚úÖ T√¨m th·∫•y 1 GPU(s).
2025-10-16 19:32:47 - [INFO] - ‚úÖ ƒêang s·ª≠ d·ª•ng GPU: NVIDIA GeForce RTX 5070 Ti
2025-10-16 19:32:47 - [INFO] - Ki·∫øn tr√∫c c·ªßa m√¥ h√¨nh:
XLMRobertaForSequenceClassification(
  (roberta): XLMRobertaModel(
    (embeddings): XLMRobertaEmbeddings(
      (word_embeddings): Embedding(250002, 1024, padding_idx=1)
      (position_embeddings): Embedding(514, 1024, padding_idx=1)
      (token_type_embeddings): Embedding(1, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.05, inplace=False)
    )
    (encoder): XLMRobertaEncoder(
      (layer): ModuleList(
        (0-23): 24 x XLMRobertaLayer(
          (attention): XLMRobertaAttention(
            (self): XLMRobertaSdpaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.05, inplace=False)
            )
            (output): XLMRobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.05, inplace=False)
            )
          )
          (intermediate): XLMRobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): XLMRobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.05, inplace=False)
          )
        )
      )
    )
  )
  (classifier): XLMRobertaClassificationHead(
    (dense): Linear(in_features=1024, out_features=1024, bias=True)
    (dropout): Dropout(p=0.05, inplace=False)
    (out_proj): Linear(in_features=1024, out_features=3, bias=True)
  )
)
2025-10-16 19:32:47 - [INFO] - Scheduler will run for 3500 total steps (350 per epoch)
2025-10-16 19:32:47 - [INFO] - Warmup steps: 350
2025-10-16 19:32:47 - [INFO] - S·ª≠ d·ª•ng Class Weights & Label smoothing cho h√†m loss.
2025-10-16 19:32:47 - [INFO] - --- Epoch 1/10 ---
2025-10-16 19:37:12 - [INFO] - Loss trung b√¨nh tr√™n t·∫≠p train: 0.9016
2025-10-16 19:37:12 - [INFO] - Current Learning Rate: 6.00e-06
2025-10-16 19:37:12 - [INFO] - B·∫Øt ƒë·∫ßu ƒë√°nh gi√° tr√™n t·∫≠p validation...
2025-10-16 19:37:33 - [INFO] - Validation Loss: 0.6703
2025-10-16 19:37:33 - [INFO] - Validation Accuracy: 0.7800
2025-10-16 19:37:33 - [INFO] - Validation Macro-F1: 0.7782
2025-10-16 19:37:33 - [INFO] - Classification Report tr√™n t·∫≠p validation:
              precision    recall  f1-score   support

   intrinsic     0.8376    0.6735    0.7466       490
   extrinsic     0.7716    0.7549    0.7632       461
          no     0.7459    0.9220    0.8247       449

    accuracy                         0.7800      1400
   macro avg     0.7850    0.7835    0.7782      1400
weighted avg     0.7865    0.7800    0.7771      1400

2025-10-16 19:37:33 - [INFO] - üéâ Macro-F1 c·∫£i thi·ªán. ƒêang l∆∞u model t·ªët nh·∫•t v√†o '/home/guest/Projects/CS221/models/xlm-roberta-large-xnli-tuned'...
2025-10-16 19:37:35 - [INFO] - L∆∞u model th√†nh c√¥ng.
2025-10-16 19:37:35 - [INFO] - --- Epoch 2/10 ---
2025-10-16 19:41:59 - [INFO] - Loss trung b√¨nh tr√™n t·∫≠p train: 0.5629
2025-10-16 19:41:59 - [INFO] - Current Learning Rate: 5.82e-06
2025-10-16 19:41:59 - [INFO] - B·∫Øt ƒë·∫ßu ƒë√°nh gi√° tr√™n t·∫≠p validation...
2025-10-16 19:42:19 - [INFO] - Validation Loss: 0.6481
2025-10-16 19:42:19 - [INFO] - Validation Accuracy: 0.7993
2025-10-16 19:42:19 - [INFO] - Validation Macro-F1: 0.7999
2025-10-16 19:42:19 - [INFO] - Classification Report tr√™n t·∫≠p validation:
              precision    recall  f1-score   support

   intrinsic     0.8218    0.7245    0.7701       490
   extrinsic     0.7426    0.8200    0.7794       461
          no     0.8410    0.8597    0.8502       449

    accuracy                         0.7993      1400
   macro avg     0.8018    0.8014    0.7999      1400
weighted avg     0.8019    0.7993    0.7988      1400

2025-10-16 19:42:19 - [INFO] - üéâ Macro-F1 c·∫£i thi·ªán. ƒêang l∆∞u model t·ªët nh·∫•t v√†o '/home/guest/Projects/CS221/models/xlm-roberta-large-xnli-tuned'...
2025-10-16 19:42:21 - [INFO] - L∆∞u model th√†nh c√¥ng.
2025-10-16 19:42:21 - [INFO] - --- Epoch 3/10 ---
2025-10-16 19:46:45 - [INFO] - Loss trung b√¨nh tr√™n t·∫≠p train: 0.4306
2025-10-16 19:46:45 - [INFO] - Current Learning Rate: 5.30e-06
2025-10-16 19:46:45 - [INFO] - B·∫Øt ƒë·∫ßu ƒë√°nh gi√° tr√™n t·∫≠p validation...
2025-10-16 19:47:05 - [INFO] - Validation Loss: 0.6733
2025-10-16 19:47:05 - [INFO] - Validation Accuracy: 0.7986
2025-10-16 19:47:05 - [INFO] - Validation Macro-F1: 0.7987
2025-10-16 19:47:05 - [INFO] - Classification Report tr√™n t·∫≠p validation:
              precision    recall  f1-score   support

   intrinsic     0.8009    0.7633    0.7816       490
   extrinsic     0.7773    0.7722    0.7748       461
          no     0.8168    0.8641    0.8398       449

    accuracy                         0.7986      1400
   macro avg     0.7983    0.7999    0.7987      1400
weighted avg     0.7982    0.7986    0.7980      1400

2025-10-16 19:47:05 - [WARNING] - Macro-F1 kh√¥ng c·∫£i thi·ªán. Patience: 1/2
2025-10-16 19:47:05 - [INFO] - --- Epoch 4/10 ---
2025-10-16 19:51:27 - [INFO] - Loss trung b√¨nh tr√™n t·∫≠p train: 0.3274
2025-10-16 19:51:27 - [INFO] - Current Learning Rate: 4.50e-06
2025-10-16 19:51:27 - [INFO] - B·∫Øt ƒë·∫ßu ƒë√°nh gi√° tr√™n t·∫≠p validation...
2025-10-16 19:51:48 - [INFO] - Validation Loss: 0.8162
2025-10-16 19:51:48 - [INFO] - Validation Accuracy: 0.7886
2025-10-16 19:51:48 - [INFO] - Validation Macro-F1: 0.7881
2025-10-16 19:51:48 - [INFO] - Classification Report tr√™n t·∫≠p validation:
              precision    recall  f1-score   support

   intrinsic     0.7707    0.7612    0.7659       490
   extrinsic     0.7990    0.7245    0.7600       461
          no     0.7972    0.8842    0.8384       449

    accuracy                         0.7886      1400
   macro avg     0.7890    0.7900    0.7881      1400
weighted avg     0.7885    0.7886    0.7872      1400

2025-10-16 19:51:48 - [WARNING] - Macro-F1 kh√¥ng c·∫£i thi·ªán. Patience: 2/2
2025-10-16 19:51:48 - [INFO] - Early stopping! D·ª´ng hu·∫•n luy·ªán.
2025-10-16 19:51:48 - [INFO] - üèÅ Qu√° tr√¨nh hu·∫•n luy·ªán ho√†n t·∫•t.
2025-10-16 19:51:48 - [INFO] - Model t·ªët nh·∫•t v·ªõi Macro-F1 = 0.7999 ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i '/home/guest/Projects/CS221/models/xlm-roberta-large-xnli-tuned'
2025-10-16 19:51:48 - [INFO] - Ph√¢n ph·ªëi k·∫øt qu·∫£ tr√™n t·ª´ng l·ªõp:
  true_label  correct  incorrect  total correct_rate incorrect_rate
0  extrinsic      334        127    461       72.45%         27.55%
1  intrinsic      373        117    490       76.12%         23.88%
2         no      397         52    449       88.42%         11.58%
