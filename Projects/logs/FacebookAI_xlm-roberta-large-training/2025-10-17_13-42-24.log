2025-10-17 13:42:24 - [INFO] - Logger cho 'FacebookAI_xlm-roberta-large-training' ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o. File log: logs/FacebookAI_xlm-roberta-large-training/2025-10-17_13-42-24.log
2025-10-17 13:42:24 - [INFO] - Logger initialized for FacebookAI/xlm-roberta-large
2025-10-17 13:42:24 - [INFO] - ============================================================
2025-10-17 13:42:24 - [INFO] - üöÄ STARTING TRAINING SESSION
2025-10-17 13:42:24 - [INFO] - ============================================================
2025-10-17 13:42:24 - [INFO] - ROOT_DIR: /home/guest/Projects/CS221
2025-10-17 13:42:24 - [INFO] - DATA_DIR: /home/guest/Projects/CS221/data
2025-10-17 13:42:24 - [INFO] - TRAIN_FILE: /home/guest/Projects/CS221/data/vihallu-train.csv
2025-10-17 13:42:24 - [INFO] - TEST_FILE: /home/guest/Projects/CS221/data/vihallu-public-test.csv
2025-10-17 13:42:24 - [INFO] - SUBMISSION_DIR: /home/guest/Projects/CS221/submission
2025-10-17 13:42:24 - [INFO] - SUBMISSION_CSV: submit.csv
2025-10-17 13:42:24 - [INFO] - SUBMISSION_ZIP: submit.zip
2025-10-17 13:42:24 - [INFO] - MODEL_NAME: FacebookAI/xlm-roberta-large
2025-10-17 13:42:24 - [INFO] - MODEL_OUTPUT_DIR: /home/guest/Projects/CS221/models/xlm-roberta-large-tuned
2025-10-17 13:42:24 - [INFO] - MAX_LENGTH: 512
2025-10-17 13:42:24 - [INFO] - RANDOM_STATE: 42
2025-10-17 13:42:24 - [INFO] - EPOCHS: 10
2025-10-17 13:42:24 - [INFO] - BATCH_SIZE: 4
2025-10-17 13:42:24 - [INFO] - GRADIENT_ACCUMULATION_STEPS: 4
2025-10-17 13:42:24 - [INFO] - SCHEDULER_TYPE: cosine
2025-10-17 13:42:24 - [INFO] - LEARNING_RATE: 8e-06
2025-10-17 13:42:24 - [INFO] - WEIGHT_DECAY: 0.03
2025-10-17 13:42:24 - [INFO] - NUM_CYCLES: 3
2025-10-17 13:42:24 - [INFO] - CLASSIFIER_DROPOUT: 0.1
2025-10-17 13:42:24 - [INFO] - LABEL_SMOOTHING: 0.05
2025-10-17 13:42:24 - [INFO] - TOTAL_STEP_SCALE: 0.1
2025-10-17 13:42:24 - [INFO] - EPSILON: 1e-08
2025-10-17 13:42:24 - [INFO] - PATIENCE_LIMIT: 3
2025-10-17 13:42:24 - [INFO] - VALIDATION_SPLIT_SIZE: 0.2
2025-10-17 13:42:24 - [INFO] - LABEL_MAP: {'no': 0, 'extrinsic': 1, 'intrinsic': 2}
2025-10-17 13:42:24 - [INFO] - ID2LABEL: {0: 'no', 1: 'extrinsic', 2: 'intrinsic'}
2025-10-17 13:42:24 - [INFO] - CLASS_WEIGHTS: [1.0393466963622866, 1.0114145354717525, 0.9531590413943355]
2025-10-17 13:42:24 - [INFO] - ============================================================
2025-10-17 13:42:27 - [INFO] - B·∫Øt ƒë·∫ßu pipeline hu·∫•n luy·ªán.
2025-10-17 13:42:27 - [INFO] - B∆∞·ªõc 1: Chu·∫©n b·ªã d·ªØ li·ªáu...
2025-10-17 13:42:27 - [INFO] - Chia d·ªØ li·ªáu: 5600 m·∫´u train, 1400 m·∫´u validation.
2025-10-17 13:42:27 - [INFO] - B∆∞·ªõc 2: T·∫£i model 'FacebookAI/xlm-roberta-large' v√† tokenizer...
2025-10-17 13:42:29 - [INFO] - Ph√¢n t√≠ch ki·∫øn tr√∫c m√¥ h√¨nh b·∫±ng torchinfo...
2025-10-17 13:42:32 - [INFO] - Ki·∫øn tr√∫c chi ti·∫øt c·ªßa m√¥ h√¨nh:
=====================================================================================================================================================================
Layer (type:depth-idx)                                            Input Shape               Output Shape              Param #                   Mult-Adds
=====================================================================================================================================================================
XLMRobertaForSequenceClassification                               --                        [4, 3]                    --                        --
‚îú‚îÄXLMRobertaModel: 1-1                                            [4, 512]                  [4, 512, 1024]            --                        --
‚îÇ    ‚îî‚îÄXLMRobertaEmbeddings: 2-1                                  --                        [4, 512, 1024]            --                        --
‚îÇ    ‚îÇ    ‚îî‚îÄEmbedding: 3-1                                        [4, 512]                  [4, 512, 1024]            256,002,048               1,024,008,192
‚îÇ    ‚îÇ    ‚îî‚îÄEmbedding: 3-2                                        [4, 512]                  [4, 512, 1024]            1,024                     4,096
‚îÇ    ‚îÇ    ‚îî‚îÄEmbedding: 3-3                                        [4, 512]                  [4, 512, 1024]            526,336                   2,105,344
‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-4                                        [4, 512, 1024]            [4, 512, 1024]            2,048                     8,192
‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-5                                          [4, 512, 1024]            [4, 512, 1024]            --                        --
‚îÇ    ‚îî‚îÄXLMRobertaEncoder: 2-2                                     [4, 512, 1024]            [4, 512, 1024]            --                        --
‚îÇ    ‚îÇ    ‚îî‚îÄModuleList: 3-6                                       --                        --                        302,309,376               --
‚îú‚îÄXLMRobertaClassificationHead: 1-2                               [4, 512, 1024]            [4, 3]                    --                        --
‚îÇ    ‚îî‚îÄDropout: 2-3                                               [4, 1024]                 [4, 1024]                 --                        --
‚îÇ    ‚îî‚îÄLinear: 2-4                                                [4, 1024]                 [4, 1024]                 1,049,600                 4,198,400
‚îÇ    ‚îî‚îÄDropout: 2-5                                               [4, 1024]                 [4, 1024]                 --                        --
‚îÇ    ‚îî‚îÄLinear: 2-6                                                [4, 1024]                 [4, 3]                    3,075                     12,300
=====================================================================================================================================================================
Total params: 559,893,507
Trainable params: 559,893,507
Non-trainable params: 0
Total mult-adds (Units.GIGABYTES): 2.24
=====================================================================================================================================================================
Input size (MB): 0.02
Forward/backward pass size (MB): 4496.33
Params size (MB): 2239.57
Estimated Total Size (MB): 6735.92
=====================================================================================================================================================================
2025-10-17 13:42:32 - [INFO] - B∆∞·ªõc 3: T·∫°o Dataset v√† DataLoader...
2025-10-17 13:42:32 - [INFO] - ‚úÖ T·∫°o DataLoader th√†nh c√¥ng v·ªõi DataCollatorWithPadding chu·∫©n!
2025-10-17 13:42:32 - [INFO] - Gradient accumulation steps: 4 | Effective batch size: 16
2025-10-17 13:42:32 - [INFO] - B∆∞·ªõc 4: Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng hu·∫•n luy·ªán v√† ki·∫øn tr√∫c model...
2025-10-17 13:42:32 - [INFO] - S·ª≠ d·ª•ng thi·∫øt b·ªã: cuda
2025-10-17 13:42:32 - [INFO] - ‚úÖ T√¨m th·∫•y 1 GPU(s).
2025-10-17 13:42:32 - [INFO] - ‚úÖ ƒêang s·ª≠ d·ª•ng GPU: NVIDIA GeForce RTX 5070 Ti
2025-10-17 13:42:32 - [INFO] - Ki·∫øn tr√∫c c·ªßa m√¥ h√¨nh:
XLMRobertaForSequenceClassification(
  (roberta): XLMRobertaModel(
    (embeddings): XLMRobertaEmbeddings(
      (word_embeddings): Embedding(250002, 1024, padding_idx=1)
      (position_embeddings): Embedding(514, 1024, padding_idx=1)
      (token_type_embeddings): Embedding(1, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): XLMRobertaEncoder(
      (layer): ModuleList(
        (0-23): 24 x XLMRobertaLayer(
          (attention): XLMRobertaAttention(
            (self): XLMRobertaSdpaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): XLMRobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): XLMRobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): XLMRobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (classifier): XLMRobertaClassificationHead(
    (dense): Linear(in_features=1024, out_features=1024, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=1024, out_features=3, bias=True)
  )
)
2025-10-17 13:42:32 - [INFO] - Scheduler will run for 3500 total steps (350 per epoch)
2025-10-17 13:42:32 - [INFO] - S·ª≠ d·ª•ng scheduler chung: cosine
2025-10-17 13:42:32 - [INFO] - Warmup steps: 350
2025-10-17 13:42:32 - [INFO] - S·ª≠ d·ª•ng Class Weights & Label smoothing cho h√†m loss.
2025-10-17 13:42:32 - [INFO] - --- Epoch 1/10 ---
2025-10-17 13:46:59 - [INFO] - Loss trung b√¨nh tr√™n t·∫≠p train: 1.0761
2025-10-17 13:46:59 - [INFO] - Current Learning Rate: 8.00e-06
2025-10-17 13:46:59 - [INFO] - B·∫Øt ƒë·∫ßu ƒë√°nh gi√° tr√™n t·∫≠p validation...
2025-10-17 13:47:19 - [INFO] - Validation Loss: 0.9351
2025-10-17 13:47:19 - [INFO] - Validation Accuracy: 0.5850
2025-10-17 13:47:19 - [INFO] - Validation Macro-F1: 0.5267
2025-10-17 13:47:19 - [INFO] - Classification Report tr√™n t·∫≠p validation:
              precision    recall  f1-score   support

          no     0.6561    0.7862    0.7153       449
   extrinsic     0.5313    0.8829    0.6634       461
   intrinsic     0.6146    0.1204    0.2014       490

    accuracy                         0.5850      1400
   macro avg     0.6007    0.5965    0.5267      1400
weighted avg     0.6005    0.5850    0.5183      1400

2025-10-17 13:47:19 - [INFO] - üéâ Macro-F1 c·∫£i thi·ªán. ƒêang l∆∞u model t·ªët nh·∫•t v√†o '/home/guest/Projects/CS221/models/xlm-roberta-large-tuned'...
2025-10-17 13:47:22 - [INFO] - L∆∞u model th√†nh c√¥ng.
2025-10-17 13:47:22 - [INFO] - --- Epoch 2/10 ---
2025-10-17 13:51:47 - [INFO] - Loss trung b√¨nh tr√™n t·∫≠p train: 0.8234
2025-10-17 13:51:47 - [INFO] - Current Learning Rate: 7.76e-06
2025-10-17 13:51:47 - [INFO] - B·∫Øt ƒë·∫ßu ƒë√°nh gi√° tr√™n t·∫≠p validation...
2025-10-17 13:52:08 - [INFO] - Validation Loss: 0.7438
2025-10-17 13:52:08 - [INFO] - Validation Accuracy: 0.7229
2025-10-17 13:52:08 - [INFO] - Validation Macro-F1: 0.7144
2025-10-17 13:52:08 - [INFO] - Classification Report tr√™n t·∫≠p validation:
              precision    recall  f1-score   support

          no     0.7457    0.8753    0.8053       449
   extrinsic     0.6644    0.8330    0.7392       461
   intrinsic     0.7966    0.4796    0.5987       490

    accuracy                         0.7229      1400
   macro avg     0.7356    0.7293    0.7144      1400
weighted avg     0.7367    0.7229    0.7112      1400

2025-10-17 13:52:08 - [INFO] - üéâ Macro-F1 c·∫£i thi·ªán. ƒêang l∆∞u model t·ªët nh·∫•t v√†o '/home/guest/Projects/CS221/models/xlm-roberta-large-tuned'...
2025-10-17 13:52:11 - [INFO] - L∆∞u model th√†nh c√¥ng.
2025-10-17 13:52:11 - [INFO] - --- Epoch 3/10 ---
2025-10-17 13:56:36 - [INFO] - Loss trung b√¨nh tr√™n t·∫≠p train: 0.6770
2025-10-17 13:56:36 - [INFO] - Current Learning Rate: 7.06e-06
2025-10-17 13:56:36 - [INFO] - B·∫Øt ƒë·∫ßu ƒë√°nh gi√° tr√™n t·∫≠p validation...
2025-10-17 13:56:57 - [INFO] - Validation Loss: 0.6567
2025-10-17 13:56:57 - [INFO] - Validation Accuracy: 0.7800
2025-10-17 13:56:57 - [INFO] - Validation Macro-F1: 0.7799
2025-10-17 13:56:57 - [INFO] - Classification Report tr√™n t·∫≠p validation:
              precision    recall  f1-score   support

          no     0.7800    0.8686    0.8219       449
   extrinsic     0.7981    0.7462    0.7713       461
   intrinsic     0.7633    0.7306    0.7466       490

    accuracy                         0.7800      1400
   macro avg     0.7805    0.7818    0.7799      1400
weighted avg     0.7801    0.7800    0.7789      1400

2025-10-17 13:56:57 - [INFO] - üéâ Macro-F1 c·∫£i thi·ªán. ƒêang l∆∞u model t·ªët nh·∫•t v√†o '/home/guest/Projects/CS221/models/xlm-roberta-large-tuned'...
2025-10-17 13:56:59 - [INFO] - L∆∞u model th√†nh c√¥ng.
2025-10-17 13:56:59 - [INFO] - --- Epoch 4/10 ---
2025-10-17 14:01:23 - [INFO] - Loss trung b√¨nh tr√™n t·∫≠p train: 0.5776
2025-10-17 14:01:23 - [INFO] - Current Learning Rate: 6.00e-06
2025-10-17 14:01:23 - [INFO] - B·∫Øt ƒë·∫ßu ƒë√°nh gi√° tr√™n t·∫≠p validation...
2025-10-17 14:01:44 - [INFO] - Validation Loss: 0.6918
2025-10-17 14:01:44 - [INFO] - Validation Accuracy: 0.7779
2025-10-17 14:01:44 - [INFO] - Validation Macro-F1: 0.7783
2025-10-17 14:01:44 - [INFO] - Classification Report tr√™n t·∫≠p validation:
              precision    recall  f1-score   support

          no     0.7955    0.8575    0.8253       449
   extrinsic     0.8253    0.7072    0.7617       461
   intrinsic     0.7255    0.7714    0.7478       490

    accuracy                         0.7779      1400
   macro avg     0.7821    0.7787    0.7783      1400
weighted avg     0.7808    0.7779    0.7772      1400

2025-10-17 14:01:44 - [WARNING] - Macro-F1 kh√¥ng c·∫£i thi·ªán. Patience: 1/3
2025-10-17 14:01:44 - [INFO] - --- Epoch 5/10 ---
2025-10-17 14:06:06 - [INFO] - Loss trung b√¨nh tr√™n t·∫≠p train: 0.5002
2025-10-17 14:06:06 - [INFO] - Current Learning Rate: 4.69e-06
2025-10-17 14:06:06 - [INFO] - B·∫Øt ƒë·∫ßu ƒë√°nh gi√° tr√™n t·∫≠p validation...
2025-10-17 14:06:27 - [INFO] - Validation Loss: 0.7194
2025-10-17 14:06:27 - [INFO] - Validation Accuracy: 0.7779
2025-10-17 14:06:27 - [INFO] - Validation Macro-F1: 0.7785
2025-10-17 14:06:27 - [INFO] - Classification Report tr√™n t·∫≠p validation:
              precision    recall  f1-score   support

          no     0.8068    0.8463    0.8261       449
   extrinsic     0.7894    0.7397    0.7637       461
   intrinsic     0.7404    0.7510    0.7457       490

    accuracy                         0.7779      1400
   macro avg     0.7789    0.7790    0.7785      1400
weighted avg     0.7778    0.7779    0.7774      1400

2025-10-17 14:06:27 - [WARNING] - Macro-F1 kh√¥ng c·∫£i thi·ªán. Patience: 2/3
2025-10-17 14:06:27 - [INFO] - --- Epoch 6/10 ---
2025-10-17 14:06:49 - [INFO] - üèÅ Qu√° tr√¨nh hu·∫•n luy·ªán ho√†n t·∫•t.
2025-10-17 14:06:49 - [INFO] - Model t·ªët nh·∫•t v·ªõi Macro-F1 = 0.7799 ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i '/home/guest/Projects/CS221/models/xlm-roberta-large-tuned'
2025-10-17 14:06:52 - [INFO] - Ph√¢n ph·ªëi k·∫øt qu·∫£ tr√™n t·ª´ng l·ªõp:
  true_label  correct  incorrect  total correct_rate incorrect_rate
0  extrinsic      341        120    461       73.97%         26.03%
1  intrinsic      368        122    490       75.10%         24.90%
2         no      380         69    449       84.63%         15.37%
