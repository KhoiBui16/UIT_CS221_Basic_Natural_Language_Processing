2025-10-17 11:58:25 - [INFO] - Logger cho 'FacebookAI_xlm-roberta-large-training' ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o. File log: logs/FacebookAI_xlm-roberta-large-training/2025-10-17_11-58-25.log
2025-10-17 11:58:25 - [INFO] - Logger initialized for FacebookAI/xlm-roberta-large
2025-10-17 11:58:25 - [INFO] - ============================================================
2025-10-17 11:58:25 - [INFO] - üöÄ STARTING TRAINING SESSION
2025-10-17 11:58:25 - [INFO] - ============================================================
2025-10-17 11:58:25 - [INFO] - ROOT_DIR: /home/guest/Projects/CS221
2025-10-17 11:58:25 - [INFO] - DATA_DIR: /home/guest/Projects/CS221/data
2025-10-17 11:58:25 - [INFO] - TRAIN_FILE: /home/guest/Projects/CS221/data/vihallu-train.csv
2025-10-17 11:58:25 - [INFO] - TEST_FILE: /home/guest/Projects/CS221/data/vihallu-public-test.csv
2025-10-17 11:58:25 - [INFO] - SUBMISSION_DIR: /home/guest/Projects/CS221/submission
2025-10-17 11:58:25 - [INFO] - SUBMISSION_CSV: submit.csv
2025-10-17 11:58:25 - [INFO] - SUBMISSION_ZIP: submit.zip
2025-10-17 11:58:25 - [INFO] - MODEL_NAME: FacebookAI/xlm-roberta-large
2025-10-17 11:58:25 - [INFO] - MODEL_OUTPUT_DIR: /home/guest/Projects/CS221/models/xlm-roberta-large-tuned
2025-10-17 11:58:25 - [INFO] - MAX_LENGTH: 512
2025-10-17 11:58:25 - [INFO] - RANDOM_STATE: 42
2025-10-17 11:58:25 - [INFO] - EPOCHS: 10
2025-10-17 11:58:25 - [INFO] - BATCH_SIZE: 4
2025-10-17 11:58:25 - [INFO] - GRADIENT_ACCUMULATION_STEPS: 4
2025-10-17 11:58:25 - [INFO] - SCHEDULER_TYPE: cosine_with_restarts
2025-10-17 11:58:25 - [INFO] - LEARNING_RATE: 8e-06
2025-10-17 11:58:25 - [INFO] - WEIGHT_DECAY: 0.01
2025-10-17 11:58:25 - [INFO] - CLASSIFIER_DROPOUT: 0.1
2025-10-17 11:58:25 - [INFO] - LABEL_SMOOTHING: 0.05
2025-10-17 11:58:25 - [INFO] - EPSILON: 1e-08
2025-10-17 11:58:25 - [INFO] - PATIENCE_LIMIT: 2
2025-10-17 11:58:25 - [INFO] - TOTAL_STEP_SCALE: 0.1
2025-10-17 11:58:25 - [INFO] - VALIDATION_SPLIT_SIZE: 0.2
2025-10-17 11:58:25 - [INFO] - LABEL_MAP: {'no': 0, 'extrinsic': 1, 'intrinsic': 2}
2025-10-17 11:58:25 - [INFO] - ID2LABEL: {0: 'no', 1: 'extrinsic', 2: 'intrinsic'}
2025-10-17 11:58:25 - [INFO] - CLASS_WEIGHTS: [1.0393466963622866, 1.0114145354717525, 0.9531590413943355]
2025-10-17 11:58:25 - [INFO] - ============================================================
2025-10-17 11:58:27 - [INFO] - B·∫Øt ƒë·∫ßu pipeline hu·∫•n luy·ªán.
2025-10-17 11:58:27 - [INFO] - B∆∞·ªõc 1: Chu·∫©n b·ªã d·ªØ li·ªáu...
2025-10-17 11:58:27 - [INFO] - Chia d·ªØ li·ªáu: 5600 m·∫´u train, 1400 m·∫´u validation.
2025-10-17 11:58:27 - [INFO] - B∆∞·ªõc 2: T·∫£i model 'FacebookAI/xlm-roberta-large' v√† tokenizer...
2025-10-17 11:58:29 - [INFO] - Ph√¢n t√≠ch ki·∫øn tr√∫c m√¥ h√¨nh b·∫±ng torchinfo...
2025-10-17 11:58:32 - [INFO] - Ki·∫øn tr√∫c chi ti·∫øt c·ªßa m√¥ h√¨nh:
=====================================================================================================================================================================
Layer (type:depth-idx)                                            Input Shape               Output Shape              Param #                   Mult-Adds
=====================================================================================================================================================================
XLMRobertaForSequenceClassification                               --                        [4, 3]                    --                        --
‚îú‚îÄXLMRobertaModel: 1-1                                            [4, 512]                  [4, 512, 1024]            --                        --
‚îÇ    ‚îî‚îÄXLMRobertaEmbeddings: 2-1                                  --                        [4, 512, 1024]            --                        --
‚îÇ    ‚îÇ    ‚îî‚îÄEmbedding: 3-1                                        [4, 512]                  [4, 512, 1024]            256,002,048               1,024,008,192
‚îÇ    ‚îÇ    ‚îî‚îÄEmbedding: 3-2                                        [4, 512]                  [4, 512, 1024]            1,024                     4,096
‚îÇ    ‚îÇ    ‚îî‚îÄEmbedding: 3-3                                        [4, 512]                  [4, 512, 1024]            526,336                   2,105,344
‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-4                                        [4, 512, 1024]            [4, 512, 1024]            2,048                     8,192
‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-5                                          [4, 512, 1024]            [4, 512, 1024]            --                        --
‚îÇ    ‚îî‚îÄXLMRobertaEncoder: 2-2                                     [4, 512, 1024]            [4, 512, 1024]            --                        --
‚îÇ    ‚îÇ    ‚îî‚îÄModuleList: 3-6                                       --                        --                        302,309,376               --
‚îú‚îÄXLMRobertaClassificationHead: 1-2                               [4, 512, 1024]            [4, 3]                    --                        --
‚îÇ    ‚îî‚îÄDropout: 2-3                                               [4, 1024]                 [4, 1024]                 --                        --
‚îÇ    ‚îî‚îÄLinear: 2-4                                                [4, 1024]                 [4, 1024]                 1,049,600                 4,198,400
‚îÇ    ‚îî‚îÄDropout: 2-5                                               [4, 1024]                 [4, 1024]                 --                        --
‚îÇ    ‚îî‚îÄLinear: 2-6                                                [4, 1024]                 [4, 3]                    3,075                     12,300
=====================================================================================================================================================================
Total params: 559,893,507
Trainable params: 559,893,507
Non-trainable params: 0
Total mult-adds (Units.GIGABYTES): 2.24
=====================================================================================================================================================================
Input size (MB): 0.02
Forward/backward pass size (MB): 4496.33
Params size (MB): 2239.57
Estimated Total Size (MB): 6735.92
=====================================================================================================================================================================
2025-10-17 11:58:32 - [INFO] - B∆∞·ªõc 3: T·∫°o Dataset v√† DataLoader...
2025-10-17 11:58:32 - [INFO] - ‚úÖ T·∫°o DataLoader th√†nh c√¥ng v·ªõi DataCollatorWithPadding chu·∫©n!
2025-10-17 11:58:32 - [INFO] - Gradient accumulation steps: 4 | Effective batch size: 16
2025-10-17 11:58:32 - [INFO] - B∆∞·ªõc 4: Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng hu·∫•n luy·ªán v√† ki·∫øn tr√∫c model...
2025-10-17 11:58:32 - [INFO] - S·ª≠ d·ª•ng thi·∫øt b·ªã: cuda
2025-10-17 11:58:32 - [INFO] - ‚úÖ T√¨m th·∫•y 1 GPU(s).
2025-10-17 11:58:32 - [INFO] - ‚úÖ ƒêang s·ª≠ d·ª•ng GPU: NVIDIA GeForce RTX 5070 Ti
2025-10-17 11:58:32 - [INFO] - Ki·∫øn tr√∫c c·ªßa m√¥ h√¨nh:
XLMRobertaForSequenceClassification(
  (roberta): XLMRobertaModel(
    (embeddings): XLMRobertaEmbeddings(
      (word_embeddings): Embedding(250002, 1024, padding_idx=1)
      (position_embeddings): Embedding(514, 1024, padding_idx=1)
      (token_type_embeddings): Embedding(1, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): XLMRobertaEncoder(
      (layer): ModuleList(
        (0-23): 24 x XLMRobertaLayer(
          (attention): XLMRobertaAttention(
            (self): XLMRobertaSdpaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): XLMRobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): XLMRobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): XLMRobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (classifier): XLMRobertaClassificationHead(
    (dense): Linear(in_features=1024, out_features=1024, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=1024, out_features=3, bias=True)
  )
)
2025-10-17 11:58:32 - [INFO] - Scheduler will run for 3500 total steps (350 per epoch)
2025-10-17 11:58:32 - [INFO] - Warmup steps: 350
2025-10-17 11:58:32 - [INFO] - S·ª≠ d·ª•ng Class Weights & Label smoothing cho h√†m loss.
2025-10-17 11:58:32 - [INFO] - --- Epoch 1/10 ---
2025-10-17 12:02:56 - [INFO] - Loss trung b√¨nh tr√™n t·∫≠p train: 1.0898
2025-10-17 12:02:56 - [INFO] - Current Learning Rate: 8.00e-06
2025-10-17 12:02:56 - [INFO] - B·∫Øt ƒë·∫ßu ƒë√°nh gi√° tr√™n t·∫≠p validation...
2025-10-17 12:03:17 - [INFO] - Validation Loss: 0.8521
2025-10-17 12:03:17 - [INFO] - Validation Accuracy: 0.6021
2025-10-17 12:03:17 - [INFO] - Validation Macro-F1: 0.5427
2025-10-17 12:03:17 - [INFO] - Classification Report tr√™n t·∫≠p validation:
              precision    recall  f1-score   support

          no     0.6771    0.8686    0.7610       449
   extrinsic     0.5363    0.8503    0.6577       461
   intrinsic     0.6559    0.1245    0.2093       490

    accuracy                         0.6021      1400
   macro avg     0.6231    0.6145    0.5427      1400
weighted avg     0.6233    0.6021    0.5339      1400

2025-10-17 12:03:17 - [INFO] - üéâ Macro-F1 c·∫£i thi·ªán. ƒêang l∆∞u model t·ªët nh·∫•t v√†o '/home/guest/Projects/CS221/models/xlm-roberta-large-tuned'...
2025-10-17 12:03:19 - [INFO] - L∆∞u model th√†nh c√¥ng.
2025-10-17 12:03:19 - [INFO] - --- Epoch 2/10 ---
2025-10-17 12:07:40 - [INFO] - Loss trung b√¨nh tr√™n t·∫≠p train: 0.7559
2025-10-17 12:07:40 - [INFO] - Current Learning Rate: 7.76e-06
2025-10-17 12:07:40 - [INFO] - B·∫Øt ƒë·∫ßu ƒë√°nh gi√° tr√™n t·∫≠p validation...
2025-10-17 12:08:01 - [INFO] - Validation Loss: 0.6829
2025-10-17 12:08:01 - [INFO] - Validation Accuracy: 0.7700
2025-10-17 12:08:01 - [INFO] - Validation Macro-F1: 0.7690
2025-10-17 12:08:01 - [INFO] - Classification Report tr√™n t·∫≠p validation:
              precision    recall  f1-score   support

          no     0.7771    0.8775    0.8243       449
   extrinsic     0.7423    0.7809    0.7611       461
   intrinsic     0.7941    0.6612    0.7216       490

    accuracy                         0.7700      1400
   macro avg     0.7712    0.7732    0.7690      1400
weighted avg     0.7716    0.7700    0.7675      1400

2025-10-17 12:08:01 - [INFO] - üéâ Macro-F1 c·∫£i thi·ªán. ƒêang l∆∞u model t·ªët nh·∫•t v√†o '/home/guest/Projects/CS221/models/xlm-roberta-large-tuned'...
2025-10-17 12:08:03 - [INFO] - L∆∞u model th√†nh c√¥ng.
2025-10-17 12:08:03 - [INFO] - --- Epoch 3/10 ---
2025-10-17 12:12:26 - [INFO] - Loss trung b√¨nh tr√™n t·∫≠p train: 0.6188
2025-10-17 12:12:26 - [INFO] - Current Learning Rate: 7.06e-06
2025-10-17 12:12:26 - [INFO] - B·∫Øt ƒë·∫ßu ƒë√°nh gi√° tr√™n t·∫≠p validation...
2025-10-17 12:12:46 - [INFO] - Validation Loss: 0.6568
2025-10-17 12:12:46 - [INFO] - Validation Accuracy: 0.7857
2025-10-17 12:12:46 - [INFO] - Validation Macro-F1: 0.7854
2025-10-17 12:12:46 - [INFO] - Classification Report tr√™n t·∫≠p validation:
              precision    recall  f1-score   support

          no     0.7916    0.8797    0.8333       449
   extrinsic     0.7732    0.7766    0.7749       461
   intrinsic     0.7922    0.7082    0.7478       490

    accuracy                         0.7857      1400
   macro avg     0.7857    0.7882    0.7854      1400
weighted avg     0.7858    0.7857    0.7842      1400

2025-10-17 12:12:46 - [INFO] - üéâ Macro-F1 c·∫£i thi·ªán. ƒêang l∆∞u model t·ªët nh·∫•t v√†o '/home/guest/Projects/CS221/models/xlm-roberta-large-tuned'...
2025-10-17 12:12:49 - [INFO] - L∆∞u model th√†nh c√¥ng.
2025-10-17 12:12:49 - [INFO] - --- Epoch 4/10 ---
2025-10-17 12:17:10 - [INFO] - Loss trung b√¨nh tr√™n t·∫≠p train: 0.5347
2025-10-17 12:17:10 - [INFO] - Current Learning Rate: 6.00e-06
2025-10-17 12:17:10 - [INFO] - B·∫Øt ƒë·∫ßu ƒë√°nh gi√° tr√™n t·∫≠p validation...
2025-10-17 12:17:31 - [INFO] - Validation Loss: 0.6736
2025-10-17 12:17:31 - [INFO] - Validation Accuracy: 0.7800
2025-10-17 12:17:31 - [INFO] - Validation Macro-F1: 0.7815
2025-10-17 12:17:31 - [INFO] - Classification Report tr√™n t·∫≠p validation:
              precision    recall  f1-score   support

          no     0.8274    0.8218    0.8246       449
   extrinsic     0.8110    0.7354    0.7713       461
   intrinsic     0.7164    0.7837    0.7485       490

    accuracy                         0.7800      1400
   macro avg     0.7849    0.7803    0.7815      1400
weighted avg     0.7831    0.7800    0.7804      1400

2025-10-17 12:17:31 - [WARNING] - Macro-F1 kh√¥ng c·∫£i thi·ªán. Patience: 1/2
2025-10-17 12:17:31 - [INFO] - --- Epoch 5/10 ---
2025-10-17 12:21:57 - [INFO] - Loss trung b√¨nh tr√™n t·∫≠p train: 0.4445
2025-10-17 12:21:57 - [INFO] - Current Learning Rate: 4.69e-06
2025-10-17 12:21:57 - [INFO] - B·∫Øt ƒë·∫ßu ƒë√°nh gi√° tr√™n t·∫≠p validation...
2025-10-17 12:22:18 - [INFO] - Validation Loss: 0.7533
2025-10-17 12:22:18 - [INFO] - Validation Accuracy: 0.7743
2025-10-17 12:22:18 - [INFO] - Validation Macro-F1: 0.7752
2025-10-17 12:22:18 - [INFO] - Classification Report tr√™n t·∫≠p validation:
              precision    recall  f1-score   support

          no     0.8211    0.8486    0.8346       449
   extrinsic     0.7840    0.7245    0.7531       461
   intrinsic     0.7235    0.7531    0.7380       490

    accuracy                         0.7743      1400
   macro avg     0.7762    0.7754    0.7752      1400
weighted avg     0.7748    0.7743    0.7740      1400

2025-10-17 12:22:18 - [WARNING] - Macro-F1 kh√¥ng c·∫£i thi·ªán. Patience: 2/2
2025-10-17 12:22:18 - [INFO] - Early stopping! D·ª´ng hu·∫•n luy·ªán.
2025-10-17 12:22:18 - [INFO] - üèÅ Qu√° tr√¨nh hu·∫•n luy·ªán ho√†n t·∫•t.
2025-10-17 12:22:18 - [INFO] - Model t·ªët nh·∫•t v·ªõi Macro-F1 = 0.7854 ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i '/home/guest/Projects/CS221/models/xlm-roberta-large-tuned'
2025-10-17 12:22:18 - [INFO] - Ph√¢n ph·ªëi k·∫øt qu·∫£ tr√™n t·ª´ng l·ªõp:
  true_label  correct  incorrect  total correct_rate incorrect_rate
0  extrinsic      334        127    461       72.45%         27.55%
1  intrinsic      369        121    490       75.31%         24.69%
2         no      381         68    449       84.86%         15.14%
