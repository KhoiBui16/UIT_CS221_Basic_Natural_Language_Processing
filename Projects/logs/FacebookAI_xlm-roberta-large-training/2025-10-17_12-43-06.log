2025-10-17 12:43:06 - [INFO] - Logger cho 'FacebookAI_xlm-roberta-large-training' ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o. File log: logs/FacebookAI_xlm-roberta-large-training/2025-10-17_12-43-06.log
2025-10-17 12:43:06 - [INFO] - Logger initialized for FacebookAI/xlm-roberta-large
2025-10-17 12:43:06 - [INFO] - ============================================================
2025-10-17 12:43:06 - [INFO] - üöÄ STARTING TRAINING SESSION
2025-10-17 12:43:06 - [INFO] - ============================================================
2025-10-17 12:43:06 - [INFO] - ROOT_DIR: /home/guest/Projects/CS221
2025-10-17 12:43:06 - [INFO] - DATA_DIR: /home/guest/Projects/CS221/data
2025-10-17 12:43:06 - [INFO] - TRAIN_FILE: /home/guest/Projects/CS221/data/vihallu-train.csv
2025-10-17 12:43:06 - [INFO] - TEST_FILE: /home/guest/Projects/CS221/data/vihallu-public-test.csv
2025-10-17 12:43:06 - [INFO] - SUBMISSION_DIR: /home/guest/Projects/CS221/submission
2025-10-17 12:43:06 - [INFO] - SUBMISSION_CSV: submit.csv
2025-10-17 12:43:06 - [INFO] - SUBMISSION_ZIP: submit.zip
2025-10-17 12:43:06 - [INFO] - MODEL_NAME: FacebookAI/xlm-roberta-large
2025-10-17 12:43:06 - [INFO] - MODEL_OUTPUT_DIR: /home/guest/Projects/CS221/models/xlm-roberta-large-tuned
2025-10-17 12:43:06 - [INFO] - MAX_LENGTH: 512
2025-10-17 12:43:06 - [INFO] - RANDOM_STATE: 42
2025-10-17 12:43:06 - [INFO] - EPOCHS: 10
2025-10-17 12:43:06 - [INFO] - BATCH_SIZE: 4
2025-10-17 12:43:06 - [INFO] - GRADIENT_ACCUMULATION_STEPS: 4
2025-10-17 12:43:06 - [INFO] - SCHEDULER_TYPE: cosine_with_restarts
2025-10-17 12:43:06 - [INFO] - LEARNING_RATE: 1e-05
2025-10-17 12:43:06 - [INFO] - WEIGHT_DECAY: 0.01
2025-10-17 12:43:06 - [INFO] - NUM_CYCLES: 3
2025-10-17 12:43:06 - [INFO] - CLASSIFIER_DROPOUT: 0.1
2025-10-17 12:43:06 - [INFO] - LABEL_SMOOTHING: 0.05
2025-10-17 12:43:06 - [INFO] - TOTAL_STEP_SCALE: 0.1
2025-10-17 12:43:06 - [INFO] - EPSILON: 1e-08
2025-10-17 12:43:06 - [INFO] - PATIENCE_LIMIT: 2
2025-10-17 12:43:06 - [INFO] - VALIDATION_SPLIT_SIZE: 0.2
2025-10-17 12:43:06 - [INFO] - LABEL_MAP: {'no': 0, 'extrinsic': 1, 'intrinsic': 2}
2025-10-17 12:43:06 - [INFO] - ID2LABEL: {0: 'no', 1: 'extrinsic', 2: 'intrinsic'}
2025-10-17 12:43:06 - [INFO] - CLASS_WEIGHTS: [1.0393466963622866, 1.0114145354717525, 0.9531590413943355]
2025-10-17 12:43:06 - [INFO] - ============================================================
2025-10-17 12:43:08 - [INFO] - B·∫Øt ƒë·∫ßu pipeline hu·∫•n luy·ªán.
2025-10-17 12:43:08 - [INFO] - B∆∞·ªõc 1: Chu·∫©n b·ªã d·ªØ li·ªáu...
2025-10-17 12:43:08 - [INFO] - Chia d·ªØ li·ªáu: 5600 m·∫´u train, 1400 m·∫´u validation.
2025-10-17 12:43:08 - [INFO] - B∆∞·ªõc 2: T·∫£i model 'FacebookAI/xlm-roberta-large' v√† tokenizer...
2025-10-17 12:43:10 - [INFO] - Ph√¢n t√≠ch ki·∫øn tr√∫c m√¥ h√¨nh b·∫±ng torchinfo...
2025-10-17 12:43:13 - [INFO] - Ki·∫øn tr√∫c chi ti·∫øt c·ªßa m√¥ h√¨nh:
=====================================================================================================================================================================
Layer (type:depth-idx)                                            Input Shape               Output Shape              Param #                   Mult-Adds
=====================================================================================================================================================================
XLMRobertaForSequenceClassification                               --                        [4, 3]                    --                        --
‚îú‚îÄXLMRobertaModel: 1-1                                            [4, 512]                  [4, 512, 1024]            --                        --
‚îÇ    ‚îî‚îÄXLMRobertaEmbeddings: 2-1                                  --                        [4, 512, 1024]            --                        --
‚îÇ    ‚îÇ    ‚îî‚îÄEmbedding: 3-1                                        [4, 512]                  [4, 512, 1024]            256,002,048               1,024,008,192
‚îÇ    ‚îÇ    ‚îî‚îÄEmbedding: 3-2                                        [4, 512]                  [4, 512, 1024]            1,024                     4,096
‚îÇ    ‚îÇ    ‚îî‚îÄEmbedding: 3-3                                        [4, 512]                  [4, 512, 1024]            526,336                   2,105,344
‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-4                                        [4, 512, 1024]            [4, 512, 1024]            2,048                     8,192
‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-5                                          [4, 512, 1024]            [4, 512, 1024]            --                        --
‚îÇ    ‚îî‚îÄXLMRobertaEncoder: 2-2                                     [4, 512, 1024]            [4, 512, 1024]            --                        --
‚îÇ    ‚îÇ    ‚îî‚îÄModuleList: 3-6                                       --                        --                        302,309,376               --
‚îú‚îÄXLMRobertaClassificationHead: 1-2                               [4, 512, 1024]            [4, 3]                    --                        --
‚îÇ    ‚îî‚îÄDropout: 2-3                                               [4, 1024]                 [4, 1024]                 --                        --
‚îÇ    ‚îî‚îÄLinear: 2-4                                                [4, 1024]                 [4, 1024]                 1,049,600                 4,198,400
‚îÇ    ‚îî‚îÄDropout: 2-5                                               [4, 1024]                 [4, 1024]                 --                        --
‚îÇ    ‚îî‚îÄLinear: 2-6                                                [4, 1024]                 [4, 3]                    3,075                     12,300
=====================================================================================================================================================================
Total params: 559,893,507
Trainable params: 559,893,507
Non-trainable params: 0
Total mult-adds (Units.GIGABYTES): 2.24
=====================================================================================================================================================================
Input size (MB): 0.02
Forward/backward pass size (MB): 4496.33
Params size (MB): 2239.57
Estimated Total Size (MB): 6735.92
=====================================================================================================================================================================
2025-10-17 12:43:13 - [INFO] - B∆∞·ªõc 3: T·∫°o Dataset v√† DataLoader...
2025-10-17 12:43:13 - [INFO] - ‚úÖ T·∫°o DataLoader th√†nh c√¥ng v·ªõi DataCollatorWithPadding chu·∫©n!
2025-10-17 12:43:13 - [INFO] - Gradient accumulation steps: 4 | Effective batch size: 16
2025-10-17 12:43:13 - [INFO] - B∆∞·ªõc 4: Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng hu·∫•n luy·ªán v√† ki·∫øn tr√∫c model...
2025-10-17 12:43:13 - [INFO] - S·ª≠ d·ª•ng thi·∫øt b·ªã: cuda
2025-10-17 12:43:13 - [INFO] - ‚úÖ T√¨m th·∫•y 1 GPU(s).
2025-10-17 12:43:13 - [INFO] - ‚úÖ ƒêang s·ª≠ d·ª•ng GPU: NVIDIA GeForce RTX 5070 Ti
2025-10-17 12:43:13 - [INFO] - Ki·∫øn tr√∫c c·ªßa m√¥ h√¨nh:
XLMRobertaForSequenceClassification(
  (roberta): XLMRobertaModel(
    (embeddings): XLMRobertaEmbeddings(
      (word_embeddings): Embedding(250002, 1024, padding_idx=1)
      (position_embeddings): Embedding(514, 1024, padding_idx=1)
      (token_type_embeddings): Embedding(1, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): XLMRobertaEncoder(
      (layer): ModuleList(
        (0-23): 24 x XLMRobertaLayer(
          (attention): XLMRobertaAttention(
            (self): XLMRobertaSdpaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): XLMRobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): XLMRobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): XLMRobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (classifier): XLMRobertaClassificationHead(
    (dense): Linear(in_features=1024, out_features=1024, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=1024, out_features=3, bias=True)
  )
)
2025-10-17 12:43:14 - [INFO] - Scheduler will run for 3500 total steps (350 per epoch)
2025-10-17 12:43:14 - [INFO] - S·ª≠ d·ª•ng scheduler chuy√™n d·ª•ng: cosine_with_hard_restarts_schedule_with_warmup v·ªõi 3 chu k·ª≥.
2025-10-17 12:43:14 - [INFO] - Warmup steps: 350
2025-10-17 12:43:14 - [INFO] - S·ª≠ d·ª•ng Class Weights & Label smoothing cho h√†m loss.
2025-10-17 12:43:14 - [INFO] - --- Epoch 1/10 ---
2025-10-17 12:47:39 - [INFO] - Loss trung b√¨nh tr√™n t·∫≠p train: 1.1025
2025-10-17 12:47:39 - [INFO] - Current Learning Rate: 1.00e-05
2025-10-17 12:47:39 - [INFO] - B·∫Øt ƒë·∫ßu ƒë√°nh gi√° tr√™n t·∫≠p validation...
2025-10-17 12:48:00 - [INFO] - Validation Loss: 0.9989
2025-10-17 12:48:00 - [INFO] - Validation Accuracy: 0.5486
2025-10-17 12:48:00 - [INFO] - Validation Macro-F1: 0.5530
2025-10-17 12:48:00 - [INFO] - Classification Report tr√™n t·∫≠p validation:
              precision    recall  f1-score   support

          no     0.5420    0.4454    0.4890       449
   extrinsic     0.7264    0.6681    0.6960       461
   intrinsic     0.4283    0.5306    0.4740       490

    accuracy                         0.5486      1400
   macro avg     0.5656    0.5481    0.5530      1400
weighted avg     0.5629    0.5486    0.5519      1400

2025-10-17 12:48:00 - [INFO] - üéâ Macro-F1 c·∫£i thi·ªán. ƒêang l∆∞u model t·ªët nh·∫•t v√†o '/home/guest/Projects/CS221/models/xlm-roberta-large-tuned'...
2025-10-17 12:48:02 - [INFO] - L∆∞u model th√†nh c√¥ng.
2025-10-17 12:48:02 - [INFO] - --- Epoch 2/10 ---
2025-10-17 12:52:28 - [INFO] - Loss trung b√¨nh tr√™n t·∫≠p train: 0.8730
2025-10-17 12:52:28 - [INFO] - Current Learning Rate: 7.50e-06
2025-10-17 12:52:28 - [INFO] - B·∫Øt ƒë·∫ßu ƒë√°nh gi√° tr√™n t·∫≠p validation...
2025-10-17 12:52:48 - [INFO] - Validation Loss: 0.7446
2025-10-17 12:52:48 - [INFO] - Validation Accuracy: 0.7493
2025-10-17 12:52:48 - [INFO] - Validation Macro-F1: 0.7512
2025-10-17 12:52:48 - [INFO] - Classification Report tr√™n t·∫≠p validation:
              precision    recall  f1-score   support

          no     0.7873    0.7996    0.7934       449
   extrinsic     0.8020    0.7115    0.7540       461
   intrinsic     0.6766    0.7388    0.7063       490

    accuracy                         0.7493      1400
   macro avg     0.7553    0.7499    0.7512      1400
weighted avg     0.7534    0.7493    0.7500      1400

2025-10-17 12:52:48 - [INFO] - üéâ Macro-F1 c·∫£i thi·ªán. ƒêang l∆∞u model t·ªët nh·∫•t v√†o '/home/guest/Projects/CS221/models/xlm-roberta-large-tuned'...
2025-10-17 12:52:50 - [INFO] - L∆∞u model th√†nh c√¥ng.
2025-10-17 12:52:50 - [INFO] - --- Epoch 3/10 ---
2025-10-17 12:57:11 - [INFO] - Loss trung b√¨nh tr√™n t·∫≠p train: 0.6763
2025-10-17 12:57:11 - [INFO] - Current Learning Rate: 2.50e-06
2025-10-17 12:57:11 - [INFO] - B·∫Øt ƒë·∫ßu ƒë√°nh gi√° tr√™n t·∫≠p validation...
2025-10-17 12:57:31 - [INFO] - Validation Loss: 0.7052
2025-10-17 12:57:31 - [INFO] - Validation Accuracy: 0.7700
2025-10-17 12:57:31 - [INFO] - Validation Macro-F1: 0.7705
2025-10-17 12:57:31 - [INFO] - Classification Report tr√™n t·∫≠p validation:
              precision    recall  f1-score   support

          no     0.7991    0.8241    0.8114       449
   extrinsic     0.7562    0.7939    0.7746       461
   intrinsic     0.7550    0.6980    0.7253       490

    accuracy                         0.7700      1400
   macro avg     0.7701    0.7720    0.7705      1400
weighted avg     0.7695    0.7700    0.7692      1400

2025-10-17 12:57:31 - [INFO] - üéâ Macro-F1 c·∫£i thi·ªán. ƒêang l∆∞u model t·ªët nh·∫•t v√†o '/home/guest/Projects/CS221/models/xlm-roberta-large-tuned'...
2025-10-17 12:57:34 - [INFO] - L∆∞u model th√†nh c√¥ng.
2025-10-17 12:57:34 - [INFO] - --- Epoch 4/10 ---
2025-10-17 13:01:56 - [INFO] - Loss trung b√¨nh tr√™n t·∫≠p train: 0.5705
2025-10-17 13:01:56 - [INFO] - Current Learning Rate: 1.00e-05
2025-10-17 13:01:56 - [INFO] - B·∫Øt ƒë·∫ßu ƒë√°nh gi√° tr√™n t·∫≠p validation...
2025-10-17 13:02:16 - [INFO] - Validation Loss: 0.6984
2025-10-17 13:02:16 - [INFO] - Validation Accuracy: 0.7764
2025-10-17 13:02:16 - [INFO] - Validation Macro-F1: 0.7773
2025-10-17 13:02:16 - [INFO] - Classification Report tr√™n t·∫≠p validation:
              precision    recall  f1-score   support

          no     0.8043    0.8241    0.8141       449
   extrinsic     0.7918    0.7505    0.7706       461
   intrinsic     0.7376    0.7571    0.7472       490

    accuracy                         0.7764      1400
   macro avg     0.7779    0.7772    0.7773      1400
weighted avg     0.7768    0.7764    0.7764      1400

2025-10-17 13:02:16 - [INFO] - üéâ Macro-F1 c·∫£i thi·ªán. ƒêang l∆∞u model t·ªët nh·∫•t v√†o '/home/guest/Projects/CS221/models/xlm-roberta-large-tuned'...
2025-10-17 13:02:19 - [INFO] - L∆∞u model th√†nh c√¥ng.
2025-10-17 13:02:19 - [INFO] - --- Epoch 5/10 ---
2025-10-17 13:06:40 - [INFO] - Loss trung b√¨nh tr√™n t·∫≠p train: 0.6016
2025-10-17 13:06:40 - [INFO] - Current Learning Rate: 7.50e-06
2025-10-17 13:06:40 - [INFO] - B·∫Øt ƒë·∫ßu ƒë√°nh gi√° tr√™n t·∫≠p validation...
2025-10-17 13:07:00 - [INFO] - Validation Loss: 0.7256
2025-10-17 13:07:00 - [INFO] - Validation Accuracy: 0.7529
2025-10-17 13:07:00 - [INFO] - Validation Macro-F1: 0.7548
2025-10-17 13:07:00 - [INFO] - Classification Report tr√™n t·∫≠p validation:
              precision    recall  f1-score   support

          no     0.8373    0.7105    0.7687       449
   extrinsic     0.7825    0.7570    0.7696       461
   intrinsic     0.6736    0.7878    0.7262       490

    accuracy                         0.7529      1400
   macro avg     0.7645    0.7518    0.7548      1400
weighted avg     0.7620    0.7529    0.7541      1400

2025-10-17 13:07:00 - [WARNING] - Macro-F1 kh√¥ng c·∫£i thi·ªán. Patience: 1/2
2025-10-17 13:07:00 - [INFO] - --- Epoch 6/10 ---
2025-10-17 13:11:22 - [INFO] - Loss trung b√¨nh tr√™n t·∫≠p train: 0.5101
2025-10-17 13:11:22 - [INFO] - Current Learning Rate: 2.50e-06
2025-10-17 13:11:22 - [INFO] - B·∫Øt ƒë·∫ßu ƒë√°nh gi√° tr√™n t·∫≠p validation...
2025-10-17 13:11:43 - [INFO] - Validation Loss: 0.7056
2025-10-17 13:11:43 - [INFO] - Validation Accuracy: 0.7771
2025-10-17 13:11:43 - [INFO] - Validation Macro-F1: 0.7778
2025-10-17 13:11:43 - [INFO] - Classification Report tr√™n t·∫≠p validation:
              precision    recall  f1-score   support

          no     0.8035    0.8285    0.8158       449
   extrinsic     0.7729    0.7679    0.7704       461
   intrinsic     0.7557    0.7388    0.7472       490

    accuracy                         0.7771      1400
   macro avg     0.7774    0.7784    0.7778      1400
weighted avg     0.7767    0.7771    0.7768      1400

2025-10-17 13:11:43 - [INFO] - üéâ Macro-F1 c·∫£i thi·ªán. ƒêang l∆∞u model t·ªët nh·∫•t v√†o '/home/guest/Projects/CS221/models/xlm-roberta-large-tuned'...
2025-10-17 13:11:46 - [INFO] - L∆∞u model th√†nh c√¥ng.
2025-10-17 13:11:46 - [INFO] - --- Epoch 7/10 ---
2025-10-17 13:16:09 - [INFO] - Loss trung b√¨nh tr√™n t·∫≠p train: 0.4125
2025-10-17 13:16:09 - [INFO] - Current Learning Rate: 1.00e-05
2025-10-17 13:16:09 - [INFO] - B·∫Øt ƒë·∫ßu ƒë√°nh gi√° tr√™n t·∫≠p validation...
2025-10-17 13:16:31 - [INFO] - Validation Loss: 0.7736
2025-10-17 13:16:31 - [INFO] - Validation Accuracy: 0.7750
2025-10-17 13:16:31 - [INFO] - Validation Macro-F1: 0.7759
2025-10-17 13:16:31 - [INFO] - Classification Report tr√™n t·∫≠p validation:
              precision    recall  f1-score   support

          no     0.8100    0.8263    0.8181       449
   extrinsic     0.7805    0.7484    0.7641       461
   intrinsic     0.7380    0.7531    0.7455       490

    accuracy                         0.7750      1400
   macro avg     0.7762    0.7759    0.7759      1400
weighted avg     0.7751    0.7750    0.7749      1400

2025-10-17 13:16:31 - [WARNING] - Macro-F1 kh√¥ng c·∫£i thi·ªán. Patience: 1/2
2025-10-17 13:16:31 - [INFO] - --- Epoch 8/10 ---
2025-10-17 13:20:57 - [INFO] - Loss trung b√¨nh tr√™n t·∫≠p train: 0.4659
2025-10-17 13:20:57 - [INFO] - Current Learning Rate: 7.50e-06
2025-10-17 13:20:57 - [INFO] - B·∫Øt ƒë·∫ßu ƒë√°nh gi√° tr√™n t·∫≠p validation...
2025-10-17 13:21:18 - [INFO] - Validation Loss: 0.8372
2025-10-17 13:21:18 - [INFO] - Validation Accuracy: 0.7357
2025-10-17 13:21:18 - [INFO] - Validation Macro-F1: 0.7381
2025-10-17 13:21:18 - [INFO] - Classification Report tr√™n t·∫≠p validation:
              precision    recall  f1-score   support

          no     0.8144    0.7327    0.7714       449
   extrinsic     0.8251    0.6551    0.7304       461
   intrinsic     0.6333    0.8143    0.7125       490

    accuracy                         0.7357      1400
   macro avg     0.7576    0.7340    0.7381      1400
weighted avg     0.7545    0.7357    0.7373      1400

2025-10-17 13:21:18 - [WARNING] - Macro-F1 kh√¥ng c·∫£i thi·ªán. Patience: 2/2
2025-10-17 13:21:18 - [INFO] - Early stopping! D·ª´ng hu·∫•n luy·ªán.
2025-10-17 13:21:18 - [INFO] - üèÅ Qu√° tr√¨nh hu·∫•n luy·ªán ho√†n t·∫•t.
2025-10-17 13:21:18 - [INFO] - Model t·ªët nh·∫•t v·ªõi Macro-F1 = 0.7778 ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i '/home/guest/Projects/CS221/models/xlm-roberta-large-tuned'
2025-10-17 13:21:18 - [INFO] - Ph√¢n ph·ªëi k·∫øt qu·∫£ tr√™n t·ª´ng l·ªõp:
  true_label  correct  incorrect  total correct_rate incorrect_rate
0  extrinsic      302        159    461       65.51%         34.49%
1  intrinsic      399         91    490       81.43%         18.57%
2         no      329        120    449       73.27%         26.73%
